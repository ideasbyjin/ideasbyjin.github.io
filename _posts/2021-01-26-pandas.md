---
layout: post
title: "My favourite Pandas tricks"
date: 2021-01-26
category: stats
summary: Pandas tips & tricks
mathjax: true
---

These are some Pandas tricks I use frequently; I hope it's just as useful to you too!

## Visual aids

### TQDM
I absolutely love TQDM, partly because of how much I end up coding in `IPython` or a Jupyter environment.
It's always helpful to know how far along I've gone along in applying some function:

```python
from tqdm import tqdm
tqdm.pandas()

# foo is a function to apply on a value of the column
df['column'].progress_apply(lambda z: foo(z)) # watch magic happen!
```

### Plotting!
I don't think I take advantage of this feature enough, partly because libraries like Seaborn, plotly, plotnine
and Altair all work natively with `Dataframe` objects. But if you just want something quick, these go a long
way, too:
```python
df['column'].hist() 
df.plot(x = 'column1', y = 'column2', kind = 'scatter')
```

## Column manipulation

### Strings as aggregation functions
There's loads of these, e.g. `std`, `mean`, `first`, etc... 

| Category | Colour  | Item name | Price |
| -------- | ------- | ----------| ----- |
| Shirt    | Red | T-shirt   | 20    |
| Shirt    | White | Oxford shirt   | 30    |
| Trousers   | Blue | Jeans   | 10    |
| Shirt    | Black | Oxford-shirt   | 22   |

```python
df.groupby("Category").agg({
    "Colour": "first", # could be useful for string variables
    "Price": "mean"    # this just calls a really efficient average operation
})
```

### Getting unique values
Gone are the days of using something like `set(df["column"])`; 

```python
df["column"].unique()  # get the unique items in a column
df["column"].nunique() # get the number of unique items
```

## Slightly more efficient CSV reading/handling

### Chunkifying data
I have to credit my old colleague Magda for this trick:
```python
# foo is a function that you can use to subset/filter a subsection of the bigger dataframe
df = pd.concat([chunk for chunk in pd.read_csv(PATH, chunksize=1000) if foo(chunk)])
```

### Only getting some columns
This is a three-stage process, but it saves memory, which is a bonus!
```python
header = pd.read_csv(PATH, nrows = 1).columns

# foo is a function that you can use to subset/filter some columns
usecols = [c for c in header if foo(c)]
df = pd.read_csv(PATH, usecols = usecols) 
```

### Memory efficiency
This is not what I do immediately, but I find that it sometimes has benefits, especially when memory is a
bit precious and I have to make ends meet:

```python
pd.to_numeric(df['numeric_column'], downcast='unsigned') # only really works for positive integers
pd.to_numeric(df['numeric_column'], downcast='Sparse[int]') # more effective with lots of 0s
df['column'].astype(bool) # is your data full of 0s and 1s...?
```

## SQL(?) for Pandas
Yes, you can call SQL via Pandas, e.g.
```python
conn = sqlite3.connect() # or a sqlalchemy connection... etc.
pd.read_sql("""SELECT * FROM ... """, con = conn)
```

but you can also do this! I find this is slightly more readable, especially when there's lots of conditions
```python
df.query("Colour == 'Red' and Price > 15")
```

## Serialisation
While TSV/CSV is the go-to format for many, I've found that working with them can be a bit of a pain,
especially when my files get big. Some formats I've played with lately are Apache's `feather` and `Parquet`
formats. 

While they sometimes don't offer as much compression as the humble `gzip`, they're still much
better at reading; remember to have `pyarrow` installed!
```python
df.to_parquet()
pd.read_parquet()
```  

Next time I'll cover `numba`, which has been one of the most exciting things I've worked with lately.