[{"content":"I\u0026rsquo;ve recently migrated my blog to Hugo instead of Jekyll. It\u0026rsquo;s an ongoing process and there might be some broken elements (e.g. images) but will get to updating these soon. In the meantime, enjoy the new format.\nI\u0026rsquo;ll do a post at some stage about how I\u0026rsquo;ve done this. Long story short, I was a bit tired of Jekyll\u0026rsquo;s dependency on Ruby which required a lot of tinkering on my Mac (with the Big Sur update). In fact, I\u0026rsquo;m still not sure how the Ruby version permutations had ended up working out for me, but I decided to go for something more modern and sleek. Not to mention searchability, an archive format, and reading time estimates were huge wins for me.\nEnjoy!\n","permalink":"https://ideasbyjin.github.io/post/2021-02-20-hugo-migration/","summary":"I\u0026rsquo;ve recently migrated my blog to Hugo instead of Jekyll. It\u0026rsquo;s an ongoing process and there might be some broken elements (e.g. images) but will get to updating these soon. In the meantime, enjoy the new format.\nI\u0026rsquo;ll do a post at some stage about how I\u0026rsquo;ve done this. Long story short, I was a bit tired of Jekyll\u0026rsquo;s dependency on Ruby which required a lot of tinkering on my Mac (with the Big Sur update).","title":"Hugo Migration"},{"content":"    I’m a biochemist-turned-bioinformatician. Since my D.Phil (PhD) days, I have been fascinated by the prospect of using technology, domain knowledge, and of course, data, to tackle some big problems in biomedical informatics. More broadly, I like to think about how \u0026ldquo;what makes us human\u0026rdquo; can help us solve problems and design algorithms.\nIn my spare time, I apply lessons from tech and data science to other domains (e.g. sports). Oh, and I cycle and bake a bit.\n","permalink":"https://ideasbyjin.github.io/about/","summary":"I’m a biochemist-turned-bioinformatician. Since my D.Phil (PhD) days, I have been fascinated by the prospect of using technology, domain knowledge, and of course, data, to tackle some big problems in biomedical informatics. More broadly, I like to think about how \u0026ldquo;what makes us human\u0026rdquo; can help us solve problems and design algorithms.\nIn my spare time, I apply lessons from tech and data science to other domains (e.","title":"About me"},{"content":"I\u0026rsquo;ve discovered numba a few months ago, and haven\u0026rsquo;t looked back since. I wouldn\u0026rsquo;t consider myself a numba expert as there\u0026rsquo;s lots of capabilities I\u0026rsquo;m still wrapping my head around (e.g. @overload decorators), but there\u0026rsquo;s some use cases where Numba could be really useful. If you find you\u0026rsquo;re doing a lot of numpy for your work, this is for you!\nIf you have:\n 30 seconds: @numba.jit(nopython=True) your functions if it\u0026rsquo;s purely numpy driven. Can be a bit of effort though. 5 minutes: read on.  Okay, what is numba, exactly? Numba is a \u0026ldquo;just-in-time\u0026rdquo; (JIT) compiler, which essentially means that a function you create will be compiled and can run independently of the Python interpreter. Simply, if your code involves lots of numpy and math, chances are, you can speed it up even more.\nThat sounds great, but\u0026hellip; Anecdotally speaking I\u0026rsquo;ve seen about 100-200x performance gains for some of the functions I\u0026rsquo;ve written with numba. The beauty of it is that it involves almost no work, provided that the code is mostly already in numpy, and that numba is compatible with your implementation. This is the catch that I think the 5-minute numba docs don\u0026rsquo;t cover.\nnumba is heavily typed and has specific implementations and signatures of numpy functions. In other words, you might have found that you can \u0026ldquo;get away\u0026rdquo; with calling numpy in specific ways in \u0026ldquo;normal\u0026rdquo; Python, but upon compiling with numba, you will come to some very surprising errors. You could argue that this is helpful though.\nExample The example we\u0026rsquo;re going to cover is the RAPDF function from Samudrala and Moult. RAPDF represents a sort of log odds ratio that two atoms of types $i$ and $j$ come into contact in a specific distance bin $d$:\n$$\\textrm{RAPDF} = -log\\left(\\dfrac{\\dfrac{n(d_{ij})}{\\sum_d n(d_{ij})}}{\\dfrac{\\sum_{ij} n(d_{ij})}{\\sum_d \\sum_{ij} n(d_{ij})}}\\right) $$\nThis is typically the kind of data where RAPDF would be applied to; I\u0026rsquo;ve simplified the contacts to amino acids rather than atoms, but in practice, it would involve the same computation:\nimport pandas as pd # this is just a randomly generated contact map # each cell shows number of contacts between 2 amino acids # at a given distance bin (e.g. 0-3 Angstroms) df = pd.read_csv(\u0026#34;contact_map.tsv\u0026#34;, sep=\u0026#39;\\t\u0026#39;, index_col = 0) df.head(5) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  If you\u0026rsquo;re curious:, the top fraction in the numerator represents the number of contacts between types $$i$$ and $$j$$ in distance bin $$d$$, divided by the total number of contacts between types $$i$$ and $$j$$ across all distance bins. This is then divided by the bottom fraction, where the number of contacts between all atom types at a specific distance bin $$d$$ is divided by all contacts across all distance bins.\nYou may have noticed some 0s in the data; since we\u0026rsquo;re going to compute a log later, we\u0026rsquo;ll add a pseudocount of 0.0001.\ndf = df+1e-4 df.head(5) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Now to the code We can code RAPDF as so:\nimport numpy as np def rapdf(contact_map: np.ndarray): all_contact_d = np.sum(contact_map, axis=0) all_contact_all_d = np.sum(contact_map) # this is constant per distance bin, so only calculate once! denominator = all_contact_d / all_contact_all_d scores = np.zeros(contact_map.shape) for i, row in enumerate(contact_map): # this just ensures we don\u0026#39;t divide by 0 rowsum = max(row.sum(), 1) for j, cell in enumerate(row): numerator = (cell/rowsum) scores[i][j] = -np.log(numerator/denominator[j]).round(2) return scores Now we can see it in action:\nfrom time import time t1 = time() scores = rapdf(df.values) t2 = time() print(scores, \u0026#34;This took {} miroseconds\u0026#34;.format((t2-t1)*(10**6))) [[-0.07 -0.23 0.65 -0.82 0.2 10.55] [-0.84 -0.19 0. 0.73 10.55 0. ] [-0.52 0.64 1.3 10.55 -0.54 -0.65] ... [10.73 0.48 -0.4 -0.62 -0.59 1.54] [-0.14 1.71 -0.27 -0.49 -0.57 10.88] [-0.14 -0.08 0.98 -0.24 0.24 -0.27]] This took 13474.225997924805 miroseconds  Cool, we can also use the timeit magic function to do this with more runs:\n%timeit rapdf(df.values) 7.64 ms ± 186 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  ~7.8ms (or about 7800 microseconds) isn\u0026rsquo;t bad, but this is where numba will shine. To start, we just need to apply a decorator, @numba.jit(nopython=True). This tells numba to compile the function in nopython mode, meaning maximum performance gains; otherwise, without the flag, I\u0026rsquo;ve found that the performance gains I had gotten wasn\u0026rsquo;t really worth the effort.\nimport numba # This is a decorator that tells numba to compile this function in \u0026#34;no python mode\u0026#34;. @numba.jit(nopython=True) def rapdf(contact_map: np.ndarray): all_contact_d = np.sum(contact_map, axis=0) all_contact_all_d = np.sum(contact_map) # this is constant per distance bin, so only calculate once! denominator = all_contact_d / all_contact_all_d scores = np.zeros(contact_map.shape) for i, row in enumerate(contact_map): # this just ensures we don\u0026#39;t divide by 0 rowsum = max(row.sum(), 1) for j, cell in enumerate(row): numerator = (cell/rowsum) scores[i][j] = -np.log(numerator/denominator[j]).round(2) return scores To start with, let\u0026rsquo;s use the exact same definition as above; then we can call rapdf again:\nt1 = time() scores = rapdf(df.values) t2 = time() print(scores, \u0026#34;This took {} miroseconds\u0026#34;.format((t2-t1)*(10**6))) ... TypingError: Failed in nopython mode pipeline (step: nopython frontend) Unknown attribute 'round' of type float64 File \u0026quot;\u0026lt;ipython-input-6-e7160609fabf\u0026gt;\u0026quot;, line 22: def rapdf(contact_map: np.ndarray): \u0026lt;source elided\u0026gt; numerator = (cell/rowsum) scores[i][j] = -np.log(numerator/denominator[j]).round(2) ^  This was half-expected. From experience, there will always be something that numba isn\u0026rsquo;t compatible with; in this case, the round operation. We can \u0026ldquo;get rid of it\u0026rdquo; by doing a trick:\n@numba.jit(nopython=True) def rapdf(contact_map: np.ndarray): # The rest of the function has remained identical ... for j, cell in enumerate(row): numerator = (cell/rowsum) # notice where the round operator has gone now! scores[i][j] = np.round(-np.log(numerator/denominator[j]), 2) return scores And now, will it run?\nt1 = time() scores = rapdf(df.values) t2 = time() print(scores, \u0026#34;This took {} miroseconds\u0026#34;.format((t2-t1)*(10**6))) [[-0.07 -0.23 0.65 -0.82 0.2 10.55] [-0.84 -0.19 0. 0.73 10.55 0. ] [-0.52 0.64 1.3 10.55 -0.54 -0.65] ... [10.73 0.48 -0.4 -0.62 -0.59 1.54] [-0.14 1.71 -0.27 -0.49 -0.57 10.88] [-0.14 -0.08 0.98 -0.24 0.24 -0.27]] This took 1207432.9853057861 miroseconds  Now it works! You might now be thinking, wait a minute, this was actually slower than before! Hold on a minute. The reason for this is that numba had to compile your function the first time you call it. This is where most of the time has gone in. In fact, if you now call rapdf again, you\u0026rsquo;ll see:\nt1 = time() scores = rapdf(df.values) t2 = time() print(scores, \u0026#34;This took {} miroseconds\u0026#34;.format((t2-t1)*(10**6))) [[-0.07 -0.23 0.65 -0.82 0.2 10.55] [-0.84 -0.19 0. 0.73 10.55 0. ] [-0.52 0.64 1.3 10.55 -0.54 -0.65] ... [10.73 0.48 -0.4 -0.62 -0.59 1.54] [-0.14 1.71 -0.27 -0.49 -0.57 10.88] [-0.14 -0.08 0.98 -0.24 0.24 -0.27]] This took 232.93495178222656 miroseconds  And if we do this over several runs:\n%timeit rapdf(df.values) 28.1 µs ± 341 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  Now look at that! It\u0026rsquo;s shrunk down to 28 microseconds in comparison to the \u0026gt;7 miliseconds we had before. That\u0026rsquo;s equivalent to a 250-fold speed up. You can imagine that if you had code that you\u0026rsquo;d call over and over again, this is where numba could be so useful.\n","permalink":"https://ideasbyjin.github.io/post/2021-01-30-numba/","summary":"Accelerate your Numpy code with numba","title":"Numba in action!"},{"content":"These are some Pandas tricks I use frequently; I hope it\u0026rsquo;s just as useful to you too!\nUpdate: we\u0026rsquo;ll now use iris for this (thanks Sam!):\nfrom sklearn.datasets import load_iris import pandas as pd iris = load_iris() df = pd.DataFrame( data = iris[\u0026#39;data\u0026#39;], columns = iris[\u0026#39;feature_names\u0026#39;] ) df[\u0026#39;species\u0026#39;] = iris[\u0026#39;target_names\u0026#39;][iris[\u0026#39;target\u0026#39;]] df.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Visual aids TQDM I absolutely love TQDM, partly because of how much I end up coding in IPython or a Jupyter environment. It\u0026rsquo;s always helpful to know how far along I\u0026rsquo;ve gone along in applying some function:\nfrom tqdm import tqdm tqdm.pandas() def foo(z: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; A pretty meaningless function \u0026#34;\u0026#34;\u0026#34; if z \u0026gt;= 5: return 1 else: return 0 # foo is a function to apply on a value of the column df[\u0026#39;sepal length (cm)\u0026#39;].progress_apply(lambda z: foo(z)) # watch magic happen! 100%|██████████| 150/150 [00:00\u0026lt;00:00, 81930.67it/s] 0 1 1 0 2 0 3 0 4 1 .. 145 1 146 1 147 1 148 1 149 1 Name: sepal length (cm), Length: 150, dtype: int64  Plotting! I don\u0026rsquo;t think I take advantage of this feature enough, partly because libraries like Seaborn, plotly, plotnine and Altair all work natively with Dataframe objects. But if you just want something quick, these go a long way, too:\n_ = df[\u0026#39;sepal length (cm)\u0026#39;].hist() _ = df.plot(x = \u0026#39;sepal length (cm)\u0026#39;, y = \u0026#39;sepal width (cm)\u0026#39;, kind = \u0026#39;scatter\u0026#39;) Column manipulation Strings as aggregation functions There\u0026rsquo;s loads of these, e.g. std, mean, first, etc\u0026hellip;\ndf.groupby(\u0026#34;species\u0026#34;).agg({ \u0026#34;petal length (cm)\u0026#34;: \u0026#34;mean\u0026#34;, \u0026#34;petal width (cm)\u0026#34;: \u0026#34;std\u0026#34; }) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Getting unique values Gone are the days of using something like set(df[\u0026quot;column\u0026quot;]); behold, unique and nunique are your friend!\ndf[\u0026#39;species\u0026#39;].unique() array(['setosa', 'versicolor', 'virginica'], dtype=object)  df[\u0026#39;species\u0026#39;].nunique() 3  # this also works df[df[\u0026#39;sepal length (cm)\u0026#39;] \u0026gt; 6].agg( { \u0026#34;species\u0026#34;: \u0026#34;unique\u0026#34; } ) species [versicolor, virginica] dtype: object  Slightly more efficient CSV reading/handling Only getting some columns This is a three-stage process, but it saves memory, and leads to faster reading too, which is a bonus! Notice how, without this, it can be a big TSV to read:\nPATH = \u0026#34;../gene_exp/E-MTAB-5214-query-results.tpms.tsv\u0026#34; gtex = pd.read_csv(PATH, comment=\u0026#39;#\u0026#39;, sep=\u0026#39;\\t\u0026#39;) gtex .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Let\u0026rsquo;s do this a bit better:\ndef get_col(column_name: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Simple function to filter column names \u0026#34;\u0026#34;\u0026#34; if \u0026#34;gene\u0026#34; in column_name.lower() or \u0026#34;blood\u0026#34; in column_name.lower(): return True else: return False # Get the header by reading the first line header = pd.read_csv(PATH, nrows = 1, sep=\u0026#39;\\t\u0026#39;, comment=\u0026#39;#\u0026#39;).columns # Filter the columns of interest usecols = [c for c in header if get_col(c)] # now read it gtex = pd.read_csv(PATH, usecols = usecols, comment=\u0026#39;#\u0026#39;, sep = \u0026#39;\\t\u0026#39;) gtex .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Chunkifying data I have to credit my old colleague Magda for this trick:\nimport re pattern = re.compile(r\u0026#34;^IL1[A-Z]$\u0026#34;) def filter_interleukin1(chunk): \u0026#34;\u0026#34;\u0026#34; Apply a regex to filter out chunks \u0026#34;\u0026#34;\u0026#34; return chunk[ chunk[\u0026#39;Gene Name\u0026#39;].apply(lambda z: True if pattern.findall(z) else False) ] gtex = pd.concat( [filter_interleukin1(chunk) for chunk in pd.read_csv(\u0026#34;../gene_exp/E-MTAB-5214-query-results.tpms.tsv\u0026#34;, sep=\u0026#39;\\t\u0026#39;, comment = \u0026#39;#\u0026#39;, iterator=True, chunksize=1000)] ) gtex .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Memory efficiency This is not what I do immediately, but I find that it sometimes has benefits, especially when memory is a bit precious and I have to make ends meet:\npd.to_numeric(df[\u0026#39;numeric_column\u0026#39;], downcast=\u0026#39;unsigned\u0026#39;) # only really works for positive integers pd.to_numeric(df[\u0026#39;numeric_column\u0026#39;], downcast=\u0026#39;Sparse[int]\u0026#39;) # more effective with lots of 0s df[\u0026#39;column\u0026#39;].astype(bool) # is your data full of 0s and 1s...? SQL(?) for Pandas Yes, you can call SQL via Pandas, e.g.\nconn = sqlite3.connect() # or a sqlalchemy connection... etc. pd.read_sql(\u0026#34;\u0026#34;\u0026#34;SELECT * FROM ... \u0026#34;\u0026#34;\u0026#34;, con = conn) but you can also write string queries for your Pandas data! Let\u0026rsquo;s look at iris again:\ndf.head(5) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  # you may know this as df[df[\u0026#39;species\u0026#39;] == \u0026#39;setosa\u0026#39;] df.query(\u0026#34;species == \u0026#39;setosa\u0026#39;\u0026#34;).head(10) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  I find this is slightly more readable, especially when there\u0026rsquo;s lots of conditions, such as:\n# you may know this as df[(df[\u0026#39;species\u0026#39;] == \u0026#39;setosa\u0026#39;)\u0026amp;(df[\u0026#39;sepal width (cm)\u0026#39;] \u0026lt; 3.2)] df.query(\u0026#34;species == \u0026#39;setosa\u0026#39; and `sepal width (cm)` \u0026lt; 3.2\u0026#34;).head(10) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Serialisation While TSV/CSV is the go-to format for many, I\u0026rsquo;ve found that working with them can be a bit of a pain, especially when my files get big. Some formats I\u0026rsquo;ve played with lately are Apache\u0026rsquo;s feather and Parquet formats.\nWhile they sometimes don\u0026rsquo;t offer as much compression as the humble gzip, they\u0026rsquo;re still much better at reading; remember to have pyarrow installed!\ndf.to_parquet() pd.read_parquet() Next time I\u0026rsquo;ll cover numba, which has been one of the most exciting things I\u0026rsquo;ve worked with lately.\n","permalink":"https://ideasbyjin.github.io/post/2021-01-26-pandas/","summary":"Pandas tips \u0026amp; tricks","title":"My favourite Pandas tricks"},{"content":"Our knowledge of the universe and (arguably) our modern society are built upon computer code. One would hope that said code is tested rigorously, produces consistent results, and robust to fault. As part of this wider conversation, I would argue that reproducibility - especially on different machines and computing environments - is critical. Ignoring reproducibility is the equivalent of buying an iPhone that only works in the Apple Store but as soon as it comes to your house, it breaks!\nDocker was built for ensuring reproducibility. Long story short, what\u0026rsquo;s neat about Docker is that you just have to ship your Dockerfile and some other files (e.g. requirements.txt to set up your coding environment). In theory, a script should run identically between your machine and anyone else who receives those files!\nThis post is a cheat sheet to get you up and running. Plus, the logo is an adorable whale - what\u0026rsquo;s not to like about Docker?\nTable of contents  Docker Jargon Making Dockerfiles Example Dockerfile  Toy example Bioinformatics example   Docker commands  Examples that I show here are in this repo.\nDocker Jargon    Term What is it Real-life Equivalent     Dockerfile A file that specifies how to build a Docker image Instructions on how to write a bread recipe   Docker image A file that acts as a template for Docker containers The bread recipe   Docker container Instances of the Docker image The bread    Making Dockerfiles # Define a base for importFROMIMAGE:tag# Copy files overCOPY FILENAME /PATH/TO/DEST/IN/CONTAINER/# Run commands while building the imageRUN COMMAND1 \u0026amp;\u0026amp; COMMAND2# This is the command to run when running the container.# This is a strict executable that runs unless --entrypoint is ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;, ... ]# This is the command to run but parameters are overwritableCMD [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;, ...] Example Dockerfiles Toy example Make sure to have a requirements.txt file.\nscikit-learn==0.23.1 And let\u0026rsquo;s have a simple Python script that just plots the number of objects in the iris dataset\n# iris_histogram.py from sklearn.datasets import load_iris iris = load_iris() iris_dict = dict([(i,v) for i,v in enumerate(iris[\u0026#39;target_names\u0026#39;]) ]) iris_names = [iris_dict[_] for _ in iris[\u0026#39;target\u0026#39;]] iris_counts = dict([ (t, iris_names.count(t)) for t in iris[\u0026#39;target_names\u0026#39;] ]) for k,v in iris_counts.items(): print(f\u0026#34;{k.ljust(10)}: {\u0026#39;*\u0026#39;*v}\u0026#34;) This is all we need for our Dockerfile:\nFROMpython:3.7-slim-stretchCOPY requirements.txt /RUN pip install -r requirements.txtCMD [\u0026#34;python\u0026#34;, \u0026#34;iris_histogram.py\u0026#34;]To run it, we simply need to do\n$ docker build -t iris_example -f Dockerfile . ... $ docker run iris_example setosa : ************************************************** versicolor: ************************************************** virginica : ************************************************** Bioinformatics example I\u0026rsquo;ll process a PDB file of my favourite antibody ever: an antibody that binds Sonic hedgehog\n# requirements.txt biopython==1.76 This is our basic script:\n# example.py from Bio.PDB import PDBParser p = PDBParser(QUIET=True) s = p.get_structure(\u0026#39;3mxw.pdb\u0026#39;, \u0026#39;3mxw.pdb\u0026#39;) n_c = len(list(s.get_chains())) print(f\u0026#34;This protein has {n_c} chains\u0026#34;) Our Dockerfile:\nFROMpython:3.7-slim-stretchCOPY . /RUN pip install -r requirements.txtCMD [\u0026#34;python\u0026#34;, \u0026#34;example.py\u0026#34;]And running it is as simple as:\n$ docker build -t pdb_example -f Dockerfile . ... $ docker run pdb_example This protein has 3 chains and 220 residues in chain H Docker commands Build image # docker build -t [IMAGE_TAG] -f [LOCATION_OF_DOCKERFILE] [CONTEXT_DIRECTORY] docker build -t my_image -f Dockerfile . Run image (Build a container) Default run # docker run [IMAGE TAG] docker run my_image Run image interactively (e.g. a bash shell) # docker run -it [IMAGE TAG] [COMMAND] # Spin up a new Docker container and run a bash shell docker run -it my_image bash Run a command on a running container # docker exec -it [CONTAINER TAG] [COMMAND] # Open bash on an existing, running container docker exec -it my_container bash List things # List docker images docker image ls # List containers docker ps -a Delete containers and images # Remove docker container docker rm [CONTAINER_TAG] # Remove all docker containers docker rm $(docker ps -a -q) # Remove docker image docker rmi [IMAGE_TAG] # Remove all docker images docker rmi $(docker image ls -a -q) ","permalink":"https://ideasbyjin.github.io/post/2020-05-23-docker/","summary":"A list of essential commands and examples to use Docker","title":"Docker Cheat Sheet"},{"content":"The COVID-19 Coronavirus has had a once-in-a-generation type of impact on our lives. Its devasatation has reached far and wide; whether it\u0026rsquo;s our jobs, our day-to-day lives, or even loved ones. The virus remains an enigma - some individuals seem to suffer more than others, and the virus' behaviour seems to be more and more unpredictable as the days go by. One strategy that has been hailed as an effective method to manage the crisis is testing: finding those who have been infected with the virus. Here in the UK, the government has recently turned to getting tests from Roche.\nThese diagnostic tests from Roche are serology tests: they take a sample of an individual\u0026rsquo;s blood, and test for antibodies. Hold on a minute!\n What exactly are antibodies? What does having antibodies mean? How does this help with testing for COVID-19?  In this post, I\u0026rsquo;ll do a little primer on what antibodies are, and hopefully show you why they are so cool.\nIf you have\u0026hellip;\n 30 seconds: They are Y-shaped proteins that can recognise foreign molecules. Good antibodies lead to the rest of your immune system helping to clear out that foreign molecule. 10 minutes: Go on.   NB: Unlike my other posts this one will be fairly biology heavy, you\u0026rsquo;ve been warned. I\u0026rsquo;ll have a jargon buster session at the end.\n The antibody molecule Antibodies are proteins. In humans, they look like the letter Y, as shown below:\nThe antibody molecule has two pairs of two protein chains:\n Two \u0026ldquo;heavy\u0026rdquo; chains (green) Two \u0026ldquo;light\u0026rdquo; chains (cyan)  Each chain can be sub-divided into globular units, or domains. A single chain can have:\n A variable domain (VH, VL) A constant domain (CH, CL)  The two variable domains from each chain (VH+VL) form the Fv region. Within the Fv are six loops, known as the CDR loops.\nThis is key: the amino acid sequence (and thus the shape) of the CDR loops determines whether an antibody can bind things, e.g. the COVID-19 virus, with sufficient strength. Put another way, I like to think of the entire Fv region as being similar to our hands, and the CDR loops are like our fingers. Depending (largely) on the shape of your fingers, you can hold on to different sized objects.\nFor example, this is the molecular structure of an antibody binding its target molecule, also known as the antigen:\nNotice that the pink blob (antigen) is in close contact with the CDR loops (coloured red, yellow, purple\u0026hellip;). Furthermore, as the antibody has two Fv regions, it can bind to two antigens simultaneously. One more thing to mention here is that the combined set of CDR loops recognise a particular spot on the antigen, known as the epitope. This means that, in theory, that pink blob can be recognised by lots of different antibodies throughout its entire surface.\nThe antibody\u0026rsquo;s origins Antibodies are synthesised from a type of white blood cell called the B-lymphocyte. To cut a couple weeks' and textbook chapters short, every person has billions of B-cells, each of which produces one antibody. Each B-cell performs a process known as V(D)J recombination to randomly stitch together parts of the genome to produce an antibody:\nTo start with, an antibody starts its life as a protein on the surface of the B-cell (technically called a B-cell receptor). When a circulating B-cell comes in contact with its target antigen, the B-cell converts the B-cell receptor to a soluble form that circulates in the bloodstream. This entire process takes a few weeks after the initial infection.\nAltogether, if COVID-19 binding antibodies are detected in an individual, one can assume that they have been infected with the virus a couple of weeks prior to the time of taking a test.\nWhat do antibodies do again? Basically antibodies bind things. What\u0026rsquo;s important is what happens afterward. Antibodies trigger a series of responses from the immune system after recognising the antigen.\nBroadly speaking, in all three cases, the antibody acts as a \u0026ldquo;flag\u0026rdquo; that tells other parts of the immune response to do something about the antigen. Against viruses, antibodies can also stop them from exiting an infected human cell. In fact, the range of \u0026ldquo;healthy\u0026rdquo; responses mediated by antibodies is discussed in this paper by Tay et al.\nAre all antibodies equal? Not quite. In some unfortunate cases, antibodies might facilitate viral entry, as shown in panel B below:\nThis phenomenon, known as antibody-directed enhancement, has been documented in that paper by Tay et al. that I described above. Other reviews like this one by de Alwis et al speculate that antibodies can cause hyper-active immune responses, which can also be detrimental.\nThen how do these antibody tests work? Let\u0026rsquo;s recap everything discussed above:\n Antibodies have two pairs of heavy-light chain pairs, leading to two identical binding sites (Fv regions) An antibody comes from a B-cell after infection, and is then released to the bloodstream Antibodies can trigger immune responses in individuals, but sometimes those antibodies may not be so useful  Diagnostic tests like those from Roche leverage the first two points. It looks to see if a patient has developed antibodies against COVID-19 (whether it\u0026rsquo;s protective or not). This is done by a \u0026ldquo;double antigen sandwich\u0026rdquo;:\nEssentially, the test counts on the fact that a single antibody molecule can bind two antigens - one on each Fv. Each antigen is either:\n Hooked up to a biotin molecule (which acts as an anchor to hold antibodies in place), or It is ruthenylated, making the antigen \u0026ldquo;light up\u0026rdquo; in the testing kit  I\u0026rsquo;m deliberately skipping a lot of the details of the test, but these are the basic biological details. While the test provides valuable information for whether or not you have antibodies against COVID-19, the test does not confirm whether you are immune to the virus. To confirm this, other tests would be necessary. For example, antibodies would have to be isolated and tested to see if they can neutralise the virus in a separate lab experiment.\nWrap-up I think what is extremely remarkable about antibodies is that they are natural, in-built defence mechanisms. Almost magically, they can protect us from viruses that we have never seen before, like COVID-19. By understanding their biological structure and mechanism, this has allowed us to develop instrumental tools like Roche\u0026rsquo;s diagnostic tests.\nThis post was intended to be a basic primer that (hopefully) landed somewhere between a Guardian article and a journal article (so, a textbook I guess?) There is still so much more we can talk about in this space, whether it\u0026rsquo;s:\n How do we design external antibodies to fight the virus? What makes antibodies go bad? How can artificial intelligence approaches alongside antibodies help to fight COVID-19?  But perhaps that\u0026rsquo;s for another time.\nAcknowledgements / References For any uncited figures, they are almost all from my PhD thesis. The exception is the picture of an antibody binding two pink blobs, which I made for a presentation.\nOtherwise I\u0026rsquo;ve referred to the figures directly and I\u0026rsquo;m incredibly grateful for scientific illustrators and creators at NPG and Roche. Good illustrations go a long way to helping everyone understand how things work!\n","permalink":"https://ideasbyjin.github.io/post/2020-05-16-antibodies/","summary":"A primer on antibodies","title":"What are antibodies, anyway?"},{"content":"I\u0026rsquo;ve been asked a really simple question since my last post on distributions.\n What do you do with them? How do they help you in real world applications?\n These are both really good questions, so I\u0026rsquo;ll use this post today to discuss:\n What you can do with the hypergeometric distribution (a discrete case) What you can do with the beta distribution (a continuous case)  If you have\u0026hellip;\n 30 seconds: knowing the various PMFs and PDFs is super handy, and allow you to make neat inferences, but they are ultimately \u0026ldquo;models\u0026rdquo;. 10 minutes: read on.  # Import some stuff import scipy.stats as sp import matplotlib.pyplot as plt import seaborn as sns import numpy as np # Make things pretty. sns.set_style(\u0026#34;ticks\u0026#34;) sns.set_context(\u0026#34;paper\u0026#34;) The Discrete Case: the hypergeometric distribution Recall that the hypergeometric distribution is a probability mass function (PMF). It is used to describe scenarios where we sample \u0026ldquo;type I\u0026rdquo; objects from a population. The canonical example is:\n I have a bag containing K white balls and N-K black balls. If I sample n balls from the bag, how many of these are white (k)?\n Thus, the hypergeometric distribution represents the probability that $$k$$ takes on a specific value.\nThe hypergeometric virus Let\u0026rsquo;s suppose we live in a town with 1000 people. We know from official statistics that 10 people have been diagnosed with COVID-19.\n Disclaimer: this post is intended to be an example, providing a more intuitive guide on how statistical models can help us think about this huge global problem. This is not an infectious disease modelling exercise, and in fact the post assumes the virus is not transmissible. We know that\u0026rsquo;s definitely not the case. Please stay home.\n With the lockdown / social distancing in place, you decide to use your \u0026ldquo;exercise once a day\u0026rdquo; credit for a walk in the local park. However, you see 20 other individuals in the park. You may then ask,\n Can I estimate how many of these 20 individuals have COVID-19?\n Intuitively, we know that the chances of bumping into one of those ten individuals with COVID-19 in this town is low, let alone two, or even three! Furthermore, what are the chances that this would happen in the park? Put another way, we can visualise the distribution as a table:\n    Has COVID-19 Doesn\u0026rsquo;t have COVID-19 Total     Park k n-k n   Not the park K-k N-K-n+k N-n   Total K N-K N    This is when having an understanding of probability distributions, in particular the hypergeometric distribution, can be very handy. By understanding that the hypergeometric distribution is an appropriate model for this type of data, we can then do two further analyses:\n Illustrate the probability of seeing $$k$$ individuals with COVID-19 in the park Perform hypothesis tests  1. Illustration of the probability space # Let\u0026#39;s draw the probability mass function of the hypergeometric distribution def hypergeometric_draw(k, K = 10, n = 20, N = 1000): \u0026#34;\u0026#34;\u0026#34; Return the hypergeometric PMF at k. :param: k: number of \u0026#34;successful\u0026#34; cases in the sample :param: K: total number of \u0026#34;successful\u0026#34; cases :param: n: sample size :param: N: population size \u0026#34;\u0026#34;\u0026#34; # The scipy notation for the hypergeometric distribution is ... a little odd. return sp.hypergeom.pmf(k, n = K, N = n, M = N) # Let\u0026#39;s visualise the probability space for seeing 0 to 10 COVID-19 patients # in our sample of 20 individuals in the park. n_vector = np.arange(0, 11, 1) fig, ax = plt.subplots() # We\u0026#39;ll plot the hypergeometric distribution PMF sns.barplot( n_vector, list(map(hypergeometric_draw, n_vector)), ax = ax ) sns.despine() ax.set_xlabel(\u0026#34;$k$\u0026#34;) ax.set_ylabel(\u0026#34;$Pr(X = k)$\u0026#34;) _ = ax.set_title(\u0026#34;PMF of sampling $k$ COVID-19 individuals\\nin a sample of 20 people from a population of 1000\u0026#34;) plt.savefig(\u0026#34;hypergeometric_pmf.png\u0026#34;, dpi = 300) You\u0026rsquo;ll notice two things here:\n In this hypothetical town, it is most likely (~80% probability) that none of those 20 individuals in the park had COVID-19. The chance that one of those 20 has COVID-19 is around 15%. Anything more, the probability becomes almost negligible.  2. A hypothesis test with the hypergeometric distribution I\u0026rsquo;ll assume you know what hypothesis tests are in this section, though a brief primer is provided at the Appendix below. Given our scenario described above, we can formulate the following hypotheses:\n The null hypothesis: the number of COVID-19 patients is distributed randomly between the park and everywhere else, according to the hypergeometric distribution, as described in the table above. The alternative hypothesis: the number of COVID-19 patients is higher in the park than elsewhere.  Going with our previous numbers of:\n 1000 people in the whole town 10 of whom have COVID-19 20 people were seen in the park  \u0026hellip;suppose that two of those 20 individuals in the park were tested positive for COVID-19.\nHypothesis tests then seek to quantify\n How significant is this observation assuming that the null hypothesis is true?\n Estimating the p-value is equivalent to getting the \u0026ldquo;area under the curve\u0026rdquo; of the hypergeometric distribution described above.\n And in our example, it can be obtained by 1 - CDF(k-1) where $$k$$ is 2.\n # Number of confirmed COVID-19 patients out of the 20 k = 2 # P-value calculation p_value = 1 - sp.hypergeom.cdf(k-1, n = 10, N = 20, M = 1000) print(f\u0026#34;The p-value is: {p_value}\u0026#34;) fig, ax = plt.subplots() # Inset axis plot - inspired by https://scipython.com/blog/inset-plots-in-matplotlib/ from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset # Create an inset that is 40%/70% of the original plot size with some padding. inner_ax = inset_axes(ax, \u0026#34;40%\u0026#34;, \u0026#34;70%\u0026#34; ,loc=\u0026#34;lower right\u0026#34;, borderpad=3) n_vector = np.array(n_vector) pmf_hypergeom = np.array(list(map(hypergeometric_draw, n_vector))) # Plot the barplot of the main axis sns.barplot( n_vector, pmf_hypergeom, ax = ax ) # Plot the barplot of the inset sns.barplot( n_vector, pmf_hypergeom, ax = inner_ax, palette = [\u0026#39;#e3e3e3\u0026#39; if _ \u0026lt; k else \u0026#39;#ffd700\u0026#39; for _ in n_vector ] ) inner_ax.set_xlim((1.5, 5.5)) inner_ax.set_ylim((0, 0.02)) ax.set_title(\u0026#34;Hypergeometric Distribution PMF\u0026#34;) inner_ax.set_title(\u0026#34;p-value: sum of all the yellow bars\u0026#34;) mark_inset(ax, inner_ax, loc1 = 2, loc2 = 4, fc = \u0026#39;none\u0026#39;, ec = \u0026#39;0.7\u0026#39;, linestyle = \u0026#39;--\u0026#39;) sns.despine() plt.savefig(\u0026#34;inset_hypergeometric.png\u0026#34;, dpi = 300) The p-value is: 0.015542413797821064  The p-value for this test is a low 0.0155! This means that, if the distribution of individuals with COVID-19 was indeed random , and following the hypergeometric distribution, the probability of seeing two (or more) COVID-19 patients in the park is 0.0155.\nThis gives us a safe bet to reject the null hypothesis. Now let\u0026rsquo;s cover what we can do with knowing the distributions of continuous random variables.\nThe Continuous Case: the beta distribution Recall that the beta distribution is a probability distribution function (PDF). It is used to describe random variables whose values lie within 0 and 1. This makes it a pretty good choice for sampling percentages, probabilities, etc.\nLet\u0026rsquo;s get some baskets (Inspired by Variance Explained) Let\u0026rsquo;s switch topics for a second.\nThe COVID-19 lockdown has put the NBA season on hold, and the season has shut down. You\u0026rsquo;re the general manager and tasked with drafting a player for the new season. We\u0026rsquo;re considering drafting LaMelo Ball, who\u0026rsquo;s currently playing in Australia (who knew?!)\nIt turns out that there isn\u0026rsquo;t that much data available for LaMelo Ball, so it\u0026rsquo;s kind of difficult to extrapolate how good of a scorer he\u0026rsquo;ll be when he comes to the NBA. One of the numbers that represent a player\u0026rsquo;s scoring efficiency is the field goal (FG) percentage, which is simply\n$$ FG% = \\dfrac{\\textrm{Shots Made}}{\\textrm{Shots Attempted}} $$\nThe beta distribution is an appropriate model for estimating LaMelo\u0026rsquo;s FG percentage as it is a value between 0 and 1! With the beta distribution, we can:\n Illustrate a possible distribution of LaMelo\u0026rsquo;s true FG percentage Perform Bayesian inference  For this experiment we\u0026rsquo;ll just use LaMelo\u0026rsquo;s NBL numbers, but in the Appendix I\u0026rsquo;ll explain some limitations of using this type of data.\n1. Illustration of the probability space Recall that the beta distribution is appropriate for representing random variables that are like percentages, and influenced by two shape parameters: $$\\alpha$$ and $$\\beta$$.\nWhat is the interpretation behind $$\\alpha$$ and $$\\beta$$? What\u0026rsquo;s even an appropriate set of values for $$\\alpha$$ and $$\\beta$$ for this problem? Well, we can rely on two hints:\n Beta distributions with high $$\\alpha$$ and low $$\\beta$$ are negative-skewed (larger probabilities $$\\rightarrow$$ higher values of the PDF), and vice-versa. The expected value (average) of a beta distribution is  $$\\mathbb{E} [ X ] = \\dfrac{\\alpha}{\\alpha+\\beta}$$\nLook familiar? The $$\\alpha$$ represents shots made and $$\\beta$$ shots missed. In LaMelo\u0026rsquo;s case, since he made 75 and attempted 199 (thus missing 124) in his single NBL season, we can use that to parameterise the beta distribution.\n# Let\u0026#39;s draw the probability distribution function of the beta distribution def beta_draw(x, a, b): \u0026#34;\u0026#34;\u0026#34; Return the beta distribution PDF at k. :param: x: value between 0 and 1 :param: a, b: shape parameters \u0026#34;\u0026#34;\u0026#34; # The scipy notation for the hypergeometric distribution is ... a little odd. return sp.beta.pdf(x, a = a, b = b) fig, ax = plt.subplots() possible_percentages = np.arange(0.2, 0.51, 0.005) fg_made = 75 fg_missed = 199-75 beta_vals = list(map(lambda x: beta_draw(x, a = fg_made, b = fg_missed), possible_percentages)) max_value = possible_percentages[np.argmax(beta_vals)] print(\u0026#34;The FG% with the highest density is {:.2f}\u0026#34;.format(max_value)) # We\u0026#39;ll plot the beta distribution PDF sns.lineplot(possible_percentages, beta_vals, ax = ax) plt.axvline(max_value, ymin = 0.05, ymax = 0.95, color = \u0026#39;#e8291c\u0026#39;, linestyle = \u0026#39;--\u0026#39;) sns.despine() # Some decorative stuff ax.set_xlabel(\u0026#34;FG%\u0026#34;) ax.set_ylabel(\u0026#34;Density\u0026#34;) _ = ax.set_title(\u0026#34;PDF of LaMelo Ball\u0026#39;s field goal percentage\u0026#34;) plt.savefig(\u0026#34;beta_pdf.png\u0026#34;, dpi = 300) The FG% with the highest density is 0.38  2. Bayesian Inference on Field Goals Now that we know the beta distribution of LaMelo\u0026rsquo;s FG percentage, we can use it as a prior for Bayesian inference. Recall that Bayesian inference can be formulated as shown below;\n$$ Posterior(FG%|Data) \\propto Prior(FG%) \\times Likelihood(Data) $$\nThis framework allows us to estimate the true field goal percentage by combining our prior distribution with some observed data. (Side note: this observed data would come from a binomial distribution!)\nHowever, as we don\u0026rsquo;t have observations, let\u0026rsquo;s generate some random numbers as our \u0026ldquo;observed\u0026rdquo; FGs that are made over 82 games of a \u0026ldquo;typical\u0026rdquo; NBA season.\nimport pymc3 as pm # LaMelo attempts about 16 shots a game (199 shots / 12 games) # As a rookie, he\u0026#39;ll probably attempt fewer shots over an 82 game season, so let\u0026#39;s use 12. np.random.seed(42) pseudo_obs = sp.randint.rvs(0, 12, size = 82) with pm.Model() as model: # Establish the prior field_goal_percentage = pm.Beta(\u0026#34;FG\u0026#34;, alpha = 75, beta = 199-75) # Set the binomial distribution to model baskets made with 12 shots attempted per game field_goal_data = pm.Binomial(\u0026#34;Baskets\u0026#34;, n = 12, p = field_goal_percentage, observed = pseudo_obs) # Run the metropolis-hastings algorithm step = pm.Metropolis() # Sample the trace trace = pm.sample(10000, step = step, cores=1) Sequential sampling (2 chains in 1 job) Metropolis: [FG] Sampling chain 0, 0 divergences: 100%|██████████| 10500/10500 [00:01\u0026lt;00:00, 8902.08it/s] Sampling chain 1, 0 divergences: 100%|██████████| 10500/10500 [00:01\u0026lt;00:00, 9060.07it/s] The number of effective samples is smaller than 10% for some parameters.  Now let\u0026rsquo;s visualise the posterior after the PyMC run:\nfig, (ax, ax2) = plt.subplots(1,2) # Plot the pseudo-observations to give context ax.hist(pseudo_obs, bins = np.arange(0,13,2), color = \u0026#39;#77dd77\u0026#39;) ax.set_title(\u0026#34;Distribution of pseudo-observed field goals\u0026#34;) # Prior distribution possible_percentages = np.arange(0.2, 0.55, 0.001) prior = list(map(lambda x: beta_draw(x, a = 75, b = 199-75), possible_percentages)) # Plot the prior ax2.plot(possible_percentages, prior, label = \u0026#39;Prior distribution\u0026#39;) # Plot the posterior posterior_density = [sp.gaussian_kde(trace[\u0026#39;FG\u0026#39;]).evaluate(_) for _ in possible_percentages] ax2.plot(possible_percentages, posterior_density, label = \u0026#34;Posterior distribution\u0026#34;) max_value_prior = possible_percentages[np.argmax(prior)] max_value_posterior = possible_percentages[np.argmax(posterior_density)] print(\u0026#34;The FG% with the highest density in the prior distribution is {:.2f}\u0026#34;.format(max_value_prior)) print(\u0026#34;The FG% with the highest density in the posterior distribution is {:.2f}\u0026#34;.format(max_value_posterior)) sns.despine() ax2.set_xlim((0.25, 0.55)) ax2.set_xlabel(\u0026#34;FG%\u0026#34;) ax2.set_ylabel(\u0026#34;Density\u0026#34;) _ = ax2.set_title(\u0026#34;PDF of LaMelo Ball\u0026#39;s FG percentage\u0026#34;) _ = ax2.legend(loc=\u0026#39;upper left\u0026#39;) fig.set_size_inches((10,5)) plt.savefig(\u0026#34;posterior_distribution.png\u0026#34;, dpi = 300) The FG% with the highest density in the prior distribution is 0.38 The FG% with the highest density in the posterior distribution is 0.47  Hopefully this gives you a brief insight into how knowing different probability distributions can be useful for an aspiring data scientist.\nIn practice, data doesn\u0026rsquo;t necessarily conform to a known probability distribution\u0026rsquo;s shape - for example, data can be bi-modal (traffic jams tend to happen closer to 9AM and just after 5PM). Probability distributions are intended to guide your reasoning more than anything else! Have fun with it, experiment, and learn.\nAppendix What are hypothesis tests? You may have heard of tests like the $$t$$-test before; tests like these are used to measure the significance of some observed lab results.\nHypothesis tests aim to quantify the extremity of a particular observation under the assumption that some \u0026ldquo;null\u0026rdquo; hypothesis is true. The extremity is measured as a probability \u0026ndash; the beloved (or despised) p-value.\nTo break it down:\n   Term Definition Example(s)     Hypothesis test A test to measure extremity of an observation $$t$$-test, $$\\chi^2$$ tests, ANOVA\u0026hellip;.   Null hypothesis Some default position regarding the nature of the random variable \u0026ldquo;The data is distributed randomly\u0026rdquo;   Alternative hypothesis Usually counteracts the null \u0026ldquo;The data is not distributed randomly\u0026rdquo;    Hypothesis tests are defined by the probability distribution of the null hypothesis. For example, the $$t$$-test is the $$t$$-test because the null hypothesis assumes that the $$t$$ statistic follows Student\u0026rsquo;s $$t$$-distribution. Similarly, the $$\\chi^2$$ test for independence assumes that the test statistic follows a $$\\chi^2$$ distribution.\nThe LaMelo ball experiment While there isn\u0026rsquo;t much data out there for LaMelo Ball, we know that his brother Lonzo is in the NBA! Assuming that, as point guards, both he and his brother Lonzo play similarly, we can potentially combine the data from Lonzo and LaMelo to estimate LaMelo\u0026rsquo;s field goal percentage in the NBA.\nHowever, we would have to\u0026hellip;\n Assume that Lonzo and LaMelo play so similarly that their field goal percentages should be more or less similar. Assume that it is just as easy to get a field goal in the NBA and the NBL Assume that season-by-season field goal percentages are more or less the same  Once those assumptions are OK to take, we can then use LaMelo\u0026rsquo;s Australian NBL field goal percentage as a prior, then use Lonzo\u0026rsquo;s NBA numbers to compute a posterior distribution.\n","permalink":"https://ideasbyjin.github.io/post/2020-04-14-distributions-applications/","summary":"This post shows how we can use a PMF and PDF to some toy problems.","title":"Example applications of probability distributions"},{"content":"In this post, I wanted to provide a primer / cheat-sheet on probability distributions to equip the community with a toolkit to tackle a wide range of data science problems \u0026ndash; for example, the current ongoing COVID-19 crisis.\n Disclaimer: please do not assume that COVID-19 behaves as simply as the probability distributions below.\n If you have\u0026hellip;\n 30 seconds: a table of distributions and some use cases are shown below. 2 minutes:  Random variables can be discrete or continuous. Probability mass functions are used to explain discrete random variables; examples include the poisson distribution, binomial distribution, and the hypergeometric distribution. Probability density functions are used to explain continuous random variables; examples include the uniform distribution, the normal distribution, and the beta distribution.   7 minutes: go on.  Distributions of discrete variables We\u0026rsquo;re going to draw 1000 samples from the following distributions.\n   Name Used to\u0026hellip; Example use case Parameters     Binomial Represent number of successes of $$n$$ trials How many heads will I get from $$n$$ coin tosses? $$p$$: probability of \u0026ldquo;success\u0026rdquo;; $$n$$: number of trials   Geometric Represent number of trials until first success How many tails will I see until the first head? $$p$$: probability of success   Negative binomial Represent number of failures before we see $$n$$ successful binary events How many tails will I see before I see $$n$$ heads? $$p$$: probability of success; $$n$$: number of successes   Poisson Represent number of successesful events where we have infinite trials How many red buses will I see at my bus stop today? $$\\lambda$$: rate of success   Hypergeometric Represent number of type $$I$$ objects drawn from a sample of size $$n$$ without replacement from a population $$N$$ From a bag with 10 red balls and 90 blue balls, I will randomly sample 20 balls; how many of these are red? $$K$$: total number of \u0026ldquo;successful\u0026rdquo; cases (i.e. type $$I$$ objects); $$n$$: sample size; $$N$$: total number of objects    Distributions of continuous variables We\u0026rsquo;re going to draw 1000 samples from the following distributions.\n   Name Used to\u0026hellip; Example use case Parameters     Uniform Represent a random variable whose value occurs between $$a$$ and $$b$$ The hyper-parameter of a model, $$\\theta$$, where $$\\theta$$ can take any value between $$a$$ and $$b$$ $$a$$: minimum value; $$b$$: maximum value   Beta Represent the distribution of a random variable whose value is between 0 and 1 Estimating the distribution of a probability (e.g. free throw percentage) Shape parameters $$\\alpha$$ and $$\\beta$$. (*NB: A beta distribution with $$\\alpha = \\beta = 1$$ is the same as a uniform distribution with $$a = 0, b = 1$$.)   Exponential Represent a random variable that decays exponentially How long do I have to wait until the first success? (The \u0026ldquo;continuous version\u0026rdquo; of the geometric distribution) $$\\lambda$$: rate parameter   Gamma Represent a random variable that decays exponentially How long do I have to wait until the $$k$$th success? $$k$$: shape parameter; $$\\theta$$: scale parameter; OR $$\\alpha$$: shape parameter (same as $$k$$); $$\\beta$$: rate parameter where $$\\beta = 1/\\theta$$   Normal Represent the behaviour of most continuous random variables (e.g. height, scores, protein mass, etc.) Captures many types of continuous variables and is usually a good approximation for most use cases. Represent quantities that follow a \u0026ldquo;bell curve\u0026rdquo; distribution $$\\mu$$: expected value of the random variable; $$\\sigma$$: standard deviation   von Mises distribution Represent \u0026ldquo;circular\u0026rdquo; random variables Sampling torsion angles of amino acids $$\\mu$$: central location (radians); $$\\kappa$$: dispersion   t-distribution Represents the distribution of the sample means from a normally distributed random variable t-tests df: degrees of freedom   $$\\chi^2$$ distribution Represents the sum of the squares of $$k$$ normally-distributed random variables Used for the a \u0026ldquo;goodness of fit\u0026rdquo; test or independence of categorical variables df: degrees of freedom   Dirichlet distribution Represent the probability distribution space of $$K$$ events If an event has $$K$$ possibilities, what would be its possible probability distributions? $$\\alpha$$: a $$K$$-sized vector of numbers    Let\u0026rsquo;s start by defining what a random variable is\u0026hellip;\n A random variable is a real-valued function that maps probability events to measurable values.\n Random variables are also considered to either be discrete or continuous. We\u0026rsquo;ll first start with the discrete case.\nDiscrete random variables and probability mass functions The definition of a random variable can sound a bit vague; the oft-quoted \u0026ldquo;coin toss\u0026rdquo; provides a good way to understand random variables.\n The \u0026ldquo;event space\u0026rdquo; is whether a coin is heads or tails. A sequence of heads/tails is thus the sequence of events. Thus, a \u0026ldquo;random variable\u0026rdquo; can be used to express the number of heads in that sequence.  These types of random variables are known as discrete random variables. Other examples include:\n The sum of two dice that are rolled The number of objects drawn from a bag The number of days elapsed between visits to the grocery store  As a rule of thumb, if you can count it / it\u0026rsquo;s an integer, it\u0026rsquo;s likely to be discrete.\nSince discrete random variables can only take on a limited set of values, it is thus possible to calculate the probability space of that random variable.\n Consider a traditional six-sided die. We know that there are six and only six possible faces, so we can calculate the probability of landing on one of those six faces.\n Thus, the probability of the discrete random variable $$X$$ taking some value $$x$$, $$Pr(X = x)$$, can be calculated by a probability mass function (PMF). Different types of discrete random variables have different PMFs. We can also visualise the probability space of PMFs using histograms.\n# Import stuff import numpy as np import scipy.stats as sp import seaborn as sns import matplotlib.pyplot as plt sns.set_style(\u0026#34;ticks\u0026#34;) sns.set_context(\u0026#34;paper\u0026#34;) # Let\u0026#39;s generate some random numbers np.random.seed(42) # 100 tosses; 20% chance of success. Do 1000 such trials. binomial = sp.binom.rvs(n = 100, p = 0.2, size = 1000) binomial_bins = np.arange(10, 41, 5) # How many trials will we need until we see the first success? 20% chance of success. Run this 1000 times. geometric = sp.geom.rvs(p=0.2, size = 1000) geometric_bins = np.arange(0, 31, 5) # How many failures will we see before the 10th success? Success rate of 40%. Run this experiment 1000 times. nbinomial = sp.nbinom.rvs(n = 10, p = 0.4, size = 1000) nbinomial_bins = np.arange(0, 11, 1) # How many red buses will we see over 1000 days if they come at a rate of 0.4? # rate (mu) = n x p where n is infinite trials, and p is the probability of success. poisson = sp.poisson.rvs(mu = 0.4, size = 1000) poisson_bins = np.arange(0, 6, 1) # 10 red balls in a bag of 100. If I sample 20, how many red balls will I get? Run this experiment 1000 times. # Scipy\u0026#39;s notation is a bit confusing hypergeometric = sp.hypergeom.rvs(M = 100, n = 10, N = 20, size = 1000) hypergeometric_bins = np.arange(0, 11, 1) fig, axes = plt.subplots(2,3) ax = axes.flatten() distributions = [ (\u0026#34;Binomial\u0026#34;, binomial, binomial_bins, \u0026#39;dodgerblue\u0026#39;, \u0026#39;n = 100, p = 0.2\u0026#39;), (\u0026#34;Geometric\u0026#34;, geometric, geometric_bins, \u0026#39;g\u0026#39;, \u0026#39;p = 0.2\u0026#39;), (\u0026#34;Negative binomial\u0026#34;, nbinomial, nbinomial_bins, \u0026#39;r\u0026#39;, \u0026#39;n_success = 10, p = 0.4\u0026#39;), (\u0026#34;Poisson\u0026#34;, poisson, poisson_bins, \u0026#39;orange\u0026#39;, \u0026#39;$\\lambda$ = 0.4\u0026#39;), (\u0026#34;Hypergeometric\u0026#34;, hypergeometric, hypergeometric_bins, \u0026#39;purple\u0026#39;, \u0026#39;n_target = 10, n_sample = 20, n_total = 100\u0026#39;) ] for i,v in enumerate(distributions): name, the_dist, the_dist_bins, c, params = v sns.distplot(the_dist, ax = ax[i], kde = False, color = c, bins = the_dist_bins) ax[i].set_title(f\u0026#34;{name} distribution\\n{params}\u0026#34;) plt.subplots_adjust(hspace = 0.5) sns.despine() fig.delaxes(ax[i+1]) fig.set_size_inches((10,8)) plt.savefig(\u0026#34;discrete_distributions.png\u0026#34;, dpi = 300) Continuous random variables and probability distribution functions Well, what about random variables that are not countable / integers? Such as\u0026hellip;\n The height of individuals Time elapsed between events (remember, you can have miliseconds, microseconds\u0026hellip;) Batting averages of baseball players  As a rule of thumb, if a random variable can take a decimal value, it\u0026rsquo;s likely to be continuous.\nUnlike discrete random variables, we cannot calculate the probability that $$X$$ is going to an exact value $$x$$. This is because we cannot enumerate all the possible values of the random variable, and thus, the probability of $$X = x$$ is 0.\n Think about in practice: what is the probability that the temperature tomorrow will be precisely 23.000000 degrees Celsius (73.400000F for Americans)? Another example, what is the probability that someone will be precisely 174.714 centimeters tall (5'8)?\n Since a continuous random variable, by definition, is uncountable, we account for them using probability distribution functions (PDFs). While they might sound similar to PMFs, PDFs do not represent probabilities, but probability densities.\n Another way of thinking about the density is that the density represents a form of relative likelihood. The higher the density, the more likely that the random variable takes on that particular value.\n For PDFs, since the probability of an exact value is 0, we typically calculate it in intervals. e.g. the probability that $$X$$ is less than or equal to $$x$$ is represented by $$Pr(X \\leq x)$$. In other words, if a random variable has the probability distribution function $$f(x)$$, the probability $$Pr(X \\leq x)$$ is\n$$Pr(X \\leq x ) = \\int_{-\\infty}^{x} f(x) dx $$\n# Let\u0026#39;s generate some random numbers np.random.seed(42) # Uniform distribution between 0,1  uniform = sp.uniform.rvs(0, 1 ,size=1000) uniform_bins = np.arange(0, 1.01, 0.01) # Beta distribution but with four different cases; a = b = 1; a \u0026gt; b; b \u0026gt; a; a \u0026lt; 1, b \u0026lt; 1, a = b;  beta_1 = sp.beta.rvs(a = 1, b = 1, size = 1000) beta_2 = sp.beta.rvs(a = 5, b = 1, size = 1000) beta_3 = sp.beta.rvs(a = 1, b = 5, size = 1000) beta_4 = sp.beta.rvs(a = 0.5, b = 0.5, size = 1000) beta_bins = uniform_bins.copy() # Still between 0 and 1 # How much time will elapse until the first success? Play with different scale values expon_1 = sp.expon.rvs(scale = 1, size = 1000) expon_2 = sp.expon.rvs(scale = 0.5, size = 1000) expon_3 = sp.expon.rvs(scale = 4, size = 1000) expon_bins = np.arange(0, 10.1, 0.01) # How much time until the ath event? Plot different cases; three scale values, two k values gamma_1 = sp.gamma.rvs(a = 1, scale = 1, size = 1000) gamma_2 = sp.gamma.rvs(a = 1, scale = 0.5, size = 1000) gamma_3 = sp.gamma.rvs(a = 1, scale = 4, size = 1000) gamma_4 = sp.gamma.rvs(a = 3, scale = 1, size = 1000) gamma_5 = sp.gamma.rvs(a = 3, scale = 0.5, size = 1000) gamma_6 = sp.gamma.rvs(a = 3, scale = 4, size = 1000) gamma_bins = expon_bins.copy() # Plot the good old\u0026#39; standard normal distribution, which is king! normal = sp.norm.rvs(size=1000) normal_bins = np.arange(-3, 3.01, 0.01) # Plot the circular normal, or von mises, distribution vm = sp.vonmises.rvs(kappa = 1, size=1000) vm_bins = np.arange(-np.pi, np.pi+0.01, 0.1) # t-distribution and chi-squared distributions with 5 degrees of freedom t = sp.t.rvs(df=5, size = 1000) t_bins = normal_bins.copy() chi = sp.chi2.rvs(df = 5, size = 1000) chi_bins = np.arange(0, int(max(chi))+1, 1) # Dirichlet distribution for 3-event space with concentration parameters of 1,1,1 dirichet = sp.dirichlet.rvs([1,1,1], size = 1000) distributions = [ (\u0026#34;Uniform\u0026#34;, [uniform], uniform_bins, [\u0026#39;dodgerblue\u0026#39;], [\u0026#39;a = 0, b = 1\u0026#39;]), (\u0026#34;Beta\u0026#34;, [beta_1, beta_2, beta_3, beta_4], beta_bins, [\u0026#39;dodgerblue\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;yellow\u0026#39;, \u0026#39;green\u0026#39;], [\u0026#39;$\\\\alpha = 1, \\\\beta = 1$\u0026#39;, \u0026#39;$\\\\alpha = 5, \\\\beta = 1$\u0026#39;, \u0026#39;$\\\\alpha = 1, \\\\beta = 5$\u0026#39;, \u0026#39;$\\\\alpha = 0.5, \\\\beta = 0.5$\u0026#39;]), (\u0026#34;Exponential\u0026#34;, [expon_1, expon_2, expon_3], expon_bins, [\u0026#39;dodgerblue\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;yellow\u0026#39;], [\u0026#39;$\\lambda = 1$\u0026#39;, \u0026#39;$\\lambda = 1/0.5 = 2$\u0026#39;, \u0026#39;$\\lambda = 1/4 = 0.25$\u0026#39;]), (\u0026#34;Gamma\u0026#34;, [gamma_1, gamma_2, gamma_3, gamma_4, gamma_5, gamma_6], gamma_bins, [\u0026#39;dodgerblue\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;yellow\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;purple\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;brown\u0026#39;], [\u0026#39;$\\\\alpha = 1, \\\\theta = 1$\u0026#39;, \u0026#39;$\\\\alpha = 1, \\\\theta = 0.5$\u0026#39;, \u0026#39;$\\\\alpha = 1, \\\\theta = 4$\u0026#39;, \u0026#39;$\\\\alpha = 3, \\\\theta = 1$\u0026#39;, \u0026#39;$\\\\alpha = 3, \\\\theta = 0.5$\u0026#39;, \u0026#39;$\\\\alpha = 3, \\\\theta = 4$\u0026#39;]), (\u0026#34;Normal\u0026#34;, [normal], normal_bins, [\u0026#39;dodgerblue\u0026#39;], [\u0026#39;$\\mu = 0, \\sigma = 1$\u0026#39;]), (\u0026#34;von Mises\u0026#34;, [vm], vm_bins, [\u0026#39;red\u0026#39;], [\u0026#39;$\\mu = 0, \\kappa = 1$\u0026#39;]), (\u0026#34;t\u0026#34;, [t], t_bins, [\u0026#39;green\u0026#39;], [\u0026#39;df = 5\u0026#39;]), (\u0026#34;$\\chi^2$\u0026#34;, [chi], chi_bins, [\u0026#39;yellow\u0026#39;], [\u0026#39;df = 5\u0026#39;]), ] fig, axes = plt.subplots(3,3) ax = axes.flatten() for i,v in enumerate(distributions): name, the_dist_list, the_dist_bins, c_list, params_list = v for dl, c, params in zip(the_dist_list, c_list, params_list): sns.distplot(dl, ax = ax[i], kde = True, color = c, label = params, bins = the_dist_bins) ax[i].set_title(f\u0026#34;{name} distribution\u0026#34;) ax[i].legend(loc = \u0026#39;upper right\u0026#39;, ncol = 2) # Custom labelling / manip ax[2].set_xlim((0,10)) ax[3].set_xlim((0,10)) ax[5].set_xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi]) ax[5].set_xticklabels([\u0026#39;$-\\pi$\u0026#39;, \u0026#39;$-\\\\frac{\\pi}{2}$\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;$\\\\frac{\\pi}{2}$\u0026#39;, \u0026#39;$\\pi$\u0026#39;]) ax[6].set_xlim(ax[4].get_xlim()) plt.subplots_adjust(hspace = 0.5) sns.despine() # plotting a Dirichlet distribution is not straight forward when K \u0026gt; 2 fig.delaxes(ax[8]) fig.set_size_inches((15,10)) plt.savefig(\u0026#34;continuous_distributions.png\u0026#34;, dpi = 300) I hope this was a useful, short intro to some foundational statistics! :) Have fun.\n","permalink":"https://ideasbyjin.github.io/post/2020-04-11-distributions/","summary":"A cheat sheet for probability distributions.","title":"Distributions Cheat sheet"},{"content":"Following on my book review of 2019, I\u0026rsquo;ve decided to divide this year\u0026rsquo;s piece into four chunks according to the quarters of the year. Here\u0026rsquo;s my thoughts on the four books I\u0026rsquo;ve read so far.\nBorn a Crime This has definitely been the page-turning read of 2020 so far. I\u0026rsquo;m a bit late to the Trevor Noah autobiography game, but this piece manages to combine political critique, great humour, and touch on elements of purpose. I started this in a plane and couldn\u0026rsquo;t put it down. A must-read for anyone who wants to know more about Apartheid, or pranking in a way only comedians know how.\nConvenience Store Woman This has to be one of the strangest books I\u0026rsquo;ve ever read. It has a Camus-style of dry humour about it, where the protagonist is very much living a life of routine existensialism. The deadpan nature of the book is surprisingly good, and while I don\u0026rsquo;t think I\u0026rsquo;d rave about it, it\u0026rsquo;s worth a try if you want something a bit different!\nUninhabitable Earth This book starts off in tremendous pace, talking about the harsh realities of global warming and what it means to the future of mankind. After about 2-3 chapters, it almost feels more like a rant than it is a scientific discussion on what we must do in order to tackle global warming. Not to mention the book contains zero figures! The book offers almost zero optimism to the planet\u0026rsquo;s future, and no real solutions for the reader, either.\nHowever, and most importantly, I\u0026rsquo;m afraid the book suffers from poor scientific writing. For example, the book continually uses Fahrenheit and Celsius in interchangeable ways. In addition, the book mentions facts after facts, without a deep interpretation on what that can mean. The book also has run-on sentences that, if read aloud, make you struggle to keep your breath (maybe this is intentional?). Finally, it has vocabulary that you wouldn\u0026rsquo;t really find in either natural science or casual science books, such as \u0026ldquo;quotidian\u0026rdquo;, \u0026ldquo;anthropogenic\u0026rdquo;, etc.\nAs a bioinformatician/data scientist, one takeaway I had from this book was just how much scientists must adhere to, and practice, good communication skills. Through deliberate practice and rigour, this will help deliver impact and convey the importance of observations in our data, and form strategies for further action.\nUltralearning This is a bit of a cheat - I actually started this book midway 2019 and stopped briefly because I was reading Bad Blood and lost track - how ironic! Either way, if you\u0026rsquo;re not familiar with Scott Young\u0026rsquo;s material, I think it\u0026rsquo;s brilliant as usual, and this book is a friendly introduction to more deliberate epistemology. Side note: if you already read a bit of Scott Young\u0026rsquo;s material, there are some elements that I feel are repeats of what he has mentioned prior. Having said that, the anecdotes of individual \u0026ldquo;Ultralarning\u0026rdquo; brilliance are incredibly interesting (e.g. the story of Van Gogh\u0026rsquo;s rise as a painter, Judit Polgar\u0026hellip; etc.)\nI\u0026rsquo;m expecting Q2 to be a bit slower, but there will be more reviews next time!\n","permalink":"https://ideasbyjin.github.io/post/2020-03-15-bookreview/","summary":"The books I\u0026rsquo;ve read in Q1 2020.","title":"Book reviews of Q1 2020"},{"content":"For most AI aficionados, deep learning may seem like the innovation of yesterday. However, deep learning methods are still revolutionising the way we think about many fields today. I\u0026rsquo;ve seen that happen with protein structure prediction.\nPerhaps the most iconic deep learning methods that you may have heard of will include:\n The convolutional neural network (CNN) and The recurrent neural network (RNN).  CNNs have changed the way we think about image classification, while RNNs have been hugely influential in sequential problems (e.g. text recognition).\nBut, before we talk about CNNs and RNNs, we should take a step back and ask what is a neural network. For some, this may still seem like a black box, especially if you\u0026rsquo;ve never had any formal training in the area. In this post, I\u0026rsquo;ll try to explain what neural networks are, and hopefully do a bit of jargon busting along the way.\nIf you have\u0026hellip;\n 30 seconds: neural networks are a family of algorithms in artificial intelligence. Inspired by biological neurons, they can perform extremely well. However, poor design choices can lead to some unusual behaviours, and for some tasks, they may not even be the method of choice. 15 minutes: go on. If you\u0026rsquo;re happy with some basic terminology, feel free to just jump to the code section.  Preamble + Jargon Buster The artificial neural network was inspired in part by the biological neuron:\nThe idea here is, each neuron takes an input (i.e. incoming neurotransmitters). From the combination of inputs, it produces an output signal (i.e. an action potential). When a series of neurons work sequentially and in tandem, you get an entire network.\nFrom the view of artificial intelligence, a \u0026ldquo;neuron\u0026rdquo; is really a unit that computes some function given some input. We can see some parallels when we use simplified circles and arrows, and how they can line up as a \u0026ldquo;network\u0026rdquo;:\nWhat makes neural networks special, in my view, are three things:\n Neuron internals: Each neuron can transform the input data. Once transformed, the data lies in a so-called hidden or \u0026ldquo;latent\u0026rdquo; space. In the original space, the data may not have had any obvious patterns, but in the latent representation, the data may show some neat patterns. Backpropagation: Using some neat calculus tricks, neural networks can be fine-tuned very well Flexible architectures: There are loads of ways to build networks to tailor for your type of data.  I\u0026rsquo;ve explained points 2 and 3 in more detail in the \u0026ldquo;Appendix\u0026rdquo; at the bottom. We\u0026rsquo;ll explain point 1 as it\u0026rsquo;s crucial.\nNeuron internals To understand what we mean by\n neural networks can transform the input data\n We have to first review the humble linear regression - the \u0026ldquo;line of best fit\u0026rdquo;.\n$$ y = mx + b$$\nThe idea is that the predictor variable, $$y$$, is equivalent to a sum of the variable $$x$$ that\u0026rsquo;s multiplied by some value $$m$$, along with an adjusting constant, $$b$$.\nYou can expand the idea to have multiple types of $$x$$s and their associated multipliers. To make life easier, and to keep consistency with math textbook notation, let\u0026rsquo;s rewrite\n$$ y = m_1x_1 + m_2x_2 + \u0026hellip; + b $$\nas\n$$ y = b_1x_1 + b_2x_2 + \u0026hellip; + b_0 $$\nEach of these $$b_1, b_2, \u0026hellip; $$ are multipliers; or in neural network speak, the weights. They are also known as a model\u0026rsquo;s \u0026ldquo;parameters\u0026rdquo;. By changing the $$b$$ values, the value of $$y$$ will change.\nThe final term, $$b_0$$, is the intercept of the linear model, or bias in neural network speak.\nWhat neural networks do is to use the output of the linear regression, then apply another mathematical function.\nThese functions are called activation functions. For example, the sigmoid function\n$$ S(y) = \\dfrac{1}{1+e^{-y}} $$\nis a non-linear function that transforms the output of the linear regression data into an S-shape.\nimport numpy as np import matplotlib.pyplot as plt # Check out my matplotlib stylesheet! plt.style.use(\u0026#34;bbc\u0026#34;) fig, ax = plt.subplots(1,2) # Let\u0026#39;s just get 20 values from -10 to +10. (we do +11 here otherwise it stops at 9.) y = np.arange(-10, 11) def sigmoid(v): return 1./(1+np.exp(-1.*v)) ax[0].plot(y) ax[1].plot(sigmoid(y)) ax[0].set_title(\u0026#34;Value of y\u0026#34;) ax[1].set_title(\u0026#34;Value of sigmoid(y)\u0026#34;) fig.set_size_inches((8,4)) Thus, a neuron with a sigmoid activation function squishes the output of a linear regression into an S-curve within 0 and 1. If we repeat a similar type of logic across all neurons, the inputs will get transformed according to what we choose as the activation function.\nOnce the data has been transformed by the activation function, this transformed data can then be used as the input to a new group of neurons downstream! The downstream layer of neurons may or may not use the same activation function on the transformed data from the previous layer.\nThere\u0026rsquo;s quite a few activation functions, and I\u0026rsquo;ve written a couple here for reference:\n   Name Equation Range What does it look like     Linear $$y$$ $$-\\infty, +\\infty$$ Just a straight diagonal line   Sigmoid $$ S(y) = \\dfrac{1}{1+e^{-y}} $$ 0,1 Shaped like an S curve   TanH $$ tanh(y) $$ -1,1 Shaped like an S curve   Rectified Linear Unit (ReLu) $$ Relu(y) = max(0,y) $$ 0, $$\\infty$$ Flat until y = 0, then diagonal   Softmax $$ Softmax(y) = \\dfrac{e^y}{\\sum_{i=1}^{K} e^{y}} $$ 0,1 Shaped like an S curve    def tanh(v): return np.tanh(v) def relu(v): return list(map(lambda _: max(0, _), v)) fig, axes = plt.subplots(2,2) ax = axes.flatten() ax[0].plot(y) ax[1].plot(sigmoid(y)) ax[2].plot(tanh(y)) ax[3].plot(relu(y)) ax[0].set_title(\u0026#34;Value of y\u0026#34;) ax[1].set_title(\u0026#34;Value of sigmoid(y)\u0026#34;) ax[2].set_title(\u0026#34;Value of tanh(y)\u0026#34;) ax[3].set_title(\u0026#34;Value of Relu(y)\u0026#34;) fig.set_size_inches((8,8)) Now when we talk about neural networks, there is some jargon that\u0026rsquo;s bound to float around. I\u0026rsquo;ll leave this here for reference.\nJargon Summary  Neuron : we refer to a neuron as one unit that computes some calculation. Loss : a metric to describe how far we are off from the true value Bias : the intercept Weights : a series of multipliers for each variable Activation function : a function that is applied to transform the output of a neuron  The task at hand To start, let\u0026rsquo;s get some RNAseq data from GTEx. This diagram from Nature just shows the huge diversity of the gene expression data sources. Building on a previous post where I clustered gene expression levels using DBSCAN, let\u0026rsquo;s try to use a supervised approach and predict the tissues instead.\nWhat are the use cases?  If there is a loss of documentation, the gene expression pattern may tell us what tissue a sample is likely to be derived from. Determine if related tissues show similar expression profiles; vice-versa, do some genes behave in the same way across some tissues? Establish the boundaries of a \u0026ldquo;normal\u0026rdquo; or \u0026ldquo;healthy\u0026rdquo; tissue based on gene expression, allowing us to detect anomalies  Data Prep As I covered in my previous post, the GTEx gene expression data is pretty big. In fact, getting it all at once is not ideal. We\u0026rsquo;re going to use some tricks:\n \u0026ldquo;Stream\u0026rdquo; the data – have a pointer to the data, but don\u0026rsquo;t bring it in all at once. Use the most variably expressed genes - if a gene is expressed everywhere with low rates, or is only found in, say, one sample, it\u0026rsquo;s not that informative. Use protein-coding genes.  # Importing useful things import pandas as pd import matplotlib.pyplot as plt from urllib.request import urlopen from gzip import GzipFile import io import multiprocessing as mp import numpy as np # Some utility functions - feel free to skip DECODE_CODEC = \u0026#39;utf-8\u0026#39; # This is the length of an Ensembl gene identifier which we\u0026#39;ll use later. ENSEMBL_LENGTH = 15 def stream_request(url): \u0026#34;\u0026#34;\u0026#34; Open a connection to some url, then stream data from it This has the advantage of: A. We don\u0026#39;t have to wait for the entire file to download to do operations B. We can perform some operations on-the-fly \u0026#34;\u0026#34;\u0026#34; fh = urlopen(url) buffer = io.StringIO() for line in fh: decoded = line.decode(DECODE_CODEC) buffer.write(decoded) fh.close() # Reset the StringIO buffer to byte position 0 buffer.seek(0) return buffer def stream_request_to_pandas(url: str, sep: str = \u0026#39;\\t\u0026#39;) -\u0026gt; pd.DataFrame: streamed_buffer = stream_request(url) return pd.read_csv(streamed_buffer, sep = sep) # Stream in and read protein-coding genes from HGNC geneUrl = \u0026#34;https://www.genenames.org/cgi-bin/download/custom?col=gd_app_sym\u0026amp;col=gd_pub_ensembl_id\u0026amp;status=Approved\u0026amp;hgnc_dbtag=on\u0026amp;order_by=gd_app_sym_sort\u0026amp;format=text\u0026amp;where=(gd_pub_chrom_map%20not%20like%20%27%25patch%25%27%20and%20gd_pub_chrom_map%20not%20like%20%27%25alternate%20reference%20locus%25%27)%0Aand%20gd_locus_type%20=%20%27gene%20with%20protein%20product%27\u0026amp;submit=submit\u0026#34; # This is a short way to convert that URL into a table gene_df = stream_request_to_pandas(geneUrl) # Let\u0026#39;s see what it looks like gene_df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  # Keep a list of protein identifiers here proteins = gene_df[\u0026#39;Ensembl gene ID\u0026#39;].values Now let\u0026rsquo;s get that big GTEx file\n# Open a handle onto the GTEx expression data URL = \u0026#34;https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_tpm.gct.gz\u0026#34; urlHandle = urlopen(URL) fh = GzipFile(fileobj=urlHandle) # ignore the first two lines as they contain shape of the file _ = fh.readline() _ = fh.readline() From that big GTEx file, let\u0026rsquo;s get the column names; this is in the third line of the file.\nheader = fh.readline().decode(DECODE_CODEC).strip().split(\u0026#39;\\t\u0026#39;) Now, each line looks something like this:\nfirst_row = np.array(fh.readline().decode(DECODE_CODEC).strip().split(\u0026#39;\\t\u0026#39;)) columns = np.append( np.array([0,1]), np.random.randint(2, len(header), 20) ) pd.DataFrame([first_row[columns]], columns=np.array(header)[columns]) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }   \u0026lt;th\u0026gt;...\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;GTEX-13OW7-0011-R4b-SM-5O9CX\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;GTEX-VJYA-1826-SM-4KL1W\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt;  Okay, cool, so we now know that the third column onward just has numbers. The first column contains gene IDs.\nKnowing this file format, let\u0026rsquo;s get the most variable protein-coding genes. We\u0026rsquo;ll also impose that at least half the samples have non-zero expression data. To assess variability, we\u0026rsquo;re going to use a metric called mean absolute deviation, then store it. Another viable approach is calculating something like the entropy value.\nBefore we do so, we\u0026rsquo;re going to log transform the data. GTEx data has lots of 0s, and TPM units can be out of scale.\ndef get_gene_mad(values): \u0026#34;\u0026#34;\u0026#34; The mean absolute deviation is equal to avg( abs(x - avg) ) \u0026#34;\u0026#34;\u0026#34; mu_x = np.mean(values) mad = np.mean(np.abs(values - mu_x)) return mad def wrapper(index, line): \u0026#34;\u0026#34;\u0026#34; This is a function we\u0026#39;ll call with a parallel pool connection. We ignore any line that contains non protein-coding genes \u0026#34;\u0026#34;\u0026#34; if index % 5000 == 0: print(\u0026#34;Processing line {}\u0026#34;.format(index)) tokenised_line = line.decode(DECODE_CODEC).strip().split(\u0026#39;\\t\u0026#39;) gene_id = tokenised_line[0][:ENSEMBL_LENGTH] # If it\u0026#39;s not a protein-coding gene, let\u0026#39;s ignore if gene_id not in proteins: return None numeric_values = np.array(tokenised_line[2:], dtype=float) array_cutoff = len(numeric_values) / 2. # If more than half the data is zero, then let\u0026#39;s ignore if sum(numeric_values == 0) \u0026gt;= array_cutoff: return None numeric_values = np.log(numeric_values+1) mad = get_gene_mad(numeric_values) return gene_id, mad, tokenised_line To see what log transformation can do, see this plot from my previous post.\nkeep_data = [] gene_mad = {} # Close the previous connection fh.close() # Re-establish the connection, and skip the first three lines urlHandle = urlopen(URL) fh = GzipFile(fileobj=urlHandle) print(\u0026#34;Re-established connection, skipping 3 lines...\u0026#34;) [fh.readline() for i in range(3)] # Let\u0026#39;s parallelise this to make it faster pool = mp.Pool(4) # Let\u0026#39;s get some results where we have just protein coding genes. jobs = [pool.apply_async(wrapper, args=(i,line,)) for i,line in enumerate(fh) ] results = [j.get() for j in jobs] filtered_results = [ r for r in results if r is not None ] del results Re-established connection, skipping 3 lines... Processing line 0 ... Processing line 55000  Brilliant, now that we have our gene MAD values we can just select the top, say, 1000 genes.\nTOP = 1000 sorted_genes = sorted(filtered_results, key = lambda x: x[1], reverse=True)[:TOP] # Let\u0026#39;s create a dataframe df = pd.DataFrame( [ np.log(np.array(_[2][2:],float)+1) for _ in sorted_genes ], columns = header[2:], index = [ _[2][0] for _ in sorted_genes ] ) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Looking out for Batch Effects While we have our top 1000 most variably expressed genes, there is a caveat! Gene expression data can be prone to batch effects (i.e. technical variation). To check for this behaviour, let\u0026rsquo;s get some sample annotations.\nsample_url = \u0026#34;https://storage.googleapis.com/gtex_analysis_v8/annotations/GTEx_Analysis_v8_Annotations_SampleAttributesDS.txt\u0026#34; sample_metadata = stream_request_to_pandas(sample_url) subset = sample_metadata[sample_metadata[\u0026#39;SAMPID\u0026#39;].isin(df.columns)].copy() tissue_column = \u0026#39;SMTSD\u0026#39; batch_column = \u0026#39;SMGEBTCH\u0026#39; subset.iloc[:3][[\u0026#39;SAMPID\u0026#39;, tissue_column, batch_column]] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  We can see that each sample has an associated batch ID. We can colour our plot by the batch ID and the tissue:\nfrom sklearn.decomposition import PCA sample_id_to_batch = subset[[\u0026#39;SAMPID\u0026#39;, batch_column]] sample_id_to_tissue = subset[[\u0026#39;SAMPID\u0026#39;, tissue_column]] pca = PCA() # We take the transpose of our gene expression because otherwise PCA would find the samples that are most varied # Rather than the genes that are most varied. coords = pca.fit_transform(df.T.values) fig, ax = plt.subplots(1,2) special_tissue = \u0026#39;Muscle - Skeletal\u0026#39; tissue_dots = np.argwhere(sample_id_to_tissue[tissue_column]==special_tissue).flatten() t1_name = \u0026#34;LCSET-4822\u0026#34; t2_name = \u0026#34;LCSET-4824\u0026#34; t3_name = \u0026#34;LCSET-4416\u0026#34; t4_name = \u0026#34;LCSET-10515\u0026#34; t1 = np.argwhere(sample_id_to_batch[batch_column]==t1_name).flatten() t2 = np.argwhere(sample_id_to_batch[batch_column]==t2_name).flatten() t3 = np.argwhere(sample_id_to_batch[batch_column]==t3_name).flatten() t4 = np.argwhere(sample_id_to_batch[batch_column]==t4_name).flatten() t1_inter = np.intersect1d(t1, tissue_dots) t2_inter = np.intersect1d(t2, tissue_dots) t3_inter = np.intersect1d(t3, tissue_dots) t4_inter = np.intersect1d(t4, tissue_dots) ax[0].scatter(coords[:,0], coords[:,1], color = \u0026#39;#c3c3c3\u0026#39;, alpha = 0.3) ax[0].scatter(coords[tissue_dots,0], coords[tissue_dots,1], color = \u0026#39;#348abd\u0026#39;, label = special_tissue) ax[1].scatter(coords[:,0], coords[:,1], color = \u0026#39;#c3c3c3\u0026#39;, alpha = 0.3) ax[1].scatter(coords[t1_inter,0], coords[t1_inter,1], color = \u0026#39;green\u0026#39;, label = \u0026#34;Batch {}\u0026#34;.format(t1_name)) ax[1].scatter(coords[t2_inter,0], coords[t2_inter,1], color = \u0026#39;#a60628\u0026#39;, label = \u0026#34;Batch {}\u0026#34;.format(t2_name)) ax[1].scatter(coords[t3_inter,0], coords[t3_inter,1], color = \u0026#39;#ffd700\u0026#39;, label = \u0026#34;Batch {}\u0026#34;.format(t3_name)) ax[1].scatter(coords[t4_inter,0], coords[t4_inter,1], color = \u0026#39;#f58426\u0026#39;, label = \u0026#34;Batch {}\u0026#34;.format(t4_name)) ax[0].legend(loc=\u0026#39;upper right\u0026#39;) ax[1].legend(loc=\u0026#39;upper right\u0026#39;) fig.set_size_inches((10,5)) There does seem to be some level of batch effect, which we can adjust using tools like ComBat, but we won\u0026rsquo;t do that here, and use the data as-is for simplicity.\nCoding up the NN To code up our neural network, we are spoiled for choice - there\u0026rsquo;s loads of solutions (TensorFlow, PyTorch), but the simplest framework in my mind is Keras. One of the advantages of Keras is that it is very easy to read, and covers a a good range of use cases (e.g. convolutional layers). To train a neural network to predict the tissue that a gene belongs to, we need to know which tissue the sample is from (i.e. the truth).\nsubset.iloc[:3][[\u0026#39;SAMPID\u0026#39;, tissue_column]] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Since our data is arranged in the format of genes $$\\times$$ samples , we are going to transpose the matrix. This is because we want the neural network to take, the expression profile per sample as opposed to per gene. This way, it\u0026rsquo;ll predict what tissue we should assign for a particular sample.\n# Get a dictionary of sample id to tissue site sample_to_tissue = dict(subset[[\u0026#39;SAMPID\u0026#39;, tissue_column]].values) # Get the tranposed matrix and subset a few columns for preview mat = df.T mat[\u0026#39;TissueSite\u0026#39;] = [ sample_to_tissue[ix] for ix in mat.index ] mat[ list(mat.columns[:3]) + [\u0026#39;TissueSite\u0026#39;] ] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Applying the Neural Network Now that we have a filtered dataset to work with (finally), we can now train a neural network. Each neural network can be described in terms of its \u0026ldquo;layers\u0026rdquo;. Each layer essentially represents a set of neurons. There are three types of layers that we\u0026rsquo;ll have in our network today:\n The input layer - a set of neurons that just emit the gene expression data The hidden layer - a set of neurons that take the gene expression values and transforms it The output layer - a set of neurons that predict what tissue it belongs to.  To illustrate this idea, it looks something like this:\nThat sounds fairly easy, right? Let\u0026rsquo;s set up our model using Keras:\n# This is the simplest type of neural network setup from keras.models import Sequential # Dense layers from keras.layers import Dense # Function to convert tissue labels to a series of 1s and 0s from keras.utils import to_categorical # Convert the tissue labels to a Keras-friendly set of 1s and 0s # Get the actual tissues tissue_labels = mat[mat.columns[-1]] # Get a mapping between tissue name and some random integer tissue_labels_int = dict([ (v,i) for i,v in enumerate(set(tissue_labels)) ]) # Reverse this as well - it\u0026#39;ll be useful for later. tissue_labels_rev = dict([ (v,k) for k,v in tissue_labels_int.items() ]) # Convert the entire column to the mapped integer tissue_labels_encoded = [tissue_labels_int[t] for t in tissue_labels] # Convert to a Keras-friendly label set of 1s and 0s labels = to_categorical(tissue_labels_encoded, num_classes=len(set(tissue_labels))) # This will be useful for later. indices = list(range(len(labels))) Our \u0026ldquo;hidden\u0026rdquo; layer, which does the bulk of the computation, will be taking in the gene expression data across all samples, then apply the activation function. Now, deciding the precise architecture for this problem is somewhat subjective. There\u0026rsquo;s no real guide (as far as I\u0026rsquo;m aware!) that recommends how many neurons should go in a particular layer, or how many layers there should be, full stop. Lots of this is about experimentation!\nIn this case, I\u0026rsquo;m going to have one hidden layer that takes the expression values of a 1000 genes, then compresses it into 200 \u0026ldquo;latent\u0026rdquo; genes using the ReLu activation function. During the development of this notebook, I also toyed with having two hidden layers (one with TanH activation, and the second with ReLu).\nlayer1_squish = 400 layer2_squish = 200 # Initialise our model model = Sequential() # Add one layer to fit in the data model.add(Dense(layer2_squish, input_shape=(len(mat.columns[:-1]),), activation=\u0026#39;relu\u0026#39;)) ### Alternative setup # model.add(Dense(layer1_squish, input_shape=(len(mat.columns[:-1]),), activation=\u0026#39;tanh\u0026#39;)) # model.add(Dense(layer2_squish, input_shape=(layer1_squish,), activation=\u0026#39;relu\u0026#39;)) To cap it off, I will have an output layer that uses the Softmax function. This is an activation function that is useful for categorical data, and can assign a value between 0 to 1 - essentially acting as a value that acts like a probability.\nFinally I will compile the model by:\n Using the stochastic gradient descent (SGD) optimiser - details for another time. A categorical cross-entropy loss function. This is essentially a function that tells the neural network how well it\u0026rsquo;s doing with respect to the true labels. The job of the neural network is to minimise this loss using SGD.  # Add one layer to do the prediction model.add(Dense(len(set(tissue_labels)), input_shape=(layer2_squish,), activation = \u0026#39;softmax\u0026#39;)) # Compile the model model.compile( optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;] ) Now that we got the model set up, what we\u0026rsquo;ll do now is to split the data into a training and test set:\nfrom sklearn.model_selection import train_test_split # Split to a 75/25 split using sklearn train_x, test_x, train_y, test_y = train_test_split(mat, indices, random_state = 42) Let\u0026rsquo;s run the model on our training data. For training, we are going to set up so that\u0026hellip;\n 400 examples are used for training at a time - this is called the \u0026ldquo;mini batch\u0026rdquo; strategy that helps speed things up Run 50 epochs, or iterations.  # Let\u0026#39;s run the model and run 50 iterations model.fit(train_x[train_x.columns[:-1]].values , labels[train_y], epochs=50, batch_size = 400) Epoch 1/50 13036/13036 [==============================] - 0s 32us/step - loss: 2.5404 - accuracy: 0.4580 Epoch 2/50 13036/13036 [==============================] - 0s 18us/step - loss: 1.0165 - accuracy: 0.7234 Epoch 3/50 13036/13036 [==============================] - 0s 21us/step - loss: 0.7074 - accuracy: 0.8045 ... Epoch 48/50 13036/13036 [==============================] - 0s 17us/step - loss: 0.1502 - accuracy: 0.9581 Epoch 49/50 13036/13036 [==============================] - 0s 18us/step - loss: 0.1515 - accuracy: 0.9571 Epoch 50/50 13036/13036 [==============================] - 0s 17us/step - loss: 0.1487 - accuracy: 0.9595 \u0026lt;keras.callbacks.callbacks.History at 0x62601afd0\u0026gt;  This is seriously impressive. The neural network starts out with 45.8% accuracy, but eventually climbs to 95.95% accuracy in assigning the correct tissue. Shall we see how it does on the test set?\n# Get Keras\u0026#39; own evaluation model.evaluate(test_x[test_x.columns[:-1]], labels[test_y])  4346/4346 [==============================] - 0s 39us/step [0.18317752908546694, 0.944086492061615]  Okay, so Keras says that it achieved a 94.4% accuracy on the test set. We can double check just to be sure.\n# Predict the classes given test set data predictions = model.predict_classes(test_x[test_x.columns[:-1]]) # Map the predictions back to a tissue name predictions_to_string = [ tissue_labels_rev[p] for p in predictions ] truth = test_x[\u0026#34;TissueSite\u0026#34;] pred_frame = pd.DataFrame(zip(truth, predictions_to_string), columns = [\u0026#34;True Tissue\u0026#34;, \u0026#34;Predicted Tissue\u0026#34;]) # Let\u0026#39;s see how many are incorrect pred_frame[pred_frame[\u0026#39;True Tissue\u0026#39;] != pred_frame[\u0026#39;Predicted Tissue\u0026#39;]].shape # Plot a confusion matrix ax_len = len(set(truth)) lookup = dict([ (v,i) for i,v in enumerate(set(truth)) ]) mat = np.zeros((ax_len, ax_len)) for a,b in zip(truth, predictions_to_string): index_x, index_y = lookup[a], lookup[b] mat[index_x, index_y] += 1 normalised_mat = mat/np.sum(mat,axis=1) plt.imshow(normalised_mat, cmap = \u0026#34;Blues\u0026#34;, interpolation = \u0026#34;nearest\u0026#34;) plt.axis(\u0026#34;off\u0026#34;) (218, 2)  So in over 4346 samples to predict, the neural network only got 218 incorrect! We can also see this visually in that coloured squares are predominantly found along the diagonal, suggesting that the true and predicted labels agree with each other.\nBenchmarking As a comparison, we can run a random forest for comparison. To get a refresher, check out my previous post on random forests.\nfrom sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression rfc = RandomForestClassifier() rfc.fit(train_x[train_x.columns[:-1]].values, tissue_labels[train_y]) RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  rf_predictions = rfc.predict(test_x[test_x.columns[:-1]].values) truth = test_x[\u0026#34;TissueSite\u0026#34;] pred_frame_rf = pd.DataFrame(zip(truth, rf_predictions), columns = [\u0026#34;True Tissue\u0026#34;, \u0026#34;Predicted Tissue (RF)\u0026#34;]) # Let\u0026#39;s see how many are incorrect pred_frame_rf[pred_frame_rf[\u0026#39;True Tissue\u0026#39;] != pred_frame_rf[\u0026#39;Predicted Tissue (RF)\u0026#39;]].shape (205, 2)  This means the random forest had a 95.3% accuracy on the test set!\nInterpretation Hmm, interesting. So for the super hopeful, this result may seem like a surprise. How, or even why, is the random forest superior in this case? There are some things to be said here\u0026hellip;\n The neural network that we\u0026rsquo;ve implemented is fairly basic. We only have one hidden layer, and for some applications, having more hidden layers can be a good way to increase accuracy. This could be one of those cases.  Possible solution: implement more layers.   The choice of the activation function can be influential; we haven\u0026rsquo;t exhaustively tested them here.  Possible solution: assess the impact of ReLU vs. Elu vs. Tanh\u0026hellip; etc.   Random forests are remarkably good at detecting which features are most important for class discrimination. This is possible because the random forest does lots and lots of sampling to figure out the most pertinent feature sets.  Possible solution: allow the neural network to train for a longer time (epochs) to determine the more relevant features in the dataset.   The data itself could have other non-linear patterns that we have not fully exploited - anywhere from transforming the data (e.g. applying a kernel), to leveraging similar genes, are all options here.  Conclusions What next? Maybe for another time I\u0026rsquo;ll cover more detailed neural network architectures, and how they can be used for other types of problems. While the genes were not ordered in a particular way, another possibility is to order them in a certain manner that makes the gene expression matrix amenable to other deep learning methods like the convolutional neural network. Or we can even explore using autoencoders to cluster gene expression data.\nFor now, this is a wrap, and see you next time!\nAppendix 2. Optimisation of weights If you\u0026rsquo;re familiar with the mathematical theory behind linear regression, you would know that it\u0026rsquo;s essentially finding a line that minimises the residuals, or errors , of the prediction against your observations. This is done by (long story short) some fancy matrix algebra along the lines of\n$$ \\beta = (X^TX)^{-1}X^Ty $$\nFor neural networks, we often define a method to quantify that error in the form of a loss function. Essentially, this loss function states how far the prediction is from the true value that we\u0026rsquo;re trying to predict. There are loss functions for various types of data we want to predict. For instance, if we\u0026rsquo;re predicting what class a data point belongs to (i.e. a classification problem), we can use cross-entropy, or when we are predicting a continuous value (e.g. length of a throw given some conditions), then we can use mean absolute error, etc.\nWhatever the loss function, neural networks use a technique called backpropagation which takes the derivative of the loss function with respect to the weights. We use the derivative because this tells us how much the value of the loss function changes when we change the weights. Neat, right? These derivatives can then be used by optimisation algorithms like gradient descent.\nIf you\u0026rsquo;re familiar with calculus notations, we\u0026rsquo;re effectively trying to evaluate\n$$ \\dfrac{\\partial L}{\\partial w} $$\nwhich is then used to update the weights,\n$$ w \\leftarrow w + \\alpha \\dfrac{\\partial L}{\\partial w} $$\nwhere $$\\alpha$$ represents a \u0026ldquo;learning rate\u0026rdquo; to help convergence. While backpropagation is powerful, it doesn\u0026rsquo;t really carry a biological meaning. The implication here is that somehow we feed things backward to tell an upstream neuron how wrong it was.\n3. Many, many different architectures Why is it that \u0026ldquo;neural networks\u0026rdquo; always seem to headline insane prediction successes? That\u0026rsquo;s because neural networks can be flexibly constructed for different data types. For example,\n Convolutional neural networks use a 2D-set up of neurons to perceive a wider \u0026ldquo;field\u0026rdquo; of data - this is particularly useful for images. Recurrent neural networks have a repeated, sequential structure that\u0026rsquo;s suited for speech / text. Even traditional \u0026ldquo;feed-forward\u0026rdquo; neural networks, like one we\u0026rsquo;ll code today, can use tricks like \u0026ldquo;dropout\u0026rdquo; to ensure that there isn\u0026rsquo;t a huge over-fit to the data.  This is a section that deserves a post on its own!\n","permalink":"https://ideasbyjin.github.io/post/2020-02-26-anns/","summary":"This is my intro to neural networks.","title":"An introduction to Artificial Neural Networks"},{"content":"Following on some amazing heroes like Barack Obama and Bill Gates, I\u0026rsquo;ve decided to reflect back on the past year and do a quick-ish review on some books I\u0026rsquo;ve read in 2019. I\u0026rsquo;m hoping that 2020 will be a year where I\u0026rsquo;ll get to read more and do something a bit more comprehensive.\nBad Blood This is one of those books that made me thinking,\n Holy sh*t\n with every chapter. As a slow reader, I go through one book roughly every 2 months, but I went through this in a week during my commutes to work. The writing is brilliant; it feels like watching a Netflix series, but in text. Ultimately, I was reminded that the work I do has long-term purpose and implications, giving me the onus to do things even more diligently and responsibly. This is something I\u0026rsquo;d recommend to anyone working in tech and/or medicine.\nEleanor Oliphant is Completely Fine Historically, I didn\u0026rsquo;t read a lot of fiction until this year. I always maintain the idea that I want to read something that will make me either laugh or think. This book managed to do both, and refreshed my desire to read fiction again.\nIt does have a British style of humour, and the writing style is perhaps not for everyone. Most importantly, it opens readers to sensitive issues, and (I think) encourages a healthy debate around topics like loneliness that can be taboos for many. It can get dark, be warned!\nFreakonomics Not exactly the \u0026ldquo;newest\u0026rdquo; book to get off the shelves, but this was a very thought-provoking book indeed. As a bioinformatician / data scientist, I think books like these remind us that there is more than meets the eye. In other words, what may seem like obvious correlations are essentially confounders, and we need to do deeper analyses to look at the causative agents that account for variations in our observations. For example, Levitt and Dubner touch on how a reduction in crime is a consequence of reduced abortions, and not increased police numbers.\nBrave New World Again, an old classic, but one where I saw many parallels with our society today. Instead of soma, I would argue that today\u0026rsquo;s \u0026ldquo;happy drug\u0026rdquo; is our smartphone - see the screen a bit, and you\u0026rsquo;re jolted back to life. Given that (possibly) my favourite fiction of all time is 1984, I was hoping to enjoy Brave New World just as much. While I found Aldous Huxley to be a bit drier than Orwell, it was shocking to see how many aspects of the book (e.g. the division of classes, low-key racism) are still making headlines today.\nFahrenheit 451 The last of old classics in the list for 2019. Another reminder of why books are, in my view, under-rated in today\u0026rsquo;s society, and deserve more attention. While it\u0026rsquo;s hard to imagine a society where books will be eradicated forever, Fahrenheit 451 reminds us that the fundamental spread of the best information, knowledge, and wisdom are in books, and they should be held as treasures.\nMeaning of Marriage Earlier this year, my wife and I got married in a church ceremony. This book was a good reminder for the types of values that we want to uphold in our marriage, and confirm our commitment to doing so. As per Tim Keller\u0026rsquo;s usual style, expect lots of C.S. Lewis and Biblical references.\n","permalink":"https://ideasbyjin.github.io/post/2019-12-31-bookreview/","summary":"These are the books of 2019 that I found to be thought-provoking.","title":"Book reviews of 2019"},{"content":"It\u0026rsquo;s Christmas, and Santa is here! He\u0026rsquo;s got his list and he\u0026rsquo;s about to see who\u0026rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.\nSanta is in trouble. Without his list, it\u0026rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who\u0026rsquo;s been nice or naughty. However, Santa reasons that if a neighbourhood is generally full of bad kids, he can skip them, for now, and give gifts elsewhere.\nIn this alternate universe, Santa is, luckily, a Bayesian statistican. Santa decides he is going to use Bayesian inference to guess the number of naughty kids.\nIt\u0026rsquo;s going to be a long night, but one that\u0026rsquo;s hopefully salvaged by Bayesian methods!\nIf you have\u0026hellip;\n 30 seconds: Bayesian inference is driven by this equation:  $$P(B\\vert A) = \\dfrac{P(A\\vert B)P(B)}{P(A)}$$\nUnlike frequentist methods that try to make point estimates of parameters, Bayesian methods are driven by estimating distributions of parameters.\n 15 minutes: It\u0026rsquo;s a long one, but hopefully it\u0026rsquo;ll be worth it. This post will discuss the basics of Bayesian inference, and how we can use the pymc3 library for statistical computing.  NB: I would not consider myself an expert in Bayesian statistics, and I\u0026rsquo;ll provide a list of references at the bottom.\n# import some stuff import pymc3 as pm import matplotlib.pyplot as plt import numpy as np from scipy.stats import gaussian_kde, norm, bernoulli plt.style.use(\u0026#34;bbc\u0026#34;) # https://github.com/ideasbyjin/bbc-plot ;) Statistics Pre-Preamble - feel free to skip In many statistical applications, we want to create a model for our data $$x$$, given some model parameter, $$\\theta$$. We can then represent this model as a probability distribution, i.e. $$P(x\\vert \\theta)$$. (NB: This is the case for parametric models; non-parametric models are different but that\u0026rsquo;s for another time.)\nI think the easiest way to visualise this idea is using histograms:\n# Create a random set of observations np.random.seed(0) random_obs = np.random.normal(size=1000) # Plot a histogram and the density estimate fig, ax = plt.subplots(1,1) ax.hist(random_obs, bins = np.arange(-3, 3.01, 0.5), density = True, alpha = 0.8) # Plot the curves of three normal distributions interval = np.arange(-3, 3.01, 0.05) ax.plot(interval, [norm.pdf(x) for x in interval], label = \u0026#34;Model 1\u0026#34;) ax.plot(interval, [norm.pdf(x, scale = 3) for x in interval ], label = \u0026#34;Model 2\u0026#34;) ax.plot(interval, [norm.pdf(x, loc = 1, scale = 1.5) for x in interval ], label = \u0026#34;Model 3\u0026#34;) ax.set_ylabel(\u0026#34;Density\u0026#34;) ax.set_xlabel(\u0026#34;$x$\u0026#34;) ax.legend() For my data (blue bars), which model (red, yellow, green lines), each with its own $$\\theta$$, is the most likely one that generated the data?\nNaturally, one can ask, what is the best $$\\theta$$ for my observed data? This can be obtained by the likelihood function, which is the joint density of the data,\n$$L(\\theta) = \\prod_{i=1}^{n} P(x_i\\vert \\theta)$$\nThis can start to get a bit confusing - why am I talking about $$\\theta$$ and likelihoods? What does it have to do with Bayesian inference?\nPhilosophically speaking, our observed data is (often) a sample and the true value of $$\\theta$$ is not known\u0026hellip;\nBayesian Approach to Models Frequentist statisticans try to deliver a point estimate of the model parameter(s), $$\\theta$$. To quantify uncertainty around that estimate, standard error and/or confidence intervals are used (see this post for a good explanation).\n$$\\theta$$ is then estimated typically using maximum likelihood estimate (MLE) methods - either analytically (e.g. differentiating the log likelihood function), or numerically (e.g. using the expectation-maximisation algorithm).\nBayesian statisticans also agree that the true value of $$\\theta$$ is fixed, but they express the uncertainty of their estimate of $$\\theta$$ using distributions. The aim then is to derive the posterior distribution, $$P(\\theta\\vert x)$$, given some data and some prior belief about $$\\theta$$ itself. In other words,\n$$P(\\theta\\vert x) = \\dfrac{P(x\\vert \\theta)P(\\theta)}{P(x)}$$\nWhat do these probabilities represent?\n $$P(\\theta)$$ represents our prior belief about the parameter $$P(x\\vert \\theta)$$ represents our observations - evidence - or\u0026hellip; likelihood! $$P(x)$$ is a normalising constant that represents the probability of $$x$$ irrespective of $$\\theta$$.  Typically, $$P(x)$$ is difficult to calculate, so we represent the posterior being proportional to the product of the likelihood and the prior.\n$$P(\\theta\\vert x) \\propto P(x\\vert \\theta)P(\\theta)$$\nSanta Bayes at work Let\u0026rsquo;s get to work! We can treat a child being naughty or nice as a binary variable, like a coin being heads or tails. Binary events like these are known as Bernoulli trials.\nThus, we can model the event of seeing a nice or naughty child as samples from a Bernoulli distribution.\n$$x_i \\sim \\text{Bernoulli}(\\theta)$$\nAt the crux of this is the rate of finding a naughty child, or $$\\theta$$. This represents the parameter of our model which we want to estimate using Bayesian inference.\n(From this point forth, a naughty child is represented by a 1, and good children by a 0).\n# Let\u0026#39;s see what Bernoulli-distributed variables look like. # This is arbitrarily chosen for example reasons. arbitrary_theta = 0.5 # Let\u0026#39;s conduct 100 Bernoulli trials of our own, i.e., Santa visits 100 homes np.random.seed(42) children = [ i for i in bernoulli.rvs(size = 100, p = arbitrary_theta) ] # Print the first 10 print(children[:10]) [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]  Neat, so what happens when we change $$\\theta$$?\n## Plot the effects of bernoulli distribution parameters fig, axes = plt.subplots(3, 2, sharey=True) theta_init = 0.3 num_houses = 100 np.random.seed(42) for i, ax in enumerate(axes.flatten()): theta = theta_init + 0.1 * i _obs = bernoulli.rvs(p = theta, size = num_houses) ax.bar([0, 1], [ (_obs==0).sum(), (_obs==1).sum() ], width = 0.5, label = \u0026#34;$\\\\theta$ = {:.1f}\u0026#34;.format(theta)) ax.set_xticks([0,1]) ax.set_xticklabels([\u0026#34;Nice\u0026#34;, \u0026#34;Naughty\u0026#34;]) ax.legend() fig.set_size_inches((6,6)) _ = fig.text(0.04, 0.5, \u0026#34;Number of naughty children\u0026#34;, va = \u0026#39;center\u0026#39;, rotation = 90) Depending on $$\\theta$$, the number of naughty children changes; higher $$\\theta$$ = more naughty kids!\nWhile the frequentist is busy trying to derive the MLE for the likelihood function $$L(\\theta)$$, the Bayesian creates yet another model describing the distribution of $$\\theta$$, as we don\u0026rsquo;t know the true value of $$\\theta$$.\nIn other words, Santa Bayes is uncertain about the true value of $$\\theta$$. However, he can make an initial stab at $$\\theta$$ - even if it isn\u0026rsquo;t hugely informative - and blend it with some observations to get an updated estimate of $$\\theta$$.\nThis initial stab is what\u0026rsquo;s known as the prior distribution of $$\\theta$$. Combined with some data, Santa will reach a new posterior distribution of $$\\theta$$.\nThe choice of the prior can affect the shape of the posterior, but in practice, more observations (moar data) will eventually swamp the effects of the prior.\nKnowing that our data - the distribution of naughty and nice children - follows a Bernoulli distribution, a suitable prior for $\\theta$ is the uniform distribution.\nThere are several nice aspects of using a uniform prior for this problem:\n It makes very little assumptions on what the true value of $$\\theta$$ can be It is very simple to implement!  Santa now decides to use a uniform prior for $$\\theta$$, what does that look like?\n# Initialise a pymc3 model model = pm.Model() with model: theta_dist = pm.Uniform(\u0026#34;theta\u0026#34;) # Sample from the prior distribution of theta rvs = theta_dist.random(size=20000) bins = np.arange(0,1.01, 0.1) h = plt.hist(rvs, density=True, bins = bins) _ = plt.title(\u0026#34;Prior distribution of $\\\\theta$\u0026#34;) With this prior distribution for $$\\theta$$ in hand, Santa goes around the neighbourhood and checks for the number of naughty kids in the neighbourhood.\nobservations_test = [1]*13 + [0]*7 # 13 naughty kids, 7 nice ones, i.e. theta ~ 0.65 These observations can then be used for the likelihood function in pymc3. The key here is that we provide the Bernoulli likelihood function in pymc3 the set of observations. We also specify that the parameter for the Bernoulli distribution is sampled from the uniform prior that we have discussed earlier.\n# Define a function to get the distribution of theta after MCMC def get_trace(observations): # Initialise a pymc3 model model = pm.Model() with model: # This creates a distribution on theta theta_dist = pm.Uniform(\u0026#34;theta\u0026#34;) # Call the Bernoulli likelihood function observed_dist # The parameter for this distribution is from the prior, theta_dist; # observations are given to observed.  observed_dist = pm.Bernoulli(\u0026#34;obs\u0026#34;, p = theta_dist, observed=observations) # Use the Metropolis-Hastings algorithm to estimate the posterior step = pm.Metropolis() trace = pm.sample(10000, step = step) return trace In the function above, we use something called the Metropolis-Hastings algorithm to determine the posterior distribution. It\u0026rsquo;s a bit outside the scope of this post, but it\u0026rsquo;s essentially a technique that provides a numeric estimate of the true posterior distribution. It\u0026rsquo;s handy when you can\u0026rsquo;t derive an analytical solution.\ntrace = get_trace(observations=observations_test) fig, ax = plt.subplots() # MCMC methods have a burn-in period, so we discard the first few thousand iterations. ax.hist(trace[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax.axvline(13./20, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;, label = \u0026#39;$\\\\theta$ estimated from relative frequency\u0026#39;) _ = ax.set_title(\u0026#34;Posterior distribution of $\\\\theta$\u0026#34;) Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:08\u0026lt;00:00, 5162.22draws/s] The number of effective samples is smaller than 25% for some parameters.  What\u0026rsquo;s happened here? Essentially, by giving a set of observations, we see that the posterior distribution of $$\\theta$$ has been estimated, and it looks very different to our prior. In fact, it creates a bell curve-like histogram around $$\\theta = 0.65$$.\nGiven this updated, posterior distribution of $$\\theta$$, Santa can then do some calculations on this new distribution, such as:\n Summary statistics of the posterior distribution (mean, standard deviation) The maximum a posteriori estimate, or the MAP The 95% credible interval - not to be confused with confidence intervals!  Following these statistics, Santa can take further action on whether there are too many naughty kids in the neighbourhood, and whether it\u0026rsquo;s worth his time to stick around. Furthermore, Santa can use this updated posterior as a new prior for future inference activities.\nIn fact, we can see what happens when he goes to a different neighbourhood with a different number of nice kids, but with an identical uniform prior as before.\nobservations_nice = [1]*3 + [0]*17 # 9 naughty kids, 11 nice ones, i.e. theta ~ 0.15 trace = get_trace(observations=observations_nice) fig, ax = plt.subplots() # MCMC methods have a burn-in period, so we discard the first few thousand iterations. ax.hist(trace[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax.axvline(3./20, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;, label = \u0026#39;$\\\\theta$ estimated from relative frequency\u0026#39;) _ = ax.set_title(\u0026#34;Posterior distribution of $\\\\theta$\u0026#34;) Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07\u0026lt;00:00, 5823.77draws/s] The number of effective samples is smaller than 25% for some parameters.  As we can see, for all intents and purposes, our implementation has remained almost identical, aside from the fact that our observation vectors are different. This leads to huge changes in the posterior distributions, and thus our understanding of $$\\theta$$!\nEffects of data size OK, so now Santa has a framework for estimating the rate in which he\u0026rsquo;ll come across naughty or nice kids. How much will his posterior be affected by the number of observations?\nFor this example, we can generate some observations using a pre-defined, arbitrary $$\\theta$$. We can then see if our posterior distribution converges to that arbitrary $$\\theta$$ value, too.\narbitrary_theta = 0.2 small, medium, large = bernoulli.rvs(p=arbitrary_theta,size=10), bernoulli.rvs(p=arbitrary_theta,size=20),\\ bernoulli.rvs(p=arbitrary_theta,size=200) low_obs_trace = get_trace(small) medium_trace = get_trace(medium) high_obs_trace = get_trace(large) Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [07:33\u0026lt;00:00, 92.67draws/s] The number of effective samples is smaller than 25% for some parameters. Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07\u0026lt;00:00, 5564.29draws/s] The number of effective samples is smaller than 25% for some parameters. Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07\u0026lt;00:00, 5974.85draws/s] The number of effective samples is smaller than 25% for some parameters.  fig, ax = plt.subplots(1,3) ax[0].hist(low_obs_trace[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[0].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[0].set_title(\u0026#34;Posterior $\\\\theta$ with 10 observations\u0026#34;) ax[1].hist(medium_trace[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[1].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[1].set_title(\u0026#34;Posterior $\\\\theta$ with 20 observations\u0026#34;) ax[2].hist(high_obs_trace[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[2].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;, label = \u0026#39;Arbitrary $\\\\theta$\u0026#39;) ax[2].set_title(\u0026#34;Posterior $\\\\theta$ with 200 observations\u0026#34;) ax[2].legend() fig.set_size_inches((12,3)) As we can see here, with more observations, we eventually diverge away from the shape of the prior. In fact, even from 10 observations, we can see that the posterior distribution of $$\\theta$$ already looks dissimilar to the uniform prior we had specified before.\nWhile the posterior distributions don\u0026rsquo;t converge tightly enough around the arbitrarily defined $$\\theta$$ value unless there\u0026rsquo;s 500 observations, it\u0026rsquo;s clear that even with a small amount of data, we see positive results!\nEffects of the prior For the skeptic, we can choose a different prior distribution for $$\\theta$$ - the Beta distribution:\n$$\\theta \\sim \\text{Beta}(\\alpha, \\beta)$$\nThe beta distribution, like the uniform distribution, is bound to values between 0 and 1. It is defined by two additional parameters, $$\\alpha$$ and $$\\beta$$. Thus, in this context, they are known as hyper-parameters for our use case.\n Interestingly, the uniform distribution is a special case of the Beta distribution where $$\\alpha = 1$$ and $$\\beta = 1$$\u0026hellip;\n Does it matter what $$\\alpha$$ and $$\\beta$$ are?\ndef get_trace_beta(observations = None, alpha = 1, beta = 1): # Initialise a pymc3 model model = pm.Model() with model: theta_dist = pm.Beta(\u0026#34;theta\u0026#34;, alpha, beta) # Call the Bernoulli likelihood function obs # The parameter is from the prior, theta_dist # observations are given to observed.  observed_dist = pm.Bernoulli(\u0026#34;obs\u0026#34;, p = theta_dist, observed=observations) # Use the Metropolis-Hastings algorithm  step = pm.Metropolis() trace = pm.sample(10000, step = step) return theta_dist, trace prior_a, hyper_a = get_trace_beta(small, alpha = 1, beta = 2) prior_b, hyper_b = get_trace_beta(small, alpha = 2, beta = 2) prior_c, hyper_c = get_trace_beta(small, alpha = 3, beta = 1) prior_1, hyper_1 = get_trace_beta(large, alpha = 1, beta = 2) prior_2, hyper_2 = get_trace_beta(large, alpha = 2, beta = 2) prior_3, hyper_3 = get_trace_beta(large, alpha = 3, beta = 1) fig, axes = plt.subplots(2,3, sharey=True) ax = axes.flatten() ax[0].hist(prior_a.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[0].hist(hyper_a[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[0].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[0].set_title(\u0026#34;$n = 10; \\\\alpha=1, \\\\beta=2$\u0026#34;) ax[1].hist(prior_b.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[1].hist(hyper_b[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[1].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[1].set_title(\u0026#34;$n = 10; \\\\alpha=2, \\\\beta=2$\u0026#34;) ax[2].hist(prior_c.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[2].hist(hyper_c[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[2].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;, label = \u0026#39;Arbitrary $\\\\theta$\u0026#39;) ax[2].set_title(\u0026#34;$n = 10; \\\\alpha=3, \\\\beta=1$\u0026#34;) ax[3].hist(prior_1.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[3].hist(hyper_1[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[3].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[3].set_title(\u0026#34;$n = 500; \\\\alpha=1, \\\\beta=2$\u0026#34;) ax[4].hist(prior_2.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[4].hist(hyper_2[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[4].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;) ax[4].set_title(\u0026#34;$n = 500; \\\\alpha=2, \\\\beta=2$\u0026#34;) ax[5].hist(prior_3.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Prior\u0026#34;) ax[5].hist(hyper_3[\u0026#34;theta\u0026#34;][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \u0026#34;Posterior\u0026#34;) ax[5].axvline(arbitrary_theta, linestyle = \u0026#39;--\u0026#39;, color = \u0026#39;k\u0026#39;, label = \u0026#39;Arbitrary $\\\\theta$\u0026#39;) ax[5].set_title(\u0026#34;$n = 500; \\\\alpha=3, \\\\beta=1$\u0026#34;) ax[5].legend() fig.text(0.5, 0.04, \u0026#34;$\\\\theta$\u0026#34;, ha = \u0026#39;center\u0026#39;) fig.text(0.08, 0.5, \u0026#34;Density\u0026#34;, va = \u0026#39;center\u0026#39;, rotation = 90) fig.set_size_inches((12,6)) In short, when given identical observation vectors, the prior does still have an effect on the shape of the posterior, but it is clear that with a large number of observations, the data dominates the shape.\nFinal thoughts and Limitations Enjoy reading this? :) I hope this was a nice intro to Bayesian inference! The take-aways really are:\n Bayesian inference is doable, but the learning curve is steep! Packages like pymc3 take away a lot of the leg-work, especially when your distributions aren\u0026rsquo;t mathematically nice to deal with. For real-world problems, they may not be as easily decomposable as we have in this example, but hopefully this gets you thinking about how Bayesians think about parameters in general. Conjugate priors and the details of MCMC haven\u0026rsquo;t been discussed but perhaps that\u0026rsquo;s for another time.  Here are some references I\u0026rsquo;ve come across and really benefited from when making this entry:\n Bayesian Methods for Hackers - hugely influential, I would say this formed a large chunk of my inspirations here. Bayesian Inference in One Hour - a solid set of lecutre notes from Patrick Lam. Refresher on Likelihood Some random SO / StackExchange threads, e.g. this one on credible intervals  ","permalink":"https://ideasbyjin.github.io/post/2019-12-25-santa-bayes/","summary":"It\u0026rsquo;s Christmas, and Santa is here! He\u0026rsquo;s got his list and he\u0026rsquo;s about to see who\u0026rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.\nSanta is in trouble. Without his list, it\u0026rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who\u0026rsquo;s been nice or naughty.","title":"Santa Bayes: a Christmas introduction to Bayesian inference"},{"content":"In a previous post, I covered arguably one of the most straight-forward clustering algorithms: hierarchical clustering. Remember that any clustering method requires a distance metric to quantify how \u0026ldquo;far apart\u0026rdquo; two points are placed in some N-dimensional space. While typically Euclidean, there\u0026rsquo;s loads of ways in doing this.\nGenerally, hierarchical clustering is a very good way of clustering your data, though it suffers from a couple of limitations:\n Users have to define the number of clusters The linkage criterion (UPGMA, Ward\u0026hellip;) can have a huge effect on the cluster shapes  Other clustering methods like K-means clustering also depend on the number of clusters to be determined beforehand, and it can be prone to hitting local minima.\nIn theory, no clustering method is perfect, as it is very much dependent on the shape of your data and use case. This diagram from Scikit-learn shows us exactly that:\nHowever, if I had to pick one clustering algorithm for any dataset, it would have to be DBSCAN (Density-based spatial clustering of applications with noise). Cheeky plug: I\u0026rsquo;ve written a paper on using it for clustering protein structures. Essentially, DBSCAN wins the \u0026ldquo;clustering trophy\u0026rdquo; for three reasons:\n It\u0026rsquo;s very intuitive to understand It\u0026rsquo;s very scalable It requires a distance parameter rather than the number of clusters  If you have\u0026hellip;\n 30 seconds: DBSCAN essentially requires a user to determine a distance parameter, $$\\epsilon$$.  $$P_i = \\begin{cases} \\text{Core} \u0026amp; \\text{if}\\ (\\sum_{j=1, j\\neq i}^{n} \\mathbb{1}(d_{P_i, P_j} \\leq \\epsilon)) \\geq m \\\n\\text{Reachable} \u0026amp; \\text{if}\\ 0 \u0026lt; (\\sum_{j=1, j\\neq i}^{n} \\mathbb{1}(d_{P_i, P_j} \\leq \\epsilon)) \u0026lt; m \\\n\\text{Outlier} \u0026amp; \\text{Otherwise.} \\end{cases}$$\nThe combinaiton of core and reachable points form a cluster, while outliers are\u0026hellip; outliers.\n 10 minutes: Read below.  To run DBSCAN, we first define some distance threshold, $$\\epsilon$$, and the minimum number of points, m, we need to form a cluster. Notice the slight difference to how we parameterise hierarchical clustering methods; instead of having a declaration such as\n I expect my dataset to have 10 clusters from 1000 points.\n This is more analogous to saying\n I expect my dataset of 1000 points to form bunches that are, at most, $\\epsilon$ apart.\n If you like LateX/equations, the i th point of a dataset, $$P_i$$, can be designated one of three things:\n$$P_i = \\begin{cases} \\text{Core} \u0026amp; \\text{if}\\ (\\sum_{j=1}^{n} \\mathbb{1}(d_{P_i, P_j} \\leq \\epsilon)) \\geq m \\\n\\text{Reachable} \u0026amp; \\text{if}\\ 0 \u0026lt; (\\sum_{j=1}^{n} \\mathbb{1}(d_{P_i, P_j} \\leq \\epsilon)) \u0026lt; m \\\n\\text{Outlier} \u0026amp; \\text{Otherwise.} \\end{cases} $$\nOne of the most helpful ways of understanding DBSCAN is using this diagram from Wikipedia:\nSuppose we had a minimum of 4 ($$m=4$$). Then,\n The red points are core points. For every red point, we see that, including itself, there are at least 4 red points within $$\\epsilon$$ (illustrated by the double-sided arrow). However, The two yellow points are only \u0026ldquo;reachable\u0026rdquo; as there\u0026rsquo;s less than m points within $$\\epsilon$$ (including itself). Hence the yellow points are not core points. And of course, the blue point is just a chillin' outlier.  The combination of red and yellow points form a cluster! We repeat this process for every point in our dataset, and we\u0026rsquo;re done. Now let\u0026rsquo;s run some code.\nData Acquisition and Cleaning # Import stuff import pandas as pd from sklearn.cluster import DBSCAN import numpy as np For this blog post, I am going to ingest a huge dataset of gene expression data from GTEx. The GTEx portal is a really interesting resource, which describes how genes are differentially expressed in specific tissues. This can be measured by a quantity known as TPM (Transcripts per million). The data itself was acquired by RNA sequencing from post-mortem samples. There\u0026rsquo;s metadata we could use to understand our data even better, but for simplicity, let\u0026rsquo;s stick to using only the TPM data, and ignore the metadata completely (though it could be useful for informing our clustering strategy).\nSince the file from GTEx is pretty big, we\u0026rsquo;re going to stream it and use some clever tricks!\nimport gzip import urllib import io # Let\u0026#39;s stream the data URL = \u0026#34;https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_tpm.gct.gz\u0026#34; urlHandle = urllib.request.urlopen(URL) # Create a Gzipped File handle - this allows line-by-line iteration fh = gzip.GzipFile(fileobj=urlHandle) Having dealt with GCT files before, with a weird format, we need to skip the first line, but the second line onward has some useful information.\nfh.readline() # skip first line dimensions = fh.readline().split() [int(x) for x in dimensions] [56200, 17382]  The GCT file format is interesting because it tells us the dimensions of the file; there are 56200 genes and 17382 samples. That\u0026rsquo;s a lot of genes to work with; not to mention quite a few samples, too. This of course means there are 56200 rows and 17382 columns, which could be pretty cumbersome for a laptop to handle. Not to mention downloading the entire thing all at once before we do anything else.\nTo trim it down a bit, let\u0026rsquo;s see what these samples are, first:\nDECODE_CODEC = \u0026#39;utf-8\u0026#39; def stream_request(url): \u0026#34;\u0026#34;\u0026#34; Open a connection to some url, then stream data from it This has the advantage of: A. We don\u0026#39;t have to wait for the entire file to download to do operations B. We can perform some operations on-the-fly \u0026#34;\u0026#34;\u0026#34; fh = urllib.request.urlopen(url) buffer = io.StringIO() for line in fh: decoded = line.decode(DECODE_CODEC) buffer.write(decoded) # Reset the StringIO buffer to byte position 0 buffer.seek(0) return buffer def stream_request_to_pandas(url: str, sep: str = \u0026#39;\\t\u0026#39;) -\u0026gt; pd.DataFrame: streamed_buffer = stream_request(url) return pd.read_csv(streamed_buffer, sep = sep) sampleUrl = \u0026#34;https://storage.googleapis.com/gtex_analysis_v8/annotations/GTEx_Analysis_v8_Annotations_SampleAttributesDS.txt\u0026#34; sample_df = stream_request_to_pandas(sampleUrl) # Just get a random 4 rows and the first 10 columns sample_df.iloc[np.random.randint(0, 1000, 4)][sample_df.columns[:10]] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  For sake of argument, I\u0026rsquo;m only going to get samples where the source was Blood, Blood Vessel, and Hear (SMTS == Blood Vessel|Blood|Heart).\ntissues_of_interest = sample_df[sample_df[\u0026#39;SMTS\u0026#39;].isin([\u0026#39;Blood\u0026#39;, \u0026#39;Blood Vessel\u0026#39;, \u0026#39;Heart\u0026#39;])] relevant_samples = tissues_of_interest[\u0026#39;SAMPID\u0026#39;].values # This will be useful later, trust me! # This is a dictionary of sample ID -\u0026gt; tissue location sample_id_to_smts = dict(tissues_of_interest[[\u0026#39;SAMPID\u0026#39;,\u0026#39;SMTS\u0026#39;]].values) sample_id_to_smtsd = dict(tissues_of_interest[[\u0026#39;SAMPID\u0026#39;,\u0026#39;SMTSD\u0026#39;]].values) Furthermore, I am only going to read the rows of data that have protein-coding genes.\ngeneUrl = \u0026#34;https://www.genenames.org/cgi-bin/download/custom?col=gd_app_sym\u0026amp;col=gd_pub_ensembl_id\u0026amp;status=Approved\u0026amp;hgnc_dbtag=on\u0026amp;order_by=gd_app_sym_sort\u0026amp;format=text\u0026amp;where=(gd_pub_chrom_map%20not%20like%20%27%25patch%25%27%20and%20gd_pub_chrom_map%20not%20like%20%27%25alternate%20reference%20locus%25%27)%0Aand%20gd_locus_type%20=%20%27gene%20with%20protein%20product%27\u0026amp;submit=submit\u0026#34; gene_df = stream_request_to_pandas(geneUrl) # Let\u0026#39;s get the Ensembl Gene IDs of things that are protein-coding gene_ensembl = gene_df[~pd.isnull(gene_df[\u0026#39;Ensembl gene ID\u0026#39;])][\u0026#39;Ensembl gene ID\u0026#39;].values OK. Now that we have:\n Protein coding genes, and Sample IDs that mark whether samples are from blood, heart, or blood vessel\u0026hellip;  We can extract what we want!\n# From that big GTEx file, let\u0026#39;s get the column names header = fh.readline().decode(\u0026#34;utf-8\u0026#34;).split(\u0026#39;\\t\u0026#39;) # Find out what columns (in terms of indices) these samples correspond to. # We want 0 and 1 by default because they are the Ensembl gene ID and Gene name header_indices = [0,1] + [ ix for ix, val in enumerate(header) if val in relevant_samples ] # Use numpy arrays because then we can use multiple integers for indexing! dataframe_columns = np.array(header)[header_indices] from tqdm.notebook import tqdm # nice little progress bar data = [] ENSEMBL_LENGTH = 15 # obtained from gene_ensembl for line in tqdm(fh): strline = line.decode(\u0026#39;utf-8\u0026#39;) if strline[:ENSEMBL_LENGTH] in gene_ensembl: tokens = np.array(strline.split(\u0026#39;\\t\u0026#39;)) data.append(tokens[header_indices]) # Create a pandas data-frame with our data expression = pd.DataFrame(data, columns = dataframe_columns) expression.iloc[np.random.randint(0, 1000, 4)] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  expression.shape (18619, 3127)  Understanding the Data # Let\u0026#39;s get the TPM values of the matrix vals = expression[expression.columns[2:]].astype(float).values.T Let\u0026rsquo;s consider what we have here. We got the TPM values from the dataframe, converted it to float, then took the transpose of this matrix. This means we now have samples per row and genes per column.\nEvery clustering algorithm needs to have some type of distance metric to compute how far apart two objects are. If we had had the genes in rows, this would have meant we\u0026rsquo;re calculating how far apart two genes are (based on TPM), though the tranpose shows how far apart two samples are.\nThis then means that we\u0026rsquo;re going to cluster on the basis of the distances between samples, rather than genes. This was a deliberate choice to see if DBSCAN can roughly identify three clusters that correspond to - you guessed it, Blood, Blood Vessel, and Heart.\n Just as an FYI, of course you can just cluster on the genes, though interpretation could be tricky. In fact, one could even do bi-clustering for something like this, but I digress\u0026hellip;\n Another nice aspect of this is computational scalability. Previously we had a $$n\\times k$$ matrix of $$n$$ genes and $$k$$ samples ($$18619 \\times 3127$$). The distance matrix of genes would be $$n \\times n$$, while a distance matrix of samples is just $$k \\times k$$.\nLet\u0026rsquo;s see what the range of TPM values can look like - this can help us understand what the distances would be like.\n%matplotlib inline import matplotlib.pyplot as plt # Let\u0026#39;s plot a heatmap first, as it\u0026#39;s a matrix plt.imshow(vals, interpolation=\u0026#39;nearest\u0026#39;, cmap = \u0026#39;Blues\u0026#39;) \u0026lt;matplotlib.image.AxesImage at 0x1c4ce20978\u0026gt;  Hmm\u0026hellip; that doesn\u0026rsquo;t look quite right. How about we flatten out those values and create a histogram?\n# Flatten to a 1-dimensional vector. valarray = vals.flatten() min_tpm = np.min(valarray) max_tpm = np.max(valarray) mean_tpm = np.mean(valarray) median_tpm = np.median(valarray) # Draw the histogram o = plt.hist(valarray, bins = np.arange(0, 1001, 10)) print(\u0026#34;Min: {}, Max: {}, Mean: {}, Median: {}\u0026#34;.format(min_tpm, max_tpm, mean_tpm, median_tpm)) Min: 0.0, Max: 747400.0, Mean: 50.455648440229176, Median: 3.181  OK, so the distribution is very skewed, with some values having very, very high values, and with nearly the entire dataset falling within 100TPM. One way to go around this is to use log transformation using a pseudocount of 1. $$\\log_\\text{TPM} = log(\\text{TPM}+1)$$.\nBy giving everything a pseudo-count of 1, this means that genes with 0 TPM in some samples will be transformed to 0, while for other genes, this shouldn\u0026rsquo;t have a huge effect.\nlogTpm = np.log(vals+1) logTpmArray = logTpm.flatten() fig, ax = plt.subplots(1,2) ax[0].hist(logTpmArray) ax[1].imshow(logTpm, interpolation=\u0026#39;nearest\u0026#39;, cmap = \u0026#39;Blues\u0026#39;) fig.set_size_inches((10,5)) Now that looks a lot better. We will calculate the pairwise distances based on this log TPM value.\n# pairwise distances - use default of Euclidean though there are various ways of doing this # This will take a while since there\u0026#39;s ~3000 pairwise distances to compute. from scipy.spatial.distance import pdist dmat = pdist(logTpm) # Let\u0026#39;s look at the distribution of distances o = plt.hist(dmat) MDS + DBSCAN For visualisation of the clusters, we\u0026rsquo;re going to create a multi-dimensional scaling plot. In a nutshell, when given some distances between objects, MDS tries to reconstruct where various points sit with respect to each other.\nThe analogy is something like, if you know the pairwise distances between New York, London, Tokyo, Sydney, and Dubai, MDS tries to figure out where those cities would sit (in a coordinate sense) from using just the distances alone.\nfrom scipy.spatial.distance import squareform # Convert distance matrix from pdist into a square matrix sqmat = squareform(dmat) # Import some stuff from sklearn.cluster import DBSCAN from sklearn.manifold import MDS # Initialise an MDS object, this allows us to visualise points in space mds = MDS(dissimilarity=\u0026#39;precomputed\u0026#39;) # This step can take a while np.random.seed(0) coords = mds.fit_transform(sqmat) # Let\u0026#39;s see what the MDS generates for the samples plt.scatter(coords[:,0], coords[:,1]) \u0026lt;matplotlib.collections.PathCollection at 0x1c51a4cb00\u0026gt;  Now that we know where the samples sit with respect to each other, let\u0026rsquo;s see what the DBSCAN algorithm generates. We mentioned earlier that the DBSCAN algorithm depends on some distance parameter, $\\epsilon$, to determine how objects are clustered together.\n For large values of $$\\epsilon$$, objects will all be consumed into one cluster For small values of $$\\epsilon$$, objects will break down into individual singletons  So then, how do we choose a good value of $$\\epsilon$$? This is usually achievable by an algorithm called OPTICS, but looking at our distance histogram from earlier, we can see that 75 could be a reasonable choice.\nfrom sklearn.cluster import DBSCAN THRESHOLD = 75 dbscan = DBSCAN(metric=\u0026#39;precomputed\u0026#39;, eps = THRESHOLD) # we calculated the distance already dbscan.fit(sqmat) print(set(dbscan.labels_)) {0, 1, 2, -1}  This is an interesting result; there are some outliers (label -1), but it found three distinct clusters even if I did not specify that there would be three clusters! How neat. Just for sake of argument, we can see how this result changes for very large or very small values of epsilon:\ndbscan_large = DBSCAN(metric=\u0026#39;precomputed\u0026#39;, eps = 250) # we calculated the distance already dbscan_large.fit(sqmat) dbscan_small = DBSCAN(metric=\u0026#39;precomputed\u0026#39;, eps = 1) # we calculated the distance already dbscan_small.fit(sqmat) print(\u0026#34;These are the clusters for large EPS: {}\u0026#34;.format(set(dbscan_large.labels_))) print(\u0026#34;These are the clusters for small EPS: {}\u0026#34;.format(set(dbscan_small.labels_))) These are the clusters for large EPS: {0} These are the clusters for small EPS: {-1}  So it seems like the optimal epsilon value sits somewhere between 1 and 250. Let\u0026rsquo;s visualise using our original results.\n# Colour points based on cluster membership cols = {0: \u0026#39;#316fba\u0026#39;, 1: \u0026#39;#e8291c\u0026#39;, 2: \u0026#39;#77ff77\u0026#39;, -1:\u0026#39;black\u0026#39;} colours = [ cols[c] for c in dbscan.labels_ ] # Now let\u0026#39;s see what the clusters look like plt.scatter(coords[:,0], coords[:,1], c = colours) \u0026lt;matplotlib.collections.PathCollection at 0x1c51d6eb38\u0026gt;  That\u0026rsquo;s beautiful isn\u0026rsquo;t it? Notice how the green dots are situated in two distinct areas of this MDS plot. This is not a surprising behaviour because we\u0026rsquo;re plotting the samples in a two-dimensional MDS plot, which is a reconstruction from the distance matrix we gave. In fact, in a higher-dimensional space, it\u0026rsquo;s likely that these green dots are indeed close, but beyond three dimensions it\u0026rsquo;s slightly hard to visualise dots.\nAnyway, let\u0026rsquo;s see if our clusters vaguely capture the cellular locations.\nsample_ids_used = expression.columns[2:] tissues_represented = {0: set(), 1: set(), 2: set(), -1: set()} for i, label in enumerate(dbscan.labels_): sample_id = sample_ids_used[i] tissue = sample_id_to_smts[sample_id] tissues_represented[label].add(tissue) tissues_represented {0: {'Blood Vessel', 'Heart'}, 1: {'Blood'}, 2: {'Blood'}, -1: {'Blood', 'Blood Vessel'}}  This is a very neat result. It seems that samples from the Blood Vessel and Heart have similar tissue expression patterns, though Blood can be broken down to two separate clusters. Let\u0026rsquo;s look at the finer location of the tissue (based on the SMTSD column from GTEx rather than SMTS)\u0026hellip;\nfiner_represented = {0: set(), 1: set(), 2: set(), -1: set()} for i, label in enumerate(dbscan.labels_): sample_id = sample_ids_used[i] smtsd = sample_id_to_smtsd[sample_id] finer_represented[label].add(smtsd) finer_represented {0: {'Artery - Aorta', 'Artery - Coronary', 'Artery - Tibial', 'Heart - Atrial Appendage', 'Heart - Left Ventricle'}, 1: {'Whole Blood'}, 2: {'Cells - EBV-transformed lymphocytes'}, -1: {'Artery - Tibial', 'Whole Blood'}}  Very interesting indeed. So the arteries are close to the heart (aorta, coronary artery), and we see these samples clustering together. On the other hand, even though samples are designated as belonging to \u0026ldquo;blood\u0026rdquo;, we see that there are two sub-samples of whole blood or from EBV-transformed lymphocytes. This is very neat.\nAgain, this shows how, even when I haven\u0026rsquo;t specified the number of clusters, but merely a distance, the algorithm detects three clusters, which broadly correspond to the three regions that we had specified earlier.\nHopefully this shows you all how cool DBSCAN can be, and coupled with MDS, gives us an intuition for where samples can lie in some 2-dimensional manifold.\n","permalink":"https://ideasbyjin.github.io/post/2019-12-18-clustering-2/","summary":"In a previous post, I covered arguably one of the most straight-forward clustering algorithms: hierarchical clustering. Remember that any clustering method requires a distance metric to quantify how \u0026ldquo;far apart\u0026rdquo; two points are placed in some N-dimensional space. While typically Euclidean, there\u0026rsquo;s loads of ways in doing this.\nGenerally, hierarchical clustering is a very good way of clustering your data, though it suffers from a couple of limitations:\n Users have to define the number of clusters The linkage criterion (UPGMA, Ward\u0026hellip;) can have a huge effect on the cluster shapes  Other clustering methods like K-means clustering also depend on the number of clusters to be determined beforehand, and it can be prone to hitting local minima.","title":"Clustering Gene Expression Data using DBSCAN"},{"content":"Last time I covered a section on clustering, a group of unsupervised learning methods – so called because they are not given the class memberships of the data$$^\\dagger$$. Don\u0026rsquo;t worry, I will do more posts on clustering soon. For now I wanted to give a quick overview of what supervised methods look like. For that, let\u0026rsquo;s look at the statistics of hockey players!\n$$\\dagger$$: this is a gross generalisation. More formally, for some dataset $$\\mathbf{X}$$, if we are trying to predict an output variable $$\\mathbf{Y}$$, we use supervised learning methods, otherwise unsupervised learning methods.\nHockey (on ice, obviously!) is a game where we have 5 skaters and 1 goalie per rotation. The 5 skaters can be divided into three$$^\\ddagger$$ subclasses:\n The wingers (LW, RW) The centre (C) Defensemen (D)  $$\\ddagger$$: the centre and wingers can also be bundled up as forwards.\nTypically, each skater\u0026rsquo;s actions are recorded, which include:\n Goal(s) scored, assist(s) made [i.e. did the player make a pass leading up to the goal?] Penalties in minutes (infringements typically lead to 2 minute bans from the game) Ice time in 5-on-5 or \u0026lsquo;penalty kill\u0026rsquo; situations Shots blocked etc.  Usually by looking at these statistics, one can have an approximate idea of the position a given hockey player plays. To many, this might seem like a pretty easy problem. Surely forwards are supposed to score goals! Defensemen are supposed to block shots!\nWait, so why do you want to know a player\u0026rsquo;s position? Predicting a player\u0026rsquo;s position is perhaps not the first classification that comes to mind. However, it\u0026rsquo;s useful for something like fantasy sports leagues. In fantasy sports, you typically have roster slots for n centres, m defensemen, and q wingers. Using those constraints, you want to (usually) maximise every statistical category.\nFor example, if a fantasy team has a bunch of goal-scoring centremen, which position player do we pick out next to max out the penalties in minutes category? Simultaneously, which position also happens to maximise the number of assists? Do we pick up a defenseman, or a gritty right wing?\nHockey is slightly more complex than meets the eye. Typically, being a defenseman or forward can largely constrain your statistical profile; there are some defensemen that are very talented on offense (e.g. Morgan Reilly), and some forwards who are tougher, and deployed on a \u0026ldquo;checking\u0026rdquo; line to provide strength.\n(This is Morgan Reilly.)\nSo, how can we predict positions using stats? Let\u0026rsquo;s find out.\nIf you have 30 seconds\u0026hellip;\n This post is really a demo of various supervised learning methods. See table below for a quick overview. The algorithm of choice is often dependent on use case, and you should consider questions like:  What is the distribution of your output? How can we interpret the model?    If you have 10 minutes\u0026hellip;\n Read on. I\u0026rsquo;ve tried to section this entry based on what bits you might be interested in. This post is very much intended to be a whirlwind tour of the various supervised learning methods, rather than a deep-dive.  import pandas as pd import matplotlib.pyplot as plt import numpy as np Data cleanup # Read in the data from Kaggle df = pd.read_csv(\u0026#34;game_skater_stats.csv\u0026#34;) # We\u0026#39;ll use this later. pinfo = pd.read_csv(\u0026#34;player_info.csv\u0026#34;) df.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  The skater stats are given per player (player_id), and per game (game_id) that they have played. We know from some good documentation that:\n The first four digits represent the season (e.g. 2010-2011 season) The next two digits represent whether the game was held in the regular season or playoffs, etc.  What we will do is some clever pandas magic to:\n Only use regular season games Aggregate the statistics per player  # Filter for regular season and annotate season ID # https://github.com/dword4/nhlapi#game-ids df[\u0026#39;game_id\u0026#39;] = df[\u0026#39;game_id\u0026#39;].astype(str) reg_season = df[df[\u0026#39;game_id\u0026#39;].apply(lambda x: x[4:6] == \u0026#34;02\u0026#34;)].copy() reg_season[\u0026#39;Season\u0026#39;] = reg_season[\u0026#39;game_id\u0026#39;].apply(lambda x: x[:4]) reg_season.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  # aggregate stats and use average for time; otherwise totals # We could do this by season but we\u0026#39;ll stick to overall totals for simplification. aggregated_stats = reg_season.groupby(\u0026#39;player_id\u0026#39;).agg( { \u0026#34;game_id\u0026#34;: len, # use this as an aggregating function to get number of games played \u0026#34;timeOnIce\u0026#34;: \u0026#34;mean\u0026#34;, \u0026#34;goals\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;assists\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;shots\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;hits\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;powerPlayGoals\u0026#34;:\u0026#34;sum\u0026#34;, \u0026#34;powerPlayAssists\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;penaltyMinutes\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;faceOffWins\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;faceoffTaken\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;takeaways\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;giveaways\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;shortHandedGoals\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;shortHandedAssists\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;blocked\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;plusMinus\u0026#34;: \u0026#34;sum\u0026#34;, \u0026#34;evenTimeOnIce\u0026#34;: \u0026#34;mean\u0026#34;, \u0026#34;shortHandedTimeOnIce\u0026#34;: \u0026#34;mean\u0026#34;, } ) aggregated_stats.columns = [\u0026#39;games_played\u0026#39;] + list(aggregated_stats.columns[1:]) aggregated_stats.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Let\u0026rsquo;s do some more feature engineering to make our lives easier\u0026hellip;\n# Powerplay and shorthanded goals/assists are typically much lower than regular goals/assists, so it\u0026#39;s convenient to take the sum. # Faceoffs are typically reported in percentages, anyway. aggregated_stats[\u0026#39;powerPlayPoints\u0026#39;] = aggregated_stats[\u0026#39;powerPlayGoals\u0026#39;] + aggregated_stats[\u0026#39;powerPlayAssists\u0026#39;] aggregated_stats[\u0026#39;shortHandedPoints\u0026#39;] = aggregated_stats[\u0026#39;shortHandedGoals\u0026#39;] + aggregated_stats[\u0026#39;shortHandedAssists\u0026#39;] # Since some players never take faceOffs, just stick to 0 to avoid zero division errors percentage = (aggregated_stats[\u0026#39;faceOffWins\u0026#39;] / aggregated_stats[\u0026#39;faceoffTaken\u0026#39;])*100 percentage = [ _ if not np.isnan(_) else 0 for _ in percentage ] aggregated_stats[\u0026#39;faceOffPercentage\u0026#39;] = percentage aggregated_stats.drop(columns=[\u0026#39;powerPlayGoals\u0026#39;, \u0026#39;powerPlayAssists\u0026#39;, \u0026#39;shortHandedGoals\u0026#39;, \u0026#39;shortHandedAssists\u0026#39;, \u0026#39;faceOffWins\u0026#39;, \u0026#39;faceoffTaken\u0026#39;]).head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Finally, for each player, if they have played fewer than 41 games, let\u0026rsquo;s remove them. I chose the number 41 because there are 82 games in a season. I want to know that a player has played at least half a season\u0026rsquo;s worth of games, otherwise we would have very little data to work with.\nsufficient_games = [] for n,g in aggregated_stats.groupby(\u0026#39;player_id\u0026#39;): if g[\u0026#39;games_played\u0026#39;].sum() \u0026gt;= 41: sufficient_games.append(n) final_stats = aggregated_stats[aggregated_stats.index.get_level_values(\u0026#34;player_id\u0026#34;).isin(sufficient_games)].copy() final_stats_players = final_stats.index.get_level_values(\u0026#39;player_id\u0026#39;) final_stats.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Train, validate, test In machine learning, training, validating, and testing your model is a fundamental piece of the puzzle. Without proper splits of your data, there is a potential to overfit your model to the training set. Furthermore, the split datasets should have similar distributions of classes so that you avoid overfitting/over-penalisation, too. For the sake of this blog, I will only split by training and testing, and make one split. There are other split strategies like $$k$$-fold cross-validation but\u0026hellip; we won\u0026rsquo;t talk about that for now. Back to topic!\nSplitting is best done using sklearn\u0026rsquo;s builtin train-test splitter:\nfrom sklearn.model_selection import train_test_split # Get skater data from pinfo skaters = pinfo[pinfo[\u0026#39;primaryPosition\u0026#39;] != \u0026#34;G\u0026#34;][[\u0026#39;player_id\u0026#39;, \u0026#39;firstName\u0026#39;, \u0026#39;lastName\u0026#39;, \u0026#39;primaryPosition\u0026#39;]].copy() skaters = skaters[skaters[\u0026#39;player_id\u0026#39;].isin(final_stats_players)] # the stratify argument makes sure we split our dataset # so that even though the test set is 1/3 the size of the training set # it has a similar distribution of wingers, defensemen... etc. # let\u0026#39;s use a seed of 0. training_ids, test_ids = train_test_split(skaters[\u0026#39;player_id\u0026#39;], random_state = 0, test_size = 0.25, stratify = skaters[\u0026#39;primaryPosition\u0026#39;]) # get the training set of data. # Since aggregated_stats is aggregated on both player id and season, # we have a multi-index object. this is a way to search on one column of that index. playerIdIndex = aggregated_stats.index.get_level_values(\u0026#34;player_id\u0026#34;) # Get the training set and test set of data. training_set = aggregated_stats[playerIdIndex.isin(training_ids)].copy() test_set = aggregated_stats[playerIdIndex.isin(test_ids)].copy() training_set.head(3) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Normally for machine learning methods, we can do some form of feature selection to use the most relevant variables. I am going to do the slightly naive approach of using every possible variable for prediction. This is something that may be done in practice, though it\u0026rsquo;s not the most clever idea; variables can be correlated. Here I\u0026rsquo;m going to do what\u0026rsquo;s called a \u0026ldquo;pairplot\u0026rdquo;:\ndef pairplot(columns, names): n_col = columns.shape[1] fig, ax = plt.subplots(n_col, n_col) short_names = { \u0026#39;timeOnIce\u0026#39;: \u0026#34;time\u0026#34;, \u0026#39;goals\u0026#39;: \u0026#34;goals\u0026#34;, \u0026#39;assists\u0026#39;: \u0026#34;assists\u0026#34;, \u0026#39;shots\u0026#39;: \u0026#34;shots\u0026#34;, \u0026#39;hits\u0026#39;: \u0026#34;hits\u0026#34;, \u0026#39;penaltyMinutes\u0026#39;: \u0026#34;PIM\u0026#34;, \u0026#39;powerPlayPoints\u0026#39;: \u0026#34;PPP\u0026#34;, \u0026#39;shortHandedPoints\u0026#39;: \u0026#34;SHP\u0026#34; } # Upper-triangular matrix shows correlation between variables for i in range(0, n_col-1): for j in range(i+1, n_col): ax[i,j].scatter(columns[:,i], columns[:,j]) if j - i \u0026gt; 0: ax[i,j].get_yaxis().set_ticklabels([]) ax[i,j].get_xaxis().set_ticklabels([]) if i == 0: ax[i,j].set_title(\u0026#34;{}\u0026#34;.format(short_names[names[j]])) if j == n_col-1: ax[i,j].yaxis.set_label_position(\u0026#34;right\u0026#34;) ax[i,j].set_ylabel(\u0026#34;{}\u0026#34;.format(short_names[names[i]])) # Diagonal contains histograms for i in range(0, n_col): for j in range(0, n_col): if i != j: continue ax[i,j].hist(columns[:,i], color = \u0026#39;#ffd700\u0026#39;) if i == 0: ax[i,j].set_title(\u0026#34;{}\u0026#34;.format(short_names[names[j]])) elif j == (n_col-1): ax[i,j].set_xlabel(\u0026#34;{}\u0026#34;.format(short_names[names[j]])) # Lower-triangular matrix is hidden for i in range(1, n_col): for j in range(0, i): ax[i,j].axis(\u0026#34;off\u0026#34;) return fig, ax columns = [\u0026#39;timeOnIce\u0026#39;, \u0026#39;goals\u0026#39;, \u0026#39;assists\u0026#39;, \u0026#39;shots\u0026#39;, \u0026#39;hits\u0026#39;, \u0026#39;penaltyMinutes\u0026#39;, \u0026#39;powerPlayPoints\u0026#39;, \u0026#39;shortHandedPoints\u0026#39;] fig, ax = pairplot(training_set[columns].values, columns) fig.set_size_inches((10,10)) # Get the names of the players train_skaters = skaters[skaters[\u0026#39;player_id\u0026#39;].isin(training_ids)].copy() test_skaters = skaters[skaters[\u0026#39;player_id\u0026#39;].isin(test_ids)].copy() # Create a dictionary of player IDs to positions, this makes label creation easier train_position = dict(train_skaters[[\u0026#39;player_id\u0026#39;,\u0026#39;primaryPosition\u0026#39;]].values) test_position = dict(test_skaters[[\u0026#39;player_id\u0026#39;,\u0026#39;primaryPosition\u0026#39;]].values) # Get \u0026#34;labels\u0026#34; which are the hockey players\u0026#39; positions. train_labels = [train_position[pid] for pid in training_set.index.get_level_values(\u0026#39;player_id\u0026#39;)] test_labels = [test_position[pid] for pid in test_set.index.get_level_values(\u0026#39;player_id\u0026#39;)] The \u0026ldquo;ML bit\u0026rdquo; For this exercise, I am going to use the following supervised learning methods; below is a summary along with some pros and cons of each method. I\u0026rsquo;ve also tried to write equations where appropriate.\n Logistic Regression – Applies the logistic (binary classes) or softmax (multiple) function to a linear combination of weighted variables to predict the probability of class membership.  Pros: Model is fairly simple to interpret, with flexibility for regularisation$$\\dagger$$. Cons: Assumes a linear relationship between features (after logistic transformation) to class membership $$Pr(Y = c) = \\dfrac{ e^{z_c}}{\\sum_{i=1}^C e^{z_i} } ~~\\mathrm{where}~~ z_i = w_iX+b_i.$$   Naive Bayes Classifier – applies \u0026ldquo;Bayes' rule\u0026rdquo; to estimate the probability of belonging to a class.  Pros: Typically shows good performance and is inexpensive to run. Cons: Assumes that each feature is independent of another $$Pr(Y = c|x_1, x_2\u0026hellip; x_n) \\propto P(c) \\prod_{i=1}^{C} Pr(x_i|c)$$   Random Forest Classifier – bootstraps$\\ddagger$ the dataset to create a series of decision trees (the \u0026ldquo;forest\u0026rdquo;). New data is then predicted according to all the decision trees, and we take the average prediction. In the case of classification, we take the majority vote.  Pros: Possible to trace the importance of specific features using the Gini index; very stable performance. Cons: Difficult to trace how the decision trees were made. For regression, $$\\hat{f} = \\dfrac{1}{T} \\sum_{i=1}^{T} f_i(X_{test})$$   Support Vector machines – finds a hyperplane that best separates classes in a dataset.  Pros: coupled with a kernel function, can be applicable for non-linear datasets Cons: sometimes a \u0026ldquo;soft\u0026rdquo; margin is required    $$\\dagger$$: \u0026ldquo;regularisation\u0026rdquo; is a technique where the weights of some terms are shrunk; examples include Lasso and Ridge.\n$$\\ddagger$$: \u0026ldquo;bootstrap\u0026rdquo; here refers to statistical bootstrapping where we sample with replacement.\n# Let\u0026#39;s get some classifiers from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB # assumes that P(x_i |y) is a Gaussian distribution from sklearn.svm import LinearSVC, SVC from sklearn.ensemble import RandomForestClassifier For any classification, we need some mechanism of calculating the performance of our models. There are many measures one can use, but for this exercise, we will simply calculate the accuracy, which is likely the easiest to interpret in this type of whistle-tour blog post. For a pretty visualisation, I will plot the predictions in what\u0026rsquo;s called a \u0026ldquo;confusion matrix\u0026rdquo;, which shows the distribution of predictions vs. the true answers.\nfrom sklearn.metrics import confusion_matrix from matplotlib import cm def accuracy(true, pred): assert pred.shape == true.shape, \u0026#34;Shape of pred and true arrays should be the same!\u0026#34; return (pred == true).sum() / pred.shape[0] def get_confusion_matrix(true,pred): label_list = list(set(pred) | set(true)) return confusion_matrix(pred,true,labels=label_list), label_list def plot_confusion_matrix(cmat, labels, cmap = cm.Greens): \u0026#34;\u0026#34;\u0026#34; Plot a heatmap \u0026#34;\u0026#34;\u0026#34; fig, ax = plt.subplots() ax.imshow(cmat, cmap = cmap) n_labels = len(labels) ticklocs = np.arange(n_labels) ax.set_xticks(ticklocs) ax.set_yticks(ticklocs) ax.set_xticklabels(labels) ax.set_yticklabels(labels) ax.set_xlim(min(ticklocs)-0.5, max(ticklocs)+0.5) ax.set_ylim(min(ticklocs)-0.5, max(ticklocs)+0.5) ax.set_xlabel(\u0026#34;Predicted\u0026#34;) ax.set_ylabel(\u0026#34;True\u0026#34;) color_threshold = np.max(cmat) * 0.75 for i in range(cmat.shape[0]): for j in range(cmat.shape[1]): value = cmat[i,j] if value \u0026gt;= color_threshold: ax.text(j, i, cmat[i,j], color = \u0026#39;white\u0026#39;, ha = \u0026#39;center\u0026#39;, va = \u0026#39;center\u0026#39;) else: ax.text(j, i, cmat[i,j], ha = \u0026#39;center\u0026#39;, va = \u0026#39;center\u0026#39;) return fig, ax Logistic Regression For the purpose of this exercise I am going to use the (default) logistic regression with the $$l_2$$ penalty (also known as Ridge regression). I won\u0026rsquo;t go into too many of the mathematical details here but an important hyper-parameter of the method is the regularisation strength, $$\\lambda$$. The higher the value of $$\\lambda$$, this ultimately shrinks the weights closer to 0.\nlm = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;,multi_class=\u0026#39;multinomial\u0026#39;, C = 0.1) lm.fit(training_set.values, train_labels) LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='multinomial', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)  pred = lm.predict(test_set.values) print(\u0026#34;Accuracy of logistic regression is: {}\u0026#34;.format(accuracy(np.array(pred), np.array(test_labels)))) Accuracy of logistic regression is: 0.7105263157894737  # plot the confusion matrix cmat, label_list = get_confusion_matrix(test_labels,pred) fig, ax = plot_confusion_matrix(cmat, label_list) plt.show() # Plot pred vs. true in a PCA plot from sklearn.decomposition import PCA def pca_plot(pred, method): # scale the data tv = (test_set - test_set.mean()) / test_set.std() pca = PCA() new_data = pca.fit_transform(tv) colors = { \u0026#34;RW\u0026#34;: \u0026#34;#ffd700\u0026#34;, \u0026#34;D\u0026#34;: \u0026#34;#1348ae\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;#90ee90\u0026#34;, \u0026#34;LW\u0026#34;: \u0026#34;#e8291c\u0026#34; } true_labels_to_colors = [ colors[pos] for pos in test_labels ] pred_labels_to_colors = np.array([ colors[pos] for pos in pred ]) fig, ax = plt.subplots(1,2, sharey=True) ax[0].scatter(new_data[:,0], new_data[:,1], alpha = 0.5, color = true_labels_to_colors) #  for lab in set(pred): pos_idx = np.argwhere(pred == lab).flatten() ax[1].scatter(new_data[pos_idx,0], new_data[pos_idx,1], color = pred_labels_to_colors[pos_idx], alpha = 0.5, label = lab) ax[0].set_title(\u0026#34;True labels\u0026#34;) ax[1].set_title(\u0026#34;Predicted labels\u0026#34;) ax[1].legend(loc = \u0026#39;upper left\u0026#39;, ncol = 2) fig.suptitle(method) fig.set_size_inches((10,5)) return fig, ax fig, ax = pca_plot(pred, \u0026#34;Logistic Regression\u0026#34;) Random Foest As before, I will just use the default implementation.\n# let\u0026#39;s train a \u0026#34;simple\u0026#34; random forest rf = RandomForestClassifier() rf.fit(training_set.values, train_labels) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  pred = rf.predict(test_set.values) print(\u0026#34;Accuracy of random forest is: {}\u0026#34;.format(accuracy(np.array(pred), np.array(test_labels)))) Accuracy of random forest is: 0.6929824561403509  cmat, label_list = get_confusion_matrix(test_labels, pred) fig, ax = plot_confusion_matrix(cmat, label_list) fig, ax = pca_plot(pred, \u0026#34;Random Forest\u0026#34;) Naive Bayes Classifier For the NBC, I will again use the default implementation but assume that every variable has a Gaussian distribution. This is not ideal by any means, but is easiest to code and gives you a flavour of what it does.\n# let\u0026#39;s train a \u0026#34;simple\u0026#34; naive bayes classifier nbc = GaussianNB() nbc.fit(training_set.values, train_labels) GaussianNB(priors=None, var_smoothing=1e-09)  pred = nbc.predict(test_set.values) print(\u0026#34;Accuracy of Naive Bayes classifier is: {}\u0026#34;.format(accuracy(np.array(pred), np.array(test_labels)))) Accuracy of Naive Bayes classifier is: 0.5847953216374269  fig, ax = pca_plot(pred, \u0026#34;NBC\u0026#34;) Support Vector Machines Here I will use the LinearSVC class; essentially we are applying a linear kernel to the data. What this means is that essentially we are assuming that no transformation is needed to draw a hyperplane that will separate the data.\nsvc = LinearSVC(max_iter=2000) svc.fit(training_set.values, train_labels) LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=2000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0)  pred = svc.predict(test_set.values) print(\u0026#34;Accuracy of SVC is: {}\u0026#34;.format(accuracy(np.array(pred), np.array(test_labels)))) Accuracy of SVC is: 0.7017543859649122  cmat, label_list = get_confusion_matrix(test_labels, pred) fig, ax = plot_confusion_matrix(cmat, label_list) plt.show() fig, ax = pca_plot(pred, \u0026#34;SVM\u0026#34;) Revision No method was a true outstanding performer. While the random forest classifier did have the highest level of accuracy, it was only marginally better than logistic regression.\nIt would be worth seeing why certain methods failed to classify a player into the correct primary position. We could go more in-depth and ask,\n Is this a case where we over-penalise ourselves (e.g. left-wing vs. right-wing players are not that different)? Is this a case where a player has out-of-position behaviours (e.g. a defenseman with some high goals/assists? a forward who is a defensive specialist?) Is there not enough game data?  Going further, we can ask\u0026hellip;\n Are there fundamental aspects of the ML methods tested here that make it unsuitable for this problem? Can we do feature selection of some sort? What other information can we get to improve prediction? For example, does stick handed-ness have any bearing on position?  from scipy.stats import gaussian_kde test_set_copy = test_set.copy() test_set_copy[\u0026#39;pred\u0026#39;] = rf.predict(test_set_copy) test_to_names = pd.merge( left = test_set_copy, right = skaters, how = \u0026#39;inner\u0026#39;, on = \u0026#39;player_id\u0026#39; ) correct = test_to_names[test_to_names[\u0026#39;primaryPosition\u0026#39;]==test_to_names[\u0026#39;pred\u0026#39;]].copy() incorrect = test_to_names[test_to_names[\u0026#39;primaryPosition\u0026#39;]!=test_to_names[\u0026#39;pred\u0026#39;]].copy() incorrect[[\u0026#39;firstName\u0026#39;, \u0026#39;lastName\u0026#39;, \u0026#39;primaryPosition\u0026#39;, \u0026#39;pred\u0026#39;]].head(5) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  correct_gp = correct[\u0026#39;games_played\u0026#39;].values incorrect_gp = incorrect[\u0026#39;games_played\u0026#39;].values # We can create a Gaussian kernel on top of the number of games played to do some comparisons correct_kde = gaussian_kde(correct_gp) incorrect_kde = gaussian_kde(incorrect_gp) num_games = np.arange(1, 801) fig, ax = plt.subplots(1,1) ax.plot(num_games, correct_kde.evaluate(num_games), color = \u0026#39;#134a8e\u0026#39;, label = \u0026#34;Correct predictions\u0026#34;) ax.plot(num_games, incorrect_kde.evaluate(num_games), color = \u0026#39;#e8291c\u0026#39;, label = \u0026#34;Incorrect predictions\u0026#34;) ax.set_xlabel(\u0026#34;Number of games played\u0026#34;) ax.set_ylabel(\u0026#34;Density\u0026#34;) Text(0, 0.5, 'Density')  What\u0026rsquo;s interesting here is that:\n For the random forest, mis-classifications are only found for forwards (no defensemen are ever classified as forwards and vice-versa). There are more winger mis-classifications (actual = RW, predicted = LW), which may imply a too-stringent classification scheme. This doesn\u0026rsquo;t seem to be affected by the number of games played by the players as they have similar distributions.  While we can explore the data further to explain misclassifications, I think that\u0026rsquo;s outside the scope of this post and that\u0026rsquo;s for next time\u0026hellip;\n","permalink":"https://ideasbyjin.github.io/post/2019-11-01-supervised/","summary":"Last time I covered a section on clustering, a group of unsupervised learning methods – so called because they are not given the class memberships of the data$$^\\dagger$$. Don\u0026rsquo;t worry, I will do more posts on clustering soon. For now I wanted to give a quick overview of what supervised methods look like. For that, let\u0026rsquo;s look at the statistics of hockey players!\n$$\\dagger$$: this is a gross generalisation. More formally, for some dataset $$\\mathbf{X}$$, if we are trying to predict an output variable $$\\mathbf{Y}$$, we use supervised learning methods, otherwise unsupervised learning methods.","title":"Supervised learning demo: what position do I play?"},{"content":"During my PhD and postdoc, my main day-to-day was driven by one question:\n How do we make the best model protein structures?\n To answer that question, this is often done by calculating the root-mean square deviation (RMSD) between the predicted structure vs. the known \u0026lsquo;true\u0026rsquo; protein structure. There are other measures (e.g. TM-score, GDT_TS), but RMSD is still the most intuitive, and (unfortunately?) the accepted standard metric for goodness-of-fit.\n RMSD has some flaws; for example, it suffers from size-dependent effects, but we will discuss this another time.  If you have less than 3 minutes\n Singular-value decomposition (SVD) allows us to find the optimal rotation and translation matrices that best aligns the predicted structure to the true protein structure. Following alignment, it\u0026rsquo;s a case of just computing $$RMSD = \\sqrt{\\dfrac{1}{n} \\sum_{i=1}^{n} d_i}$$ where $$d_i = (x_{i,nat}-x_{i, model})^2 + (y_{i,nat}-y_{i, model})^2 + (z_{i,nat}-z_{i, model})^2$$  If you have about 10 minutes\n Read on.  A very short primer on protein structure data - feel free to skip if you know what protein structures are like! The Protein Data Bank (PDB) is the central repository that contains protein structure data. The data is gathered from scientists from around the world who determine the structure of a protein through experimental methods (typically X-ray crystallography, but the trend is going toward cryo electron microscopy; cryo-EM). One thing to bear in mind is that every structure that we have, even though it\u0026rsquo;s experimental data, is in fact (yet another) a model!\nCondensing lots of physics and theory here, but in the case of X-ray crystallography, it largely has to do with the fact that the method detects and \u0026ldquo;sees\u0026rdquo; electron diffraction. It doesn\u0026rsquo;t see proteins as fancy helices or loops like this:\nThus, to resolve from an electron diffraction to signals that can be used as atomic coordinates, lots of physics and maths are involved. Essentially, the final structure we obtain is a model of the electron density data.\nProtein structures are then represented in an unusual 80-character format (historical reasons) that contain:\n Information about a protein\u0026rsquo;s sequence Information about the amino acids that make up the protein, and most importantly for this exercise, The coordinates of each atom in the protein.  The coordinates are in 3-dimensional $$(x,y,z)$$ space, and represented by an $$N \\times 3$$ matrix:\n[ 16.54 , 10.798, -30.122], [ 16.677, 12.283, -30.154], [ 16.876, 12.799, -28.737], ..., [ -7.62 , -7.475, -5.355], [-11.083, 2.95 , -9.974], [-10.271, 2.053, -12.363] leading to a structure like this:\nRMSD - the maths Given two protein structures with $$n$$ atoms, the RMSD between the two can be calulated by:\n$$RMSD = \\sqrt{\\dfrac{1}{n} \\sum_{i=1}^{n} (d_i) }$$\nwhere\n$$d_i = (x_{i,nat}-x_{i, model})^2 + (y_{i,nat}-y_{i, model})^2 + (z_{i,nat}-z_{i, model})^2$$\nSo what does this mean? We iterate across $$n$$ aligned (this will be explained later) atoms, and take the difference in their x-, y-, and z-coordinates. We then take the square root of the average deviation. This metric can be custom fit to either calculate across all atoms of two protein structures, or a subset - so long as $$n$$ is identical between the predicted and true structures.\nAlign structures before calculating RMSD! Before we calculate the RMSD, we need to align our structures for two reasons:\n PDB structures are typically situated in different starting coordinates of Euclidean space. There can be small differences in length between the true and predicted structures, and/or it may be relevant to measure the RMSD across a comparable region.  However, how do we align two protein structures? Structural alignment can be done with many variations; the green structure is the \u0026ldquo;true\u0026rdquo; answer and the purple one is the prediction.\nIntuitively, we can see that the third alignment is the \u0026ldquo;correct\u0026rdquo; one - it maximises the \u0026lsquo;fit\u0026rsquo; between the green and purple structures. This is the alignment that minimises the RMSD, and this alignment is obtained by singular value decomposition (SVD). In fact, for the purposes of calculating RMSD for model prediction, we often find this optimal alignment first, then measure the RMSD.\nComputationally, the simplest way to align two structures is to use BioPython. When we align structures, it\u0026rsquo;s good practice to be mindful of the following:\n There should be a way to align the sequences of two structures. This does not mean that there has to be an exact amino acid sequence match between the native structure and the predicted structure. However, a mechanism to compare them should be considered. Simply using residue numbers doesn\u0026rsquo;t always work, either! (but this is for another time) We can align one part of the structure to then measure the RMSD of another. This is particularly useful when we want to measure the RMSD of a very specific subsequence when the remaining bits of the protein structure are almost invariant.  Calculating RMSD using BioPython We will assume, for sake of argument, that the alpha alcohol dehydrogenase (PDB: 1hso) is the \u0026ldquo;true\u0026rdquo; structure and the gamma alcohol dehydrogenase (PDB: 1ht0) is the \u0026ldquo;model\u0026rdquo; structure.\n# This makes life so much easier. from Bio.PDB.PDBParser import PDBParser from Bio.SVDSuperimposer import SVDSuperimposer import numpy as np # Let\u0026#39;s get our two structures; call them native (true) and model  p = PDBParser(QUIET=True) native = p.get_structure(\u0026#34;native\u0026#34;, \u0026#34;1hso.pdb\u0026#34;) model = p.get_structure(\u0026#34;model\u0026#34;, \u0026#34;1ht0.pdb\u0026#34;) Protein structures contain the following:\n Chains (these are typically codified with a letter) Residues (these are almost always numbered) Atoms  A protein structure object from the PDB parser allows us to grab each; e.g., to get the set of residues in chain A of the protein,\nchain_a = native[0]['A'] residues_a = [ r for r in chain_a ] (For the eagle-eyed of you, you would have noticed a 0 indexing above. That\u0026rsquo;s because the Structure object from the BioPython PDBParser also adds another layer, called the Model layer (not to be confused with \u0026ldquo;model structure\u0026rdquo; as we have mentioned previously))\nfrom Bio.PDB.Polypeptide import three_to_one from Bio.SVDSuperimposer import SVDSuperimposer from Bio.PDB.Structure import Structure AA = [\u0026#34;ALA\u0026#34;, \u0026#34;CYS\u0026#34;, \u0026#34;ASP\u0026#34;, \u0026#34;GLU\u0026#34;, \u0026#34;PHE\u0026#34;, \u0026#34;GLY\u0026#34;, \u0026#34;HIS\u0026#34;, \u0026#34;ILE\u0026#34;, \u0026#34;LYS\u0026#34;, \u0026#34;LEU\u0026#34;, \u0026#34;MET\u0026#34;, \u0026#34;ASN\u0026#34;, \u0026#34;PRO\u0026#34;, \u0026#34;GLN\u0026#34;, \u0026#34;ARG\u0026#34;, \u0026#34;SER\u0026#34;, \u0026#34;THR\u0026#34;, \u0026#34;VAL\u0026#34;, \u0026#34;TRP\u0026#34;, \u0026#34;TYR\u0026#34;] # Type functions because it\u0026#39;s helpful. def align(native: Structure, model: Structure, atom_types = [\u0026#34;CA\u0026#34;, \u0026#34;N\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;O\u0026#34;]) -\u0026gt; SVDSuperimposer: \u0026#34;\u0026#34;\u0026#34; Aligns a model structure onto a native structure Using the atom types listed in `atom_types`. \u0026#34;\u0026#34;\u0026#34; # A long one-liner that gets the one-letter amino acid representation for each residue in a structure, # then joins those letters into one long string. native_seq = \u0026#34;\u0026#34;.join([ three_to_one(r.resname) for r in native[0].get_residues() if r.resname in AA ]) model_seq = \u0026#34;\u0026#34;.join([ three_to_one(r.resname) for r in model[0].get_residues() if r.resname in AA ]) ## Some assertions that can be used # assert model_seq in native_seq, \u0026#34;There should be an alignable sequence.\u0026#34; assert len(model_seq) == len(native_seq), \u0026#34;The sequences should be of identical length.\u0026#34; # Get the coordinates of the Atom object if the Atom is from an amino acid residue, # and the atom type is what\u0026#39;s specified in atom_types. # Traditionally RMSD is calculated for either: # Only the alpha-carbon atoms (CA), or # The \u0026#34;protein backbone\u0026#34; atoms (CA, N, C, O), or # All atoms native_coords = [ a.coord for a in native[0].get_atoms() if a.parent.resname in AA and a.name in atom_types ] model_coords = [ a.coord for a in model[0].get_atoms() if a.parent.resname in AA and a.name in atom_types ] si = SVDSuperimposer() si.set(np.array(native_coords), np.array(model_coords)) si.run() # Run the SVD alignment return si si = align(native, model) print(\u0026#34;Initial RMSD: {:.2f} angstroms; full-backbone RMSD after alignment: {:.2f} angstroms\u0026#34;.format(si.get_init_rms(), si.get_rms())) Initial RMSD: 35.26 angstroms; full-backbone RMSD after alignment: 0.84 angstroms  We can see above that aligning two protein structures prior to calculating the RMSD can have a huge effect on the RMSD value. For some, this might seem like a bit of a cheat: we aligned the structure to minimise RMSD, so obviously this is bound to happen!\nWhile that\u0026rsquo;s true, we have to account for the fact that protein structures are often not standardised to a specific region of the coordinate space, and so this is a necessary process to avoid over-penalisation. Furthermore, it\u0026rsquo;s always possible to align one region of a protein to then measure the RMSD of another region. This is an approach that was used by the Antibody modelling assessment (AMA) to calculate the RMSD of specific loops of antibodies.\nIn fact, we can do something broadly similar; we can align using half of the atoms, but then calculate the RMSD of the full protein:\ndef rmsd(native_coords, model_coords, rot, tran): model_coords_rotated = np.dot(model_coords, rot) + tran diff = native_coords - model_coords_rotated RMSD = np.sqrt(sum(sum(diff**2))/native_coords.shape[0]) return RMSD def specific_align(native: Structure, model: Structure, aln_atoms = 0.5, atom_types = [\u0026#34;CA\u0026#34;, \u0026#34;N\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;O\u0026#34;]) -\u0026gt; list: # A long one-liner that gets the one-letter amino acid representation for each residue in a structure, # then joins those letters into one long string. native_seq = \u0026#34;\u0026#34;.join([ three_to_one(r.resname) for r in native[0].get_residues() if r.resname in AA ]) model_seq = \u0026#34;\u0026#34;.join([ three_to_one(r.resname) for r in model[0].get_residues() if r.resname in AA ]) assert len(model_seq) == len(native_seq), \u0026#34;The sequences should be of identical length.\u0026#34; # Get the atoms that we want to align native_coords = [ a.coord for a in native[0].get_atoms() if a.parent.resname in AA and a.name in atom_types ] model_coords = [ a.coord for a in model[0].get_atoms() if a.parent.resname in AA and a.name in atom_types ] # Convert to numpy arrays native_coords = np.array(native_coords) model_coords = np.array(model_coords) # Use a specific percentage of atoms to align. percentage_to_aln = int(aln_atoms * len(native_coords)) si = SVDSuperimposer() si.set(native_coords[:percentage_to_aln], model_coords[:percentage_to_aln]) si.run() # The SVD superimposer above gives us the rotation and translation matrices # that we can use to \u0026#34;transform\u0026#34; the model coordinates. The rotation and translation # matrices were based on aligning 50% of the backbone atoms. I will explain this a bit more later. RMSD = rmsd(native_coords, model_coords, si.rot, si.tran) return [si, RMSD] si_specific, rmsd_bb = specific_align(native, model) print(\u0026#34;The initial RMSD is {:.2f}, with a 50% aligned backbone RMSD of {:.2f} and a full backbone RMSD of {:.2f}\u0026#34;.format( si_specific.get_init_rms(), si_specific.get_rms(), rmsd_bb)) The initial RMSD is 30.11, with a 50% aligned backbone RMSD of 0.40 and a full backbone RMSD of 1.47  So there are a couple of observations to be made here:\n The initial and 50%-aligned RMSD values are lower, but these are not comparable to the previous experiment. This is because we were calculating the distance over 50% of the backbone atoms, rather than the complete set. Essentially, the previous experiment is more likely to have a higher RMSD because there can be more anomalous atom coordinates that throw off the RMSD value. The full backbone RMSD is what\u0026rsquo;s comparable to the previous experiment. The difference is an RMSD of 0.84A when we align all backbone atoms vs. 1.47A for when we align half of the backbone atoms, but still calculate the distance across all backbone atoms. We can thus see that the alignment can have a huge impact on the reported RMSD.  The magical element in all of this is the SVD. If you want to stop here, the above code is a basic way to calculate RMSD using BioPython. Otherwise, continue!\nManually Calculating RMSD / What on earth is SVD? Essentially, it decomposes a $$m\\times n$$ matrix $$M$$ into three matrices:\n A $$m \\times n$$ matrix $$\\Sigma$$ whose diagonal contains the square-root of the eigenvalue of $$\\mathbf{MM^{*}}$$ A $$m\\times m$$ matrix U whose columns are the eigenvectors of $$\\mathbf{MM^{*}}$$ A $$n\\times n$$ matrix _V*_ whose columns are the eigenvectors of $$\\mathbf{M^{*}M}$$  I won\u0026rsquo;t go into the details of how SVD is run (outside the scope of this post).\nRemember from a previous post that a vector $$v$$ is an eigenvector if it satisfies $$Mv = \\lambda v$$, i.e., $$v$$ only changes by a scalar factor $$\\lambda$$ when it is transformed by $$M$$. In practice, this means that $$v$$ captures the direction of the transformation applied by $$M$$.\nIn the context of the work here, we use SVD to find the rotation and translation matrices that aligns the model protein structure onto the native one.\nFor ease of notation, the set of $$n \\times 3$$ coordinates from the model structure will be known as $$P$$ while the set of $$n \\times 3$$ coordinates from the native (true) structure will be denoted as $$Q$$.\nThe entire procedure for aligning protein structures using SVD is as follows:\n \u0026ldquo;Centre\u0026rdquo; the structures by the centroids $$C_P$$ and $$C_Q$$: $$P_m = P-C_P$$ and $$Q_m = Q-C_Q$$ Obtain the covariance matrix $$W = Q_m^TP_m$$ Run SVD on $$W$$ to obtain $$U, \\Sigma, V^*$$ Compute the (possible) rotation matrix $$R = (VU^T)$$.   If the determinant of $$R$$ is negative, then flip the sign of $$V_z$$ to keep it in the right-hand coordinate system; otherwise, we\u0026rsquo;re good.\n The translation matrix $$T = C_Q - C_PR$$.  # Function to get rotation and translation matrices def get_rot_tran(coord_true, coord_pred): \u0026#34;\u0026#34;\u0026#34; Given two matrices, return a rotation and translation matrix to move pred coords onto true coords. Largely based on SVDSuperimposer implementation from BioPython with some tweaks. \u0026#34;\u0026#34;\u0026#34; centroid_pred = np.sum(coord_pred, axis=0) / coord_pred.shape[0] centroid_true = np.sum(coord_true, axis=0) / coord_true.shape[0] p_prime = coord_pred - centroid_pred q_prime = coord_true - centroid_true W = np.dot(q_prime.T, p_prime) U, S, Vt = np.linalg.svd(W) V = Vt.T rot = np.dot(V, U.T) det = np.linalg.det(rot) # The determinant is needed to detect whether we need a right-hand coordinate system or not # This basically means we just have to flip the Z-axis if det \u0026lt; 0: Vt[:,2] = -Vt[:,2] V = Vt.T rot = np.dot(V, U.T) tran = centroid_true - np.dot(centroid_pred, rot) return rot, tran def get_specific_atoms(structure, atom_types = [\u0026#34;CA\u0026#34;, \u0026#34;N\u0026#34;, \u0026#34;C\u0026#34;, \u0026#34;O\u0026#34;]): \u0026#34;\u0026#34;\u0026#34; Get atom coordinates of a Structure object for specified atom types \u0026#34;\u0026#34;\u0026#34; return np.array([a.coord for a in structure.get_atoms() if a.parent.resname in AA and a.name in atom_types ]) # Let\u0026#39;s run this and compare with the BioPython superimposer native_backbone = get_specific_atoms(native) model_backbone = get_specific_atoms(model) rot, tran = get_rot_tran(native_backbone, model_backbone) si = SVDSuperimposer() si.set(native_backbone, model_backbone) si.run() Once we have the rotation and translation matrices, the calculation of RMSD is then:\nrmsd(native_backbone, model_backbone, rot, tran) 0.836309515402226  si.get_rms() 0.836309515402226  Voila!\nFurther reading  https://www.evl.uic.edu/ralph/508S98/coordinates.html - what\u0026rsquo;s a RHS? https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/ - SVD guide https://biopython.org/DIST/docs/api/Bio.SVDSuperimposer.SVDSuperimposer-class.html - BioPython reference  ","permalink":"https://ideasbyjin.github.io/post/2019-10-06-rmsd/","summary":"During my PhD and postdoc, my main day-to-day was driven by one question:\n How do we make the best model protein structures?\n To answer that question, this is often done by calculating the root-mean square deviation (RMSD) between the predicted structure vs. the known \u0026lsquo;true\u0026rsquo; protein structure. There are other measures (e.g. TM-score, GDT_TS), but RMSD is still the most intuitive, and (unfortunately?) the accepted standard metric for goodness-of-fit.","title":"RMSD using SVD"},{"content":"This September, my wife and I were in Korea visiting relatives, eating loads of Korean food, and re-connecting with friends. Initially, the idea of going to Korea made me petrified - while I am fluent, I generally avoid speaking in Korean apart from with those closest to me. I felt like I stuttered, and sounded like Jeff Goldblum from Jurassic Park. This trip would, I thought, demand most of my mental capacity to make sure I speak reasonably okay. To my surprise, the trip was so memorable not only because I re-discovered an appreciation for Korean culture, but I gained a better understanding of myself as a Korean in the Western world.\nBtw, where is Korea? To some, (South) Korea as the home of kimchi, the birthplace of BTS, or the designer of your car or cellphone/mobile. For some, Korea is home - for some, like me, Korea is mostly a birthplace, which happens to have my amazing in-laws and distant family. The dark green country is South Korea:\nWhat did I discover? As an Asian immigrant, I\u0026rsquo;ve always grown up with two cultures side-to-side. There is a \u0026ldquo;Korean\u0026rdquo; side, where I feel there is a need to respect age (or experience), share food, and get ready to perform for entertainment (context: some Koreans love karaoke, or noraebang, and this lends to a culture that applauds charismatic performances. Watch a late night Korean talk/game show and you\u0026rsquo;ll get what I mean). I always felt these facets of Korean culture were at odds with my \u0026ldquo;Western\u0026rdquo; upbringing: the idea of a \u0026ldquo;flatter\u0026rdquo; society, respect for the individual, and a tendency to be sarcastic and self-deprecating as a means of \u0026ldquo;entertainment\u0026rdquo;.\nDuring this trip, I realised that while I was uncomfortable with some \u0026ldquo;Korean\u0026rdquo; behaviours, those same behaviours had similar roots to common Western behaviours, too. For example, the Korean language has a \u0026ldquo;respectful tense\u0026rdquo;, known as 존댓말 (roughly pronounced joan-det-mal; the \u0026ldquo;casual\u0026rdquo; tense is 반말, or bahn-mal). Effectively, a vastly different vocabulary is used for communication depending on who you talk to (e.g. someone who is older vs. the same age). Some common examples of differences in 존댓말 (respectful) vs. 반말 (casual) include\u0026hellip;\n   반말 존댓말 Meaning     밥 먹었어? 식사 하셨어요? Have you eaten?   잘 자. 안녕히 주무세요. Good night.   고마워 감사합니다 Thank you.    I used to think that this type of language was over-the-top, especially because there was often an awkward exchange of defining how old/experienced you were in order to determine if 존댓말 was more appropriate for communication, etc. However, I realised that speaking to different people using \u0026ldquo;more respectful\u0026rdquo; words wasn\u0026rsquo;t as unusual as I thought it was.\nTake French or German, which use tu/vous and Du/Sie to \u0026ldquo;toggle\u0026rdquo; between friendly and formal address. Though there isn\u0026rsquo;t such a feature in English, we wouldn\u0026rsquo;t exactly chat in the same way to a friend and a boss, and we can intuitively detect some of the nuanced differences in language. Thus, I saw that there is a common underlying principle for Korean and Western languages: respect can manifest in words, and there are variations in how obviously that respect is conveyed.\nThere are other examples of apparent contrasts in cultures that weren\u0026rsquo;t truly different, but perhaps this is for another time.\nTL;DR In essence, I slowly began to see parallels between Korean and Western cultures - they weren\u0026rsquo;t as different as I thought they were, and the apparent contrasts had similar fundamental roots. I learned to be more comfortable and accept the two halves of my upbringing as a positive, than to see it as a clash of identities. This post is not intended to be a social science essay, but reflecting on what I see as what makes us human.\nData science-y question Being in Korea made me wonder - can we detect \u0026ldquo;respect\u0026rdquo; in language using NLP approaches?\n","permalink":"https://ideasbyjin.github.io/post/2019-10-05-korea/","summary":"This September, my wife and I were in Korea visiting relatives, eating loads of Korean food, and re-connecting with friends. Initially, the idea of going to Korea made me petrified - while I am fluent, I generally avoid speaking in Korean apart from with those closest to me. I felt like I stuttered, and sounded like Jeff Goldblum from Jurassic Park. This trip would, I thought, demand most of my mental capacity to make sure I speak reasonably okay.","title":"Lessons from a very Korean holiday"},{"content":"Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to \u0026ldquo;learn\u0026rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images' pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.\nTypically, machine learning methods are used for classification - that is, given some data features, such as image pixels, can we say if a photo contains a cat or a dog? Machine learning can also perform regression; for example, given Steph Curry\u0026rsquo;s scoring record for the past n games, how many points will he score this coming season? Anyway, for the purposes of this post let\u0026rsquo;s stick to classification.\nEven within classification, machine learning methods can be classified as:\n Supervised - that is, we tell our algorithm the \u0026ldquo;true\u0026rdquo; labels of our images, or Unsupervised - that is, we do not tell our algorithm what the labels are (because sometimes, we don\u0026rsquo;t know!)  The next series of posts will focus on clustering methods, which are unsupervised methods. This post will be on hierarchical clustering.\nProvisional use case I have data with lots of features, what groups do they belong to?\nExecutive summary  30 seconds: Hierarchical clustering is an unsupervised machine learning method. Users must\u0026hellip;  Define a way to quantify the distance between data points (e.g. Euclidean distance), and How those distances are leveraged for grouping data, aka the linkage criterion (e.g. use the average distance or maximum distance between hypothetical clusters)   10 minutes or more: Read down below.  Walkthrough # as always, import all the good stuff import pandas as pd import matplotlib.pyplot as plt import numpy as np from scipy.spatial.distance import pdist # pairwise distances from scipy.cluster.hierarchy import linkage, dendrogram Let\u0026rsquo;s get the data, do any cleaning necessary beforehand. We\u0026rsquo;ll do the McDonald\u0026rsquo;s food menu, courtesy of Kaggle.\ndf = pd.read_csv(\u0026#34;../../data/mcdonalds-menu/menu.csv\u0026#34;) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  What is very nice about this dataset (aside from making people hungry) is that the food categories are pre-defined for us. This gives us a nice validation of our clustering methods later on. There are some things we can see here:\n Some columns are redundant (e.g. Saturated Fat + Saturated Fat % daily value) Calories are effectively redundant (e.g. calories = sum of nutrients; total fat is related to sat. and trans. fat) Some columns are in strings and not numeric (e.g. Serving size) Some columns only have % daily value (e.g. Vitamin A) or don\u0026rsquo;t have any (e.g. Sugars) Serving sizes of drinks are mixed in with serving size of solid food  For simplicity, let\u0026rsquo;s\u0026hellip;\n  Only use the following columns for clustering:\n Serving size with grams only Saturated Fat, Trans Fat, Cholesterol, Sodium, Carbohydrates, Dietary Fiber, Sugars, Protein Vitamin A (% Daily Value), Vitamin C (% Daily Value), Calcium (% Daily Value), Iron (% Daily Value)    For serving size, use a regex to search out for weight in grams.\n  We\u0026rsquo;ll keep Category and Food Item to check what our food items are.\n  # Define some regex stuff import re gramSearch = re.compile(\u0026#39;(\\d+ g)\u0026#39;) integerSearch = re.compile(\u0026#39;\\d+\u0026#39;) def regex(string, regexCompiler): if not string: return None match = regexCompiler.findall(string) # We could print an error message # if we detect more than 1 or 0 patterns. # This is a bit limited if we have a decimal; e.g. 161.1g -\u0026gt; [\u0026#39;161\u0026#39;, \u0026#39;1\u0026#39;] # but let\u0026#39;s stick to this for now. if not match or len(match) \u0026gt; 1: return None else: return match[0] # Not clean, but does the job for a one-time implementation. servingSize = df[\u0026#39;Serving Size\u0026#39;].apply(lambda x: regex(x, gramSearch)).copy() servingSize = servingSize.apply(lambda x: regex(x, integerSearch)) df[\u0026#39;Serving Size\u0026#39;] = servingSize food = df[~pd.isnull(df[\u0026#39;Serving Size\u0026#39;])].copy() food = food[[\u0026#39;Category\u0026#39;, \u0026#39;Item\u0026#39;, \u0026#39;Serving Size\u0026#39;, \u0026#39;Saturated Fat\u0026#39;, \u0026#39;Trans Fat\u0026#39;, \u0026#39;Cholesterol\u0026#39;, \u0026#39;Sodium\u0026#39;, \u0026#39;Carbohydrates\u0026#39;, \u0026#39;Dietary Fiber\u0026#39;, \u0026#39;Sugars\u0026#39;, \u0026#39;Protein\u0026#39;, \u0026#39;Vitamin A (% Daily Value)\u0026#39;, \u0026#39;Vitamin C (% Daily Value)\u0026#39;, \u0026#39;Calcium (% Daily Value)\u0026#39;, \u0026#39;Iron (% Daily Value)\u0026#39;]] food[\u0026#39;Serving Size\u0026#39;] = food[\u0026#39;Serving Size\u0026#39;].astype(float) food.index = list(range(food.shape[0])) # reset the index for convenience food_names = dict(zip(food.index, food[\u0026#39;Item\u0026#39;])) cat_names = dict(zip(food.index, food[\u0026#39;Category\u0026#39;])) How far is an Egg McMuffin from a Sausage McMuffin? In any clustering algorithm, we need to define a distance metric, i.e., how far apart is an Egg McMuffin from a Sausage McMuffin? One of the most common ways to measure the distance between data points is Euclidean distance, also known as the $$l_2$$ norm. For two points a and b from q-dimensional data, the Euclidean distance is\n$$d = \\sqrt{\\sum_{i=1}^{q} (a_i - b_i) }$$\nWe can calculate the Euclidean distance between the food items based on the serving size and nutrition values. I chose Euclidean distance because it is the most common distance metric that\u0026rsquo;s used, but others exist (e.g. cosine distance, Manhattan distance\u0026hellip; etc. Wikipedia is good for this!)\n# Calculate the pairwise distances between items using Euclidean distance food_values = food[food.columns[2:]].values distance_matrix = pdist(food_values, metric = \u0026#39;euclidean\u0026#39;) Just a heads-up that the distance matrix calculated by SciPy is a \u0026ldquo;condensed\u0026rdquo; matrix. The distance between two points in Euclidean space is the same whether you measure it $$a \\rightarrow b$$ or $$b \\rightarrow a$$, so we only calculate this in one direction. Furthermore, the distance of a point to itself is 0.\nOnce we have this matrix, then we can apply clustering! Now, we\u0026rsquo;ll use the scipy implementation because it natively allows us to visualise dendrograms, and it also doesn\u0026rsquo;t require us to define how many clusters we expect.\nHow do we merge points? - Linkage criteria. From the distance matrix, there are several types of \u0026ldquo;linkage\u0026rdquo; criteria we can use. The linkage criteria essentially asks,\n If there are two hypothetical clusters $$c_1$$ and $$c_2$$ of sizes $$n$$ and $$m$$, do we use the minimum, maximum, or average distances between the points in $$c_1$$ and $$c_2$$?\n The cluster with the lowest minimum/maximum/average distance is then merged to the current cluster.\n The Wikipedia page on Complete clustering explains the algorithm well.\n Note that the choice of the distance metric and linkage criteria can affect the results substantially. For this exercise, I will only use Euclidean distance, but will cycle through the different linkage criteria.\nAt first instance, we can apply the average linkage criterion, also known as UPGMA (unweighted pair group method with arithmetic mean).\nUPGMA toy example with distance matrix  Feel free to skip this section if you know how UPGMA works \u0026ldquo;under the hood\u0026rdquo;\n For a pairwise distance matrix like this:\n    a b c d e     a 0 17 21 31 23   b 17 0 30 34 21   c 21 30 0 28 39   d 31 34 28 0 43   e 23 21 39 43 0     Merge points a and b as they have the lowest distance among all possible pairs. Compute the distances between the new cluster, (a,b), with respect to c, d, e.   Thus, we have the following distances: $$d_{(a,b)\\rightarrow c}, d_{(a,b)\\rightarrow d}, d_{(a,b)\\rightarrow e}$$\n  $$d_{(a,b)\\rightarrow c} = \\dfrac{\\left(d_{a\\rightarrow c} + d_{b\\rightarrow c}\\right)}{2}$$,\n  $$d_{(a,b)\\rightarrow d} = \\dfrac{\\left(d_{a\\rightarrow d} + d_{b\\rightarrow d}\\right)}{2}$$,\n  etc.\n   This creates a new distance matrix,      a,b c d e     a,b 0 25.5 32.5 22   c 25.5 0 28 39   d 32.5 28 0 43   e 22 39 43 0    Merge e to (a, b) as it has the lowest average distance (22). Compute the distances between the new cluster (a,b,e) to c and d:  Thus, $$d_{(a,b,e)\\rightarrow c} = \\dfrac{\\left(d_{a,b\\rightarrow c} + d_{a,b\\rightarrow c} + d_{e\\rightarrow c}\\right)}{3}$$, and $$d_{(a,b,e)\\rightarrow d} = \\dfrac{\\left(d_{a,b\\rightarrow d} + d_{a,b\\rightarrow d} + d_{e\\rightarrow d}\\right)}{3}$$   Which then leads to\u0026hellip;      a,b,e c d     a,b,e 0 30 36   c 30 0 28   d 36 28 0    and repeat!\nUPGMA in code # this is just to make the dendrograms a bit prettier from scipy.cluster import hierarchy hierarchy.set_link_color_palette([\u0026#39;#f58426\u0026#39;, \u0026#39;#e8291c\u0026#39;, \u0026#39;#ffc0cb\u0026#39;]) # UPGMA, or average linkage UPGMA = linkage(distance_matrix, \u0026#39;average\u0026#39;) Done! You\u0026rsquo;ve now run a UPGMA on the distance matrix. We can see the results of the clustering in a tree-like visualisation called a dendrogram:\ndend_upgma = dendrogram(UPGMA, p = 7, truncate_mode=\u0026#39;level\u0026#39;) fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \u0026#34;(\u0026#34; in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) For the dendrogram, to avoid having too many labels that are too small to read, I\u0026rsquo;ve only plotted the top 7 levels. Anything in brackets is essentially saying that there are, say, 20 items in that branch.\nThe dendrogram shows how objects are grouped together, and whether there are any singletons. The height of the dendrogram corresponds to the distance between objects.\nWe can already see patterns; for instance, premium salads with chicken group together, while those without chicken go elsewhere. Likewise, the breakfast items are also grouping together. We can run clustering again with a different linkage criteria, such as complete linkage:\nsingle = linkage(distance_matrix, method=\u0026#39;single\u0026#39;) dend = dendrogram(single, p = 10, truncate_mode=\u0026#39;level\u0026#39;) fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \u0026#34;(\u0026#34; in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) Now with Ward clustering:\nward = linkage(distance_matrix, method=\u0026#39;ward\u0026#39;) dend = dendrogram(ward, p = 5, truncate_mode=\u0026#39;level\u0026#39;) fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \u0026#34;(\u0026#34; in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) So while we have some common patterns, it\u0026rsquo;s clear that different linkage criteria affect the clustering results, leading to the different dendrograms. None of these are necessarily better than the other; they are just alternative ways to cluster your data. In fact, the linkage criterion that you end up choosing should largely depend on your data distribution; see here for an example.\nBonus visualisation As an additional visualisation, we can also apply principal component analysis (PCA) on the data, then colour the points according to their cluster membership.\nfrom scipy.spatial.distance import squareform from sklearn.decomposition import PCA from sklearn.cluster import AgglomerativeClustering pca = PCA(n_components = 2) coords = pca.fit_transform(food_values - food_values.mean()) # remember to scale the data # Let\u0026#39;s use the actual categories from McDonald\u0026#39;s as a \u0026#34;true\u0026#34; set of labels categories = food[\u0026#39;Category\u0026#39;] cat_to_int = dict([ (label,i) for i, label in enumerate(set(categories)) ]) # Here, we can choose an arbitrary number of clusters to visualise. # Since we have the categories from McDonald\u0026#39;s, we can use this for # visualisation purposes. num_clust = len(set(categories)) # Let\u0026#39;s use Ward clustering, because, why not. ac = AgglomerativeClustering(n_clusters = num_clust, linkage= \u0026#39;ward\u0026#39;) ac.fit(food[food.columns[2:]].values) # map labels to colour palette - choose something that\u0026#39;s discrete to allow easier colour disambiguation. cm = plt.get_cmap(\u0026#34;Set1\u0026#34;) # Here, we first get a numpy array of evenly-spaced values between 0 and 1 # Then we map each float to an RGB value using the ColorMap above. # The RGBA values are then mapped to integers, as sklearn\u0026#39;s labels are integers. # i.e. this will look like  # {0: (r,g,b,a), 1: (r1, g1, b1, a1)... etc.} mapped = dict([ (i, cm(v)) for i,v in enumerate(np.arange(0, 1.01, 1./num_clust)) ]) # plot the PCA and colour with our predicted cluster, and the categories from the McDonald\u0026#39;s menu as a comparison fig, ax = plt.subplots(1,2) ax[0].scatter(coords[:,0], coords[:,1], color = [ mapped[label] for label in ac.labels_ ]) ax[1].scatter(coords[:,0], coords[:,1], color = [ mapped[cat_to_int[name]] for name in food[\u0026#39;Category\u0026#39;] ]) fig.set_size_inches((10,5)) ax[0].set_title(\u0026#34;Predicted categories\u0026#34;) ax[1].set_title(\u0026#34;True categories\u0026#34;) Interpreting this plot may be difficult, but essentially, it would be ideal to have the same set of points in both the left-hand and right-hand plots to share a colour. In other words, the points that are red in the left-hand plot don\u0026rsquo;t necessarily have to be red in the right-hand plot per se. However, those same points should hopefully share one colour, whether it\u0026rsquo;s blue, pink, or whatever.\nWe see that this isn\u0026rsquo;t the case - so what then? This means that we\u0026rsquo;d have to do a more careful look into what categories the food items belong to, and question more carefully on what the \u0026ldquo;true\u0026rdquo; categories are here. Essentially, the true categories are only designating whether a food item is something you have for breakfast, or it contains fish, etc. However, we\u0026rsquo;ve clustered the data on the basis of their nutrition. In hindsight, what we used for clustering does not necessarily align with the true known information about a food item\u0026rsquo;s category. In other words, nutrition profiles aren\u0026rsquo;t exactly related to an item being a \u0026ldquo;breakfast\u0026rdquo; item.\nEarlier, we saw in the dendrograms that individually similar food items did cluster together (e.g. the different flavours of salads), so we know that there is some information of use here. However, grouping food items into larger categories may not be as intuitive.\nNonetheless, I hope this was a useful session on what clustering can offer you.\n","permalink":"https://ideasbyjin.github.io/post/2019-09-23-clustering-1/","summary":"Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to \u0026ldquo;learn\u0026rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images' pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.","title":"A primer to Clustering - Hierarchical clustering"},{"content":"Data is everywhere. Whether it\u0026rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data? This is where principal component analysis (PCA) becomes incredibly useful.\nIf you\u0026rsquo;ve got\u0026hellip;\n 30 seconds: Using linear algebra, we can project our data onto a lower-dimensional space according to the co-variance of our data features. The first two components resulting from a PCA run explain the largest proportion of variance in the data. 7 minutes or more: Read the code and the walk-through.  # import all the fancy stuff import pandas as pd import seaborn import numpy as np import matplotlib.pyplot as plt import plotly from sklearn.decomposition import PCA # let\u0026#39;s get some nutrition data from starbucks, courtesy of Kaggle df = pd.read_csv(\u0026#34;../../data/starbucks-menu/starbucks-menu-nutrition-drinks.csv\u0026#34;) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Before we do any type of analysis, we should clean up this table; there are cases where some values are simply \u0026ldquo;-\u0026rdquo; (which I will assume is null). For the purpose of this exercise, where calories are \u0026ldquo;-\u0026rdquo;, I will remove these items from the \u0026ldquo;menu\u0026rdquo;. Furthermore, I\u0026rsquo;ll also standardise \u0026ldquo;-\u0026rdquo; into a np.nan value.\n# pandas dataframes have a built-in replace function nullified = df.replace(\u0026#34;-\u0026#34;, np.nan).copy() # Get those with non-null calories clean_df = nullified[~pd.isnull(nullified[\u0026#34;Calories\u0026#34;])].copy() clean_df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  For each item in the drinks menu, we have a series of features, such as the amount of fat, etc. We would expect some of these numbers to correlate with each other to some degree, e.g. fatty foods tend to have more sodium. We can test that expectation visually through the use of a correlation plot.\nTo start, I will only get nutrients, and not calories (i.e. the third column onwards). I have chosen not to get calories because it\u0026rsquo;s effectively a weighted sum of all the other nutrients, so it\u0026rsquo;s sort of redundant.\nFurthermore, because we had had the \u0026ldquo;-\u0026rdquo; character in the dataframe beforehand, I will assert that the sub-frame is numeric. Also, sodium tends to be listed in miligrams, so I will scale this down too.\nrelevant_columns = clean_df.columns[2:] nutrition = clean_df[relevant_columns].apply(pd.to_numeric) nutrition[\u0026#39;Sodium\u0026#39;] /= 1000. # scale-down sodium nutrition.index = range(nutrition.shape[0]) # reset the index for the pandas dataframe Now let\u0026rsquo;s get that plot! As a case study, I will colour the item with the highest fibre content as purple, and the rest as Starbucks green ;)\ncolours = [ \u0026#39;#00704a\u0026#39; if i != nutrition[\u0026#39;Fiber (g)\u0026#39;].idxmax() else \u0026#39;#9b59b6\u0026#39; for i in range(nutrition.shape[0]) ] grid = seaborn.PairGrid(nutrition) grid.map_diag(plt.hist, color = \u0026#39;#00704a\u0026#39;) grid.map_lower(plt.scatter, color = colours) # Hide the upper-triangle since it\u0026#39;s a bit redundant. for i, j in zip(*np.triu_indices_from(grid.axes, 1)): grid.axes[i, j].set_visible(False) There are interesting patterns that we can see here already - for instance, the amount of sodium seems to increase with respect to protein content, while fibre is almost invariant. Another feature we can see is that the food with the highest amount of fibre does not necessarily have the highest fat or carb content, etc. In summary,\n Some nutrients co-vary with one another Each food/drink item has its own unique combination fat/carb/fibre/protein/sodium values.  Given these two observations, it would be convenient to compress the above plot into a single plot with two dimensions, while capturing any and all variation in the nutrient data. This is where PCA gets handy.\nThese are the three core steps to a PCA run:\n Centre our data to have 0-mean per column. Compute the co-variance matrix from the centred data. Alternatively we can calculate the correlation matrix of the raw data. Determine the eigenvectors/eigenvalues of the co-variance or correlation matrix. (NB: an eigenvector v of a matrix A is one that does not change direction when it is transformed by A; it can be explained by a stretch of v by a scalar $$\\lambda$$, i.e., $$Av = \\lambda v$$  In scikit-learn, this is a really easy job:\nX = data - data.mean() pca = PCA(n_components = 2) # number of desired principal components pca.fit(X) # job's done But I will go through a more manual process below:\n# Manual procedure; centre the data by subtracting the mean. X = (nutrition - nutrition.mean()) # Each row is an observation, hence rowvar = False. Eigen-decompose the covariance matrix cov = np.cov(X, rowvar=False) eigenval, eigenvec = np.linalg.eig(cov) # To project the original data onto PC space, take the dot product of the data w.r.t. the eigenvectors projected = np.dot(X, eigenvec) # Just plot the results fig, ax = plt.subplots(1,2) ax[0].bar(range(len(eigenval)), eigenval/sum(eigenval)*100) ax[1].scatter( projected[:,0], projected[:,1], color = colours ) ax[0].set_title(\u0026#34;Explained variance per PC\u0026#34;) ax[1].set_title(\u0026#34;Projected foods onto PC space\u0026#34;) fig.set_size_inches((8,4)) What do the above plots tell us? The majority of the variation in the data can be represented by the first and second principal components; this can be measured by looking at the ratio of the ith eigenvalue with respect to the sum of all eigenvalues (left plot).\nEach eigenvector represents the \u0026ldquo;directions\u0026rdquo; of a matrix A, and the corresponding (reminder: scalar!) eigenvalues represent the magnitude of those directions. In other words, the eigenvectors with the largest eigenvalues represent the greatest sources of variation in A. There can be up to $$k$$ eigenvectors for a $$n \\times k$$ matrix, though in practice, we only use $$p$$ eigenvectors ($$p \u0026lt; k$$) for the purpose of a PCA.\nIf we use plotly, then we can see where drinks are found within the PC space.\nimport plotly.graph_objects as go plot_df = pd.DataFrame(list(zip(clean_df[clean_df.columns[0]], projected[:,0], projected[:,1])), columns = [\u0026#39;Menu item\u0026#39;, \u0026#39;PC1\u0026#39;, \u0026#39;PC2\u0026#39;]) fig = go.Figure() fig.add_trace( go.Scatter( x = plot_df[\u0026#39;PC1\u0026#39;], y = plot_df[\u0026#39;PC2\u0026#39;], text = plot_df[\u0026#39;Menu item\u0026#39;], mode = \u0026#39;markers\u0026#39;, marker_color = colours ) ) fig.show() bonus section Nowadays, PCA is done by singular value decomposition (SVD) over eigen-decomposition of covariance matrices. It\u0026rsquo;s not only more efficient, but it is also the de facto method for PCA methods, such as the scikit-learn implementation. Again, we can breakdown the SVD steps to see how it works (roughly). This thread provides an excellent in-depth coverage.\n# apply singular value decomposition of the zero-centred data, rather than a covariance matrix u, s, vt = np.linalg.svd(X) # the projected points are already represented in u projected = u # the eigen values are acquired by multiplying sigma with itself, divided by n-1 eigenval = np.square(s) / (X.shape[0]-1) fig, ax = plt.subplots(1,2) ax[0].bar(range(len(eigenval)), eigenval/sum(eigenval)*100) ax[1].scatter(projected[:,0], projected[:,1], color = colours) fig.set_size_inches((8,4)) Further resources  An Introduction to Statistical Learning Skymind AI  ","permalink":"https://ideasbyjin.github.io/post/2019-09-17-pca/","summary":"Data is everywhere. Whether it\u0026rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data?","title":"Principal component analysis of Starbucks Nutrition data"},{"content":"Python is an amazing programming language for lots of applications, particularly for bioinformatics. One of the potential downsides to using Python (apart from whitespace, non-static typing) is its speed. It\u0026rsquo;s certainly faster than languages such as R, but it\u0026rsquo;s nowhere near the level of C/C++. In fact, many Python modules are already written in C/C++ (such as NumPy) but it might be practical to have your own C/C++ code to interface with your Python objects. However, getting them to work together is surprisingly difficult. Luckily, the Boost library is a great way to interface code, and can also work with NumPy objects (e.g. NumPy arrays) to give you versatility. Below is a set of example files, along with the compilation command.\nMake sure you have BOOST built on your machine, even if it is a local installation. Here\u0026rsquo;s a sample C++ file calculating the Euclidean norm of a NumPy array, as vectors.cpp:\n#include\u0026lt;cmath\u0026gt;#include\u0026lt;boost/python/module.hpp\u0026gt;#include\u0026lt;boost/python/def.hpp\u0026gt;#include\u0026lt;boost/python/extract.hpp\u0026gt;#include\u0026lt;boost/python/numpy.hpp\u0026gt; using namespace boost::python; namespace np = boost::python::numpy; /* Define a C++ function as you would */ double eucnorm(np::ndarray axis){ const int n = axis.shape(0); double norm = 0.0; for(int i = 0; i \u0026lt; n; i++){ double A = extract\u0026lt;double\u0026gt;(axis[i]); norm += A*A; } return sqrt(norm); } /* Define your module name within BOOST_PYTHON_MODULE */ BOOST_PYTHON_MODULE(vectors){ /* Initialise numpy */ np::initialize(); /* Define your function, named eucnorm */ def(\u0026#34;eucnorm\u0026#34;, eucnorm); }  Compiling this was possibly the toughest part for me, but this is the way to do it:\ng++ vectors.cpp -shared -fpic -I $PYTHONPATH\\ -I $BOOST_ROOT -L $BOOST_LIB_PATH -lboost_numpy -lboost_python -o vectors.so In Python, the module file in vectors.so is called by:\n# vectors.py from vectors import * import numpy as np v = np.array([1,1,1]) N = eucnorm(v) # 1.7320508075688772 And that\u0026rsquo;s a very, very simple tutorial of how to mesh C++ and Python together!\n","permalink":"https://ideasbyjin.github.io/post/2019-01-28-mesh-cpp-python/","summary":"Python is an amazing programming language for lots of applications, particularly for bioinformatics. One of the potential downsides to using Python (apart from whitespace, non-static typing) is its speed. It\u0026rsquo;s certainly faster than languages such as R, but it\u0026rsquo;s nowhere near the level of C/C++. In fact, many Python modules are already written in C/C++ (such as NumPy) but it might be practical to have your own C/C++ code to interface with your Python objects.","title":"Building Python modules using C++"}]