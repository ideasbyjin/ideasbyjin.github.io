<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Principal component analysis of Starbucks Nutrition data | Read between the rows</title>

<meta name="keywords" content="" />
<meta name="description" content="Data is everywhere. Whether it&rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data?">
<meta name="author" content="">
<link rel="canonical" href="https://ideasbyjin.github.io/post/2019-09-17-pca/" />
<link href="/assets/css/stylesheet.min.54720d48c4fa0c4ebe8555f04b9fc5b856112ec49cafef19cb385e89661150b7.css" integrity="sha256-VHINSMT6DE6&#43;hVXwS5/FuFYRLsScr&#43;8ZyzheiWYRULc=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://ideasbyjin.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ideasbyjin.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ideasbyjin.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ideasbyjin.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ideasbyjin.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.80.0" />



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Principal component analysis of Starbucks Nutrition data" />
<meta property="og:description" content="Data is everywhere. Whether it&rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ideasbyjin.github.io/post/2019-09-17-pca/" />
<meta property="article:published_time" content="2019-09-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-09-17T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Principal component analysis of Starbucks Nutrition data"/>
<meta name="twitter:description" content="Data is everywhere. Whether it&rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data?"/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Principal component analysis of Starbucks Nutrition data",
  "name": "Principal component analysis of Starbucks Nutrition data",
  "description": "Data is everywhere. Whether it\u0026amp;rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various …",
  "keywords": [
    
  ],
  "articleBody": "Data is everywhere. Whether it’s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data? This is where principal component analysis (PCA) becomes incredibly useful.\nIf you’ve got…\n 30 seconds: Using linear algebra, we can project our data onto a lower-dimensional space according to the co-variance of our data features. The first two components resulting from a PCA run explain the largest proportion of variance in the data. 7 minutes or more: Read the code and the walk-through.  # import all the fancy stuff import pandas as pd import seaborn import numpy as np import matplotlib.pyplot as plt import plotly from sklearn.decomposition import PCA # let's get some nutrition data from starbucks, courtesy of Kaggle df = pd.read_csv(\"../../data/starbucks-menu/starbucks-menu-nutrition-drinks.csv\") df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Before we do any type of analysis, we should clean up this table; there are cases where some values are simply “-” (which I will assume is null). For the purpose of this exercise, where calories are “-”, I will remove these items from the “menu”. Furthermore, I’ll also standardise “-” into a np.nan value.\n# pandas dataframes have a built-in replace function nullified = df.replace(\"-\", np.nan).copy() # Get those with non-null calories clean_df = nullified[~pd.isnull(nullified[\"Calories\"])].copy() clean_df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  For each item in the drinks menu, we have a series of features, such as the amount of fat, etc. We would expect some of these numbers to correlate with each other to some degree, e.g. fatty foods tend to have more sodium. We can test that expectation visually through the use of a correlation plot.\nTo start, I will only get nutrients, and not calories (i.e. the third column onwards). I have chosen not to get calories because it’s effectively a weighted sum of all the other nutrients, so it’s sort of redundant.\nFurthermore, because we had had the “-” character in the dataframe beforehand, I will assert that the sub-frame is numeric. Also, sodium tends to be listed in miligrams, so I will scale this down too.\nrelevant_columns = clean_df.columns[2:] nutrition = clean_df[relevant_columns].apply(pd.to_numeric) nutrition['Sodium'] /= 1000. # scale-down sodium nutrition.index = range(nutrition.shape[0]) # reset the index for the pandas dataframe Now let’s get that plot! As a case study, I will colour the item with the highest fibre content as purple, and the rest as Starbucks green ;)\ncolours = [ '#00704a' if i != nutrition['Fiber (g)'].idxmax() else '#9b59b6' for i in range(nutrition.shape[0]) ] grid = seaborn.PairGrid(nutrition) grid.map_diag(plt.hist, color = '#00704a') grid.map_lower(plt.scatter, color = colours) # Hide the upper-triangle since it's a bit redundant. for i, j in zip(*np.triu_indices_from(grid.axes, 1)): grid.axes[i, j].set_visible(False) There are interesting patterns that we can see here already - for instance, the amount of sodium seems to increase with respect to protein content, while fibre is almost invariant. Another feature we can see is that the food with the highest amount of fibre does not necessarily have the highest fat or carb content, etc. In summary,\n Some nutrients co-vary with one another Each food/drink item has its own unique combination fat/carb/fibre/protein/sodium values.  Given these two observations, it would be convenient to compress the above plot into a single plot with two dimensions, while capturing any and all variation in the nutrient data. This is where PCA gets handy.\nThese are the three core steps to a PCA run:\n Centre our data to have 0-mean per column. Compute the co-variance matrix from the centred data. Alternatively we can calculate the correlation matrix of the raw data. Determine the eigenvectors/eigenvalues of the co-variance or correlation matrix. (NB: an eigenvector v of a matrix A is one that does not change direction when it is transformed by A; it can be explained by a stretch of v by a scalar $$\\lambda$$, i.e., $$Av = \\lambda v$$  In scikit-learn, this is a really easy job:\nX = data - data.mean() pca = PCA(n_components = 2) # number of desired principal components pca.fit(X) # job's done But I will go through a more manual process below:\n# Manual procedure; centre the data by subtracting the mean. X = (nutrition - nutrition.mean()) # Each row is an observation, hence rowvar = False. Eigen-decompose the covariance matrix cov = np.cov(X, rowvar=False) eigenval, eigenvec = np.linalg.eig(cov) # To project the original data onto PC space, take the dot product of the data w.r.t. the eigenvectors projected = np.dot(X, eigenvec) # Just plot the results fig, ax = plt.subplots(1,2) ax[0].bar(range(len(eigenval)), eigenval/sum(eigenval)*100) ax[1].scatter( projected[:,0], projected[:,1], color = colours ) ax[0].set_title(\"Explained variance per PC\") ax[1].set_title(\"Projected foods onto PC space\") fig.set_size_inches((8,4)) What do the above plots tell us? The majority of the variation in the data can be represented by the first and second principal components; this can be measured by looking at the ratio of the ith eigenvalue with respect to the sum of all eigenvalues (left plot).\nEach eigenvector represents the “directions” of a matrix A, and the corresponding (reminder: scalar!) eigenvalues represent the magnitude of those directions. In other words, the eigenvectors with the largest eigenvalues represent the greatest sources of variation in A. There can be up to $$k$$ eigenvectors for a $$n \\times k$$ matrix, though in practice, we only use $$p$$ eigenvectors ($$p If we use plotly, then we can see where drinks are found within the PC space.\nimport plotly.graph_objects as go plot_df = pd.DataFrame(list(zip(clean_df[clean_df.columns[0]], projected[:,0], projected[:,1])), columns = ['Menu item', 'PC1', 'PC2']) fig = go.Figure() fig.add_trace( go.Scatter( x = plot_df['PC1'], y = plot_df['PC2'], text = plot_df['Menu item'], mode = 'markers', marker_color = colours ) ) fig.show() bonus section Nowadays, PCA is done by singular value decomposition (SVD) over eigen-decomposition of covariance matrices. It’s not only more efficient, but it is also the de facto method for PCA methods, such as the scikit-learn implementation. Again, we can breakdown the SVD steps to see how it works (roughly). This thread provides an excellent in-depth coverage.\n# apply singular value decomposition of the zero-centred data, rather than a covariance matrix u, s, vt = np.linalg.svd(X) # the projected points are already represented in u projected = u # the eigen values are acquired by multiplying sigma with itself, divided by n-1 eigenval = np.square(s) / (X.shape[0]-1) fig, ax = plt.subplots(1,2) ax[0].bar(range(len(eigenval)), eigenval/sum(eigenval)*100) ax[1].scatter(projected[:,0], projected[:,1], color = colours) fig.set_size_inches((8,4)) Further resources  An Introduction to Statistical Learning Skymind AI  ",
  "wordCount" : "1145",
  "inLanguage": "en",
  "datePublished": "2019-09-17T00:00:00Z",
  "dateModified": "2019-09-17T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ideasbyjin.github.io/post/2019-09-17-pca/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Read between the rows",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ideasbyjin.github.io/favicon.ico"
    }
  }
}
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ideasbyjin.github.io/" accesskey="h" title="Read between the rows (Alt + H)">Read between the rows</a>
            <span class="logo-switches">
                <a id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </a>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://ideasbyjin.github.io/search" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/archives" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/about" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/" title="home">
                    <span>home</span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      Principal component analysis of Starbucks Nutrition data
    </h1>
    <div class="post-meta">September 17, 2019&nbsp;·&nbsp;6 min

</div>
  </header> 

  <div class="toc">
    <details >
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#bonus-section" aria-label="bonus section">bonus section</a></li><li>
        <a href="#further-resources" aria-label="Further resources">Further resources</a></li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<p>Data is <em>everywhere</em>. Whether it&rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles
of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand
samples with only a few features, or only a small number of examples with tons of features. For either case, and
anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful;
however, <em>how</em> do we choose which features to use for capturing the essence of our data? This is where principal
component analysis (PCA) becomes incredibly useful.</p>
<p>If you&rsquo;ve got&hellip;</p>
<ul>
<li><strong>30 seconds</strong>: Using linear algebra, we can project our data onto a lower-dimensional space according to the co-variance of our data features. The first two components resulting from a PCA run explain the largest proportion of <em>variance</em> in the data.</li>
<li><strong>7 minutes or more</strong>: Read the code and the walk-through.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import all the fancy stuff</span>
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> seaborn
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> plotly
<span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># let&#39;s get some nutrition data from starbucks, courtesy of Kaggle</span>
df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;../../data/starbucks-menu/starbucks-menu-nutrition-drinks.csv&#34;</span>)
df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Before we do any type of analysis, we should clean up this table; there are cases where some values are simply &ldquo;-&rdquo; (which I will assume is <code>null</code>). For the purpose of this exercise, where calories are &ldquo;-&rdquo;, I will remove these items from the &ldquo;menu&rdquo;. Furthermore, I&rsquo;ll also standardise &ldquo;-&rdquo; into a <code>np.nan</code> value.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># pandas dataframes have a built-in replace function</span>
nullified <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;-&#34;</span>, np<span style="color:#f92672">.</span>nan)<span style="color:#f92672">.</span>copy()

<span style="color:#75715e"># Get those with non-null calories</span>
clean_df <span style="color:#f92672">=</span> nullified[<span style="color:#f92672">~</span>pd<span style="color:#f92672">.</span>isnull(nullified[<span style="color:#e6db74">&#34;Calories&#34;</span>])]<span style="color:#f92672">.</span>copy()
clean_df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>For each item in the drinks menu, we have a series of features, such as the amount of fat, etc. We would expect some of these numbers to correlate with each other to some degree, e.g. fatty foods tend to have more sodium. We can test that expectation visually through the use of a correlation plot.</p>
<p>To start, I will only get nutrients, and not calories (i.e. the third column onwards). I have chosen not to get calories because it&rsquo;s effectively a weighted sum of all the other nutrients, so it&rsquo;s sort of redundant.</p>
<p>Furthermore, because we had had the &ldquo;-&rdquo; character in the dataframe beforehand, I will assert that the sub-frame is numeric. Also, sodium tends to be listed in miligrams, so I will scale this down too.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">relevant_columns <span style="color:#f92672">=</span> clean_df<span style="color:#f92672">.</span>columns[<span style="color:#ae81ff">2</span>:]
nutrition <span style="color:#f92672">=</span> clean_df[relevant_columns]<span style="color:#f92672">.</span>apply(pd<span style="color:#f92672">.</span>to_numeric)
nutrition[<span style="color:#e6db74">&#39;Sodium&#39;</span>] <span style="color:#f92672">/=</span> <span style="color:#ae81ff">1000.</span> <span style="color:#75715e"># scale-down sodium</span>
nutrition<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> range(nutrition<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) <span style="color:#75715e"># reset the index for the pandas dataframe</span>
</code></pre></div><p>Now let&rsquo;s get that plot! As a case study, I will colour the item with the highest fibre content as purple, and the rest as Starbucks green ;)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">colours <span style="color:#f92672">=</span> [ <span style="color:#e6db74">&#39;#00704a&#39;</span> <span style="color:#66d9ef">if</span> i <span style="color:#f92672">!=</span> nutrition[<span style="color:#e6db74">&#39;Fiber (g)&#39;</span>]<span style="color:#f92672">.</span>idxmax() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;#9b59b6&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(nutrition<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) ]

grid <span style="color:#f92672">=</span> seaborn<span style="color:#f92672">.</span>PairGrid(nutrition)
grid<span style="color:#f92672">.</span>map_diag(plt<span style="color:#f92672">.</span>hist, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#00704a&#39;</span>)
grid<span style="color:#f92672">.</span>map_lower(plt<span style="color:#f92672">.</span>scatter, color <span style="color:#f92672">=</span> colours)

<span style="color:#75715e"># Hide the upper-triangle since it&#39;s a bit redundant.</span>
<span style="color:#66d9ef">for</span> i, j <span style="color:#f92672">in</span> zip(<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>triu_indices_from(grid<span style="color:#f92672">.</span>axes, <span style="color:#ae81ff">1</span>)):
    grid<span style="color:#f92672">.</span>axes[i, j]<span style="color:#f92672">.</span>set_visible(False)
</code></pre></div><p><img src="/assets/notebooks/pca/output_8_0.png" alt="png"></p>
<p>There are interesting patterns that we can see here already - for instance, the amount of sodium seems to increase with respect to protein content, while fibre is almost invariant. Another feature we can see is that the food with the highest amount of fibre does not necessarily have the highest fat or carb content, etc. In summary,</p>
<ul>
<li>Some nutrients co-vary with one another</li>
<li>Each food/drink item has its own unique combination fat/carb/fibre/protein/sodium values.</li>
</ul>
<p>Given these two observations, it would be convenient to compress the above plot into a <em>single</em> plot with two dimensions, while capturing any and all variation in the nutrient data. This is where PCA gets handy.</p>
<p>These are the three core steps to a PCA run:</p>
<ul>
<li>Centre our data to have 0-mean per column.</li>
<li>Compute the co-variance matrix from the centred data. Alternatively we can calculate the correlation matrix of the raw data.</li>
<li>Determine the eigenvectors/eigenvalues of the co-variance or correlation matrix. (NB: an eigenvector <em>v</em> of a matrix <em>A</em> is one that does <strong>not</strong> change direction when it is transformed by <em>A</em>; it can be explained by a stretch of <em>v</em> by a scalar $$\lambda$$, i.e., $$Av = \lambda v$$</li>
</ul>
<p>In <code>scikit-learn</code>, this is a really easy job:</p>
<pre><code>X = data - data.mean()
pca = PCA(n_components = 2) # number of desired principal components
pca.fit(X) # job's done
</code></pre><p>But I will go through a more manual process below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Manual procedure; centre the data by subtracting the mean.</span>
X <span style="color:#f92672">=</span> (nutrition <span style="color:#f92672">-</span> nutrition<span style="color:#f92672">.</span>mean())

<span style="color:#75715e"># Each row is an observation, hence rowvar = False. Eigen-decompose the covariance matrix</span>
cov <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cov(X, rowvar<span style="color:#f92672">=</span>False) 
eigenval, eigenvec <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>eig(cov)

<span style="color:#75715e"># To project the original data onto PC space, take the dot product of the data w.r.t. the eigenvectors</span>
projected <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, eigenvec)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Just plot the results</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bar(range(len(eigenval)), eigenval<span style="color:#f92672">/</span>sum(eigenval)<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(
    projected[:,<span style="color:#ae81ff">0</span>], projected[:,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> colours
)

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Explained variance per PC&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Projected foods onto PC space&#34;</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><p><img src="/assets/notebooks/pca/output_11_0.png" alt="png"></p>
<p>What do the above plots tell us? The majority of the variation in the data can be represented by the first and second principal components; this can be measured by looking at the ratio of the ith eigenvalue with respect to the sum of all eigenvalues (left plot).</p>
<p>Each eigenvector represents the &ldquo;directions&rdquo; of a matrix <em>A</em>, and the corresponding (reminder: scalar!) eigenvalues represent the magnitude of those directions. In other words, the eigenvectors with the largest eigenvalues represent the greatest sources of variation in <em>A</em>. There can be up to $$k$$ eigenvectors for a $$n \times k$$ matrix, though in practice, we only use $$p$$ eigenvectors ($$p &lt; k$$) for the purpose of a PCA.</p>
<p>If we use plotly, then we can see where drinks are found within the PC space.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> plotly.graph_objects <span style="color:#f92672">as</span> go

plot_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(list(zip(clean_df[clean_df<span style="color:#f92672">.</span>columns[<span style="color:#ae81ff">0</span>]], projected[:,<span style="color:#ae81ff">0</span>], projected[:,<span style="color:#ae81ff">1</span>])), columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Menu item&#39;</span>, <span style="color:#e6db74">&#39;PC1&#39;</span>, <span style="color:#e6db74">&#39;PC2&#39;</span>])

fig <span style="color:#f92672">=</span> go<span style="color:#f92672">.</span>Figure()
fig<span style="color:#f92672">.</span>add_trace(
    go<span style="color:#f92672">.</span>Scatter(
        x <span style="color:#f92672">=</span> plot_df[<span style="color:#e6db74">&#39;PC1&#39;</span>], y <span style="color:#f92672">=</span> plot_df[<span style="color:#e6db74">&#39;PC2&#39;</span>],
        text <span style="color:#f92672">=</span> plot_df[<span style="color:#e6db74">&#39;Menu item&#39;</span>],
        mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;markers&#39;</span>,
        marker_color <span style="color:#f92672">=</span> colours
    )
)
fig<span style="color:#f92672">.</span>show()
</code></pre></div><h2 id="bonus-section">bonus section<a hidden class="anchor" aria-hidden="true" href="#bonus-section">#</a></h2>
<p>Nowadays, PCA is done by singular value decomposition (SVD) over eigen-decomposition of covariance matrices. It&rsquo;s not only more efficient, but it is also the <em>de facto</em> method for PCA methods, such as the <code>scikit-learn</code> implementation. Again, we can breakdown the SVD steps to see how it works (roughly). This <a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">thread</a>
provides an excellent in-depth coverage.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># apply singular value decomposition of the zero-centred data, rather than a covariance matrix</span>
u, s, vt <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(X)

<span style="color:#75715e"># the projected points are already represented in u</span>
projected <span style="color:#f92672">=</span> u

<span style="color:#75715e"># the eigen values are acquired by multiplying sigma with itself, divided by n-1</span>
eigenval <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>square(s) <span style="color:#f92672">/</span> (X<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bar(range(len(eigenval)), eigenval<span style="color:#f92672">/</span>sum(eigenval)<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(projected[:,<span style="color:#ae81ff">0</span>], projected[:,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> colours)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><p><img src="/assets/notebooks/pca/output_15_0.png" alt="png"></p>
<h2 id="further-resources">Further resources<a hidden class="anchor" aria-hidden="true" href="#further-resources">#</a></h2>
<ul>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning</a></li>
<li><a href="https://skymind.ai/wiki/eigenvector">Skymind AI</a></li>
</ul>

</div>
  <footer class="post-footer">
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://ideasbyjin.github.io/">Read between the rows</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
