<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>An introduction to Artificial Neural Networks | Read between the rows</title>

<meta name="keywords" content="" />
<meta name="description" content="This is my intro to neural networks.">
<meta name="author" content="">
<link rel="canonical" href="/post/2020-02-26-anns/" />
<link href="/assets/css/stylesheet.min.ba79064eecb9dc2a7050b36f485dfa34d91c3405027f95064142d0c857cd81ba.css" integrity="sha256-unkGTuy53CpwULNvSF36NNkcNAUCf5UGQULQyFfNgbo=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" href="apple-touch-icon.png">
<link rel="mask-icon" href="safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.80.0" />



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="An introduction to Artificial Neural Networks" />
<meta property="og:description" content="This is my intro to neural networks." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2020-02-26-anns/" />
<meta property="article:published_time" content="2020-02-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-02-26T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An introduction to Artificial Neural Networks"/>
<meta name="twitter:description" content="This is my intro to neural networks."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "An introduction to Artificial Neural Networks",
  "name": "An introduction to Artificial Neural Networks",
  "description": "This is my intro to neural networks.",
  "keywords": [
    
  ],
  "articleBody": "For most AI aficionados, deep learning may seem like the innovation of yesterday. However, deep learning methods are still revolutionising the way we think about many fields today. I’ve seen that happen with protein structure prediction.\nPerhaps the most iconic deep learning methods that you may have heard of will include:\n The convolutional neural network (CNN) and The recurrent neural network (RNN).  CNNs have changed the way we think about image classification, while RNNs have been hugely influential in sequential problems (e.g. text recognition).\nBut, before we talk about CNNs and RNNs, we should take a step back and ask what is a neural network. For some, this may still seem like a black box, especially if you’ve never had any formal training in the area. In this post, I’ll try to explain what neural networks are, and hopefully do a bit of jargon busting along the way.\nIf you have…\n 30 seconds: neural networks are a family of algorithms in artificial intelligence. Inspired by biological neurons, they can perform extremely well. However, poor design choices can lead to some unusual behaviours, and for some tasks, they may not even be the method of choice. 15 minutes: go on. If you’re happy with some basic terminology, feel free to just jump to the code section.  Preamble + Jargon Buster The artificial neural network was inspired in part by the biological neuron:\nThe idea here is, each neuron takes an input (i.e. incoming neurotransmitters). From the combination of inputs, it produces an output signal (i.e. an action potential). When a series of neurons work sequentially and in tandem, you get an entire network.\nFrom the view of artificial intelligence, a “neuron” is really a unit that computes some function given some input. We can see some parallels when we use simplified circles and arrows, and how they can line up as a “network”:\nWhat makes neural networks special, in my view, are three things:\n Neuron internals: Each neuron can transform the input data. Once transformed, the data lies in a so-called hidden or “latent” space. In the original space, the data may not have had any obvious patterns, but in the latent representation, the data may show some neat patterns. Backpropagation: Using some neat calculus tricks, neural networks can be fine-tuned very well Flexible architectures: There are loads of ways to build networks to tailor for your type of data.  I’ve explained points 2 and 3 in more detail in the “Appendix” at the bottom. We’ll explain point 1 as it’s crucial.\nNeuron internals To understand what we mean by\n neural networks can transform the input data\n We have to first review the humble linear regression - the “line of best fit”.\n$$ y = mx + b$$\nThe idea is that the predictor variable, $$y$$, is equivalent to a sum of the variable $$x$$ that’s multiplied by some value $$m$$, along with an adjusting constant, $$b$$.\nYou can expand the idea to have multiple types of $$x$$s and their associated multipliers. To make life easier, and to keep consistency with math textbook notation, let’s rewrite\n$$ y = m_1x_1 + m_2x_2 + … + b $$\nas\n$$ y = b_1x_1 + b_2x_2 + … + b_0 $$\nEach of these $$b_1, b_2, … $$ are multipliers; or in neural network speak, the weights. They are also known as a model’s “parameters”. By changing the $$b$$ values, the value of $$y$$ will change.\nThe final term, $$b_0$$, is the intercept of the linear model, or bias in neural network speak.\nWhat neural networks do is to use the output of the linear regression, then apply another mathematical function.\nThese functions are called activation functions. For example, the sigmoid function\n$$ S(y) = \\dfrac{1}{1+e^{-y}} $$\nis a non-linear function that transforms the output of the linear regression data into an S-shape.\nimport numpy as np import matplotlib.pyplot as plt # Check out my matplotlib stylesheet! plt.style.use(\"bbc\") fig, ax = plt.subplots(1,2) # Let's just get 20 values from -10 to +10. (we do +11 here otherwise it stops at 9.) y = np.arange(-10, 11) def sigmoid(v): return 1./(1+np.exp(-1.*v)) ax[0].plot(y) ax[1].plot(sigmoid(y)) ax[0].set_title(\"Value of y\") ax[1].set_title(\"Value of sigmoid(y)\") fig.set_size_inches((8,4)) Thus, a neuron with a sigmoid activation function squishes the output of a linear regression into an S-curve within 0 and 1. If we repeat a similar type of logic across all neurons, the inputs will get transformed according to what we choose as the activation function.\nOnce the data has been transformed by the activation function, this transformed data can then be used as the input to a new group of neurons downstream! The downstream layer of neurons may or may not use the same activation function on the transformed data from the previous layer.\nThere’s quite a few activation functions, and I’ve written a couple here for reference:\n   Name Equation Range What does it look like     Linear $$y$$ $$-\\infty, +\\infty$$ Just a straight diagonal line   Sigmoid $$ S(y) = \\dfrac{1}{1+e^{-y}} $$ 0,1 Shaped like an S curve   TanH $$ tanh(y) $$ -1,1 Shaped like an S curve   Rectified Linear Unit (ReLu) $$ Relu(y) = max(0,y) $$ 0, $$\\infty$$ Flat until y = 0, then diagonal   Softmax $$ Softmax(y) = \\dfrac{e^y}{\\sum_{i=1}^{K} e^{y}} $$ 0,1 Shaped like an S curve    def tanh(v): return np.tanh(v) def relu(v): return list(map(lambda _: max(0, _), v)) fig, axes = plt.subplots(2,2) ax = axes.flatten() ax[0].plot(y) ax[1].plot(sigmoid(y)) ax[2].plot(tanh(y)) ax[3].plot(relu(y)) ax[0].set_title(\"Value of y\") ax[1].set_title(\"Value of sigmoid(y)\") ax[2].set_title(\"Value of tanh(y)\") ax[3].set_title(\"Value of Relu(y)\") fig.set_size_inches((8,8)) Now when we talk about neural networks, there is some jargon that’s bound to float around. I’ll leave this here for reference.\nJargon Summary  Neuron : we refer to a neuron as one unit that computes some calculation. Loss : a metric to describe how far we are off from the true value Bias : the intercept Weights : a series of multipliers for each variable Activation function : a function that is applied to transform the output of a neuron  The task at hand To start, let’s get some RNAseq data from GTEx. This diagram from Nature just shows the huge diversity of the gene expression data sources. Building on a previous post where I clustered gene expression levels using DBSCAN, let’s try to use a supervised approach and predict the tissues instead.\nWhat are the use cases?  If there is a loss of documentation, the gene expression pattern may tell us what tissue a sample is likely to be derived from. Determine if related tissues show similar expression profiles; vice-versa, do some genes behave in the same way across some tissues? Establish the boundaries of a “normal” or “healthy” tissue based on gene expression, allowing us to detect anomalies  Data Prep As I covered in my previous post, the GTEx gene expression data is pretty big. In fact, getting it all at once is not ideal. We’re going to use some tricks:\n “Stream” the data – have a pointer to the data, but don’t bring it in all at once. Use the most variably expressed genes - if a gene is expressed everywhere with low rates, or is only found in, say, one sample, it’s not that informative. Use protein-coding genes.  # Importing useful things import pandas as pd import matplotlib.pyplot as plt from urllib.request import urlopen from gzip import GzipFile import io import multiprocessing as mp import numpy as np # Some utility functions - feel free to skip DECODE_CODEC = 'utf-8' # This is the length of an Ensembl gene identifier which we'll use later. ENSEMBL_LENGTH = 15 def stream_request(url): \"\"\" Open a connection to some url, then stream data from it This has the advantage of: A. We don't have to wait for the entire file to download to do operations B. We can perform some operations on-the-fly \"\"\" fh = urlopen(url) buffer = io.StringIO() for line in fh: decoded = line.decode(DECODE_CODEC) buffer.write(decoded) fh.close() # Reset the StringIO buffer to byte position 0 buffer.seek(0) return buffer def stream_request_to_pandas(url: str, sep: str = '\\t') - pd.DataFrame: streamed_buffer = stream_request(url) return pd.read_csv(streamed_buffer, sep = sep) # Stream in and read protein-coding genes from HGNC geneUrl = \"https://www.genenames.org/cgi-bin/download/custom?col=gd_app_sym\u0026col=gd_pub_ensembl_id\u0026status=Approved\u0026hgnc_dbtag=on\u0026order_by=gd_app_sym_sort\u0026format=text\u0026where=(gd_pub_chrom_map%20not%20like%20%27%25patch%25%27%20and%20gd_pub_chrom_map%20not%20like%20%27%25alternate%20reference%20locus%25%27)%0Aand%20gd_locus_type%20=%20%27gene%20with%20protein%20product%27\u0026submit=submit\" # This is a short way to convert that URL into a table gene_df = stream_request_to_pandas(geneUrl) # Let's see what it looks like gene_df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  # Keep a list of protein identifiers here proteins = gene_df['Ensembl gene ID'].values Now let’s get that big GTEx file\n# Open a handle onto the GTEx expression data URL = \"https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_tpm.gct.gz\" urlHandle = urlopen(URL) fh = GzipFile(fileobj=urlHandle) # ignore the first two lines as they contain shape of the file _ = fh.readline() _ = fh.readline() From that big GTEx file, let’s get the column names; this is in the third line of the file.\nheader = fh.readline().decode(DECODE_CODEC).strip().split('\\t') Now, each line looks something like this:\nfirst_row = np.array(fh.readline().decode(DECODE_CODEC).strip().split('\\t')) columns = np.append( np.array([0,1]), np.random.randint(2, len(header), 20) ) pd.DataFrame([first_row[columns]], columns=np.array(header)[columns]) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }   ... GTEX-13OW7-0011-R4b-SM-5O9CX GTEX-VJYA-1826-SM-4KL1W   Okay, cool, so we now know that the third column onward just has numbers. The first column contains gene IDs.\nKnowing this file format, let’s get the most variable protein-coding genes. We’ll also impose that at least half the samples have non-zero expression data. To assess variability, we’re going to use a metric called mean absolute deviation, then store it. Another viable approach is calculating something like the entropy value.\nBefore we do so, we’re going to log transform the data. GTEx data has lots of 0s, and TPM units can be out of scale.\ndef get_gene_mad(values): \"\"\" The mean absolute deviation is equal to avg( abs(x - avg) ) \"\"\" mu_x = np.mean(values) mad = np.mean(np.abs(values - mu_x)) return mad def wrapper(index, line): \"\"\" This is a function we'll call with a parallel pool connection. We ignore any line that contains non protein-coding genes \"\"\" if index % 5000 == 0: print(\"Processing line {}\".format(index)) tokenised_line = line.decode(DECODE_CODEC).strip().split('\\t') gene_id = tokenised_line[0][:ENSEMBL_LENGTH] # If it's not a protein-coding gene, let's ignore if gene_id not in proteins: return None numeric_values = np.array(tokenised_line[2:], dtype=float) array_cutoff = len(numeric_values) / 2. # If more than half the data is zero, then let's ignore if sum(numeric_values == 0) = array_cutoff: return None numeric_values = np.log(numeric_values+1) mad = get_gene_mad(numeric_values) return gene_id, mad, tokenised_line To see what log transformation can do, see this plot from my previous post.\nkeep_data = [] gene_mad = {} # Close the previous connection fh.close() # Re-establish the connection, and skip the first three lines urlHandle = urlopen(URL) fh = GzipFile(fileobj=urlHandle) print(\"Re-established connection, skipping 3 lines...\") [fh.readline() for i in range(3)] # Let's parallelise this to make it faster pool = mp.Pool(4) # Let's get some results where we have just protein coding genes. jobs = [pool.apply_async(wrapper, args=(i,line,)) for i,line in enumerate(fh) ] results = [j.get() for j in jobs] filtered_results = [ r for r in results if r is not None ] del results Re-established connection, skipping 3 lines... Processing line 0 ... Processing line 55000  Brilliant, now that we have our gene MAD values we can just select the top, say, 1000 genes.\nTOP = 1000 sorted_genes = sorted(filtered_results, key = lambda x: x[1], reverse=True)[:TOP] # Let's create a dataframe df = pd.DataFrame( [ np.log(np.array(_[2][2:],float)+1) for _ in sorted_genes ], columns = header[2:], index = [ _[2][0] for _ in sorted_genes ] ) df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Looking out for Batch Effects While we have our top 1000 most variably expressed genes, there is a caveat! Gene expression data can be prone to batch effects (i.e. technical variation). To check for this behaviour, let’s get some sample annotations.\nsample_url = \"https://storage.googleapis.com/gtex_analysis_v8/annotations/GTEx_Analysis_v8_Annotations_SampleAttributesDS.txt\" sample_metadata = stream_request_to_pandas(sample_url) subset = sample_metadata[sample_metadata['SAMPID'].isin(df.columns)].copy() tissue_column = 'SMTSD' batch_column = 'SMGEBTCH' subset.iloc[:3][['SAMPID', tissue_column, batch_column]] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  We can see that each sample has an associated batch ID. We can colour our plot by the batch ID and the tissue:\nfrom sklearn.decomposition import PCA sample_id_to_batch = subset[['SAMPID', batch_column]] sample_id_to_tissue = subset[['SAMPID', tissue_column]] pca = PCA() # We take the transpose of our gene expression because otherwise PCA would find the samples that are most varied # Rather than the genes that are most varied. coords = pca.fit_transform(df.T.values) fig, ax = plt.subplots(1,2) special_tissue = 'Muscle - Skeletal' tissue_dots = np.argwhere(sample_id_to_tissue[tissue_column]==special_tissue).flatten() t1_name = \"LCSET-4822\" t2_name = \"LCSET-4824\" t3_name = \"LCSET-4416\" t4_name = \"LCSET-10515\" t1 = np.argwhere(sample_id_to_batch[batch_column]==t1_name).flatten() t2 = np.argwhere(sample_id_to_batch[batch_column]==t2_name).flatten() t3 = np.argwhere(sample_id_to_batch[batch_column]==t3_name).flatten() t4 = np.argwhere(sample_id_to_batch[batch_column]==t4_name).flatten() t1_inter = np.intersect1d(t1, tissue_dots) t2_inter = np.intersect1d(t2, tissue_dots) t3_inter = np.intersect1d(t3, tissue_dots) t4_inter = np.intersect1d(t4, tissue_dots) ax[0].scatter(coords[:,0], coords[:,1], color = '#c3c3c3', alpha = 0.3) ax[0].scatter(coords[tissue_dots,0], coords[tissue_dots,1], color = '#348abd', label = special_tissue) ax[1].scatter(coords[:,0], coords[:,1], color = '#c3c3c3', alpha = 0.3) ax[1].scatter(coords[t1_inter,0], coords[t1_inter,1], color = 'green', label = \"Batch {}\".format(t1_name)) ax[1].scatter(coords[t2_inter,0], coords[t2_inter,1], color = '#a60628', label = \"Batch {}\".format(t2_name)) ax[1].scatter(coords[t3_inter,0], coords[t3_inter,1], color = '#ffd700', label = \"Batch {}\".format(t3_name)) ax[1].scatter(coords[t4_inter,0], coords[t4_inter,1], color = '#f58426', label = \"Batch {}\".format(t4_name)) ax[0].legend(loc='upper right') ax[1].legend(loc='upper right') fig.set_size_inches((10,5)) There does seem to be some level of batch effect, which we can adjust using tools like ComBat, but we won’t do that here, and use the data as-is for simplicity.\nCoding up the NN To code up our neural network, we are spoiled for choice - there’s loads of solutions (TensorFlow, PyTorch), but the simplest framework in my mind is Keras. One of the advantages of Keras is that it is very easy to read, and covers a a good range of use cases (e.g. convolutional layers). To train a neural network to predict the tissue that a gene belongs to, we need to know which tissue the sample is from (i.e. the truth).\nsubset.iloc[:3][['SAMPID', tissue_column]] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Since our data is arranged in the format of genes $$\\times$$ samples , we are going to transpose the matrix. This is because we want the neural network to take, the expression profile per sample as opposed to per gene. This way, it’ll predict what tissue we should assign for a particular sample.\n# Get a dictionary of sample id to tissue site sample_to_tissue = dict(subset[['SAMPID', tissue_column]].values) # Get the tranposed matrix and subset a few columns for preview mat = df.T mat['TissueSite'] = [ sample_to_tissue[ix] for ix in mat.index ] mat[ list(mat.columns[:3]) + ['TissueSite'] ] .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  Applying the Neural Network Now that we have a filtered dataset to work with (finally), we can now train a neural network. Each neural network can be described in terms of its “layers”. Each layer essentially represents a set of neurons. There are three types of layers that we’ll have in our network today:\n The input layer - a set of neurons that just emit the gene expression data The hidden layer - a set of neurons that take the gene expression values and transforms it The output layer - a set of neurons that predict what tissue it belongs to.  To illustrate this idea, it looks something like this:\nThat sounds fairly easy, right? Let’s set up our model using Keras:\n# This is the simplest type of neural network setup from keras.models import Sequential # Dense layers from keras.layers import Dense # Function to convert tissue labels to a series of 1s and 0s from keras.utils import to_categorical # Convert the tissue labels to a Keras-friendly set of 1s and 0s # Get the actual tissues tissue_labels = mat[mat.columns[-1]] # Get a mapping between tissue name and some random integer tissue_labels_int = dict([ (v,i) for i,v in enumerate(set(tissue_labels)) ]) # Reverse this as well - it'll be useful for later. tissue_labels_rev = dict([ (v,k) for k,v in tissue_labels_int.items() ]) # Convert the entire column to the mapped integer tissue_labels_encoded = [tissue_labels_int[t] for t in tissue_labels] # Convert to a Keras-friendly label set of 1s and 0s labels = to_categorical(tissue_labels_encoded, num_classes=len(set(tissue_labels))) # This will be useful for later. indices = list(range(len(labels))) Our “hidden” layer, which does the bulk of the computation, will be taking in the gene expression data across all samples, then apply the activation function. Now, deciding the precise architecture for this problem is somewhat subjective. There’s no real guide (as far as I’m aware!) that recommends how many neurons should go in a particular layer, or how many layers there should be, full stop. Lots of this is about experimentation!\nIn this case, I’m going to have one hidden layer that takes the expression values of a 1000 genes, then compresses it into 200 “latent” genes using the ReLu activation function. During the development of this notebook, I also toyed with having two hidden layers (one with TanH activation, and the second with ReLu).\nlayer1_squish = 400 layer2_squish = 200 # Initialise our model model = Sequential() # Add one layer to fit in the data model.add(Dense(layer2_squish, input_shape=(len(mat.columns[:-1]),), activation='relu')) ### Alternative setup # model.add(Dense(layer1_squish, input_shape=(len(mat.columns[:-1]),), activation='tanh')) # model.add(Dense(layer2_squish, input_shape=(layer1_squish,), activation='relu')) To cap it off, I will have an output layer that uses the Softmax function. This is an activation function that is useful for categorical data, and can assign a value between 0 to 1 - essentially acting as a value that acts like a probability.\nFinally I will compile the model by:\n Using the stochastic gradient descent (SGD) optimiser - details for another time. A categorical cross-entropy loss function. This is essentially a function that tells the neural network how well it’s doing with respect to the true labels. The job of the neural network is to minimise this loss using SGD.  # Add one layer to do the prediction model.add(Dense(len(set(tissue_labels)), input_shape=(layer2_squish,), activation = 'softmax')) # Compile the model model.compile( optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'] ) Now that we got the model set up, what we’ll do now is to split the data into a training and test set:\nfrom sklearn.model_selection import train_test_split # Split to a 75/25 split using sklearn train_x, test_x, train_y, test_y = train_test_split(mat, indices, random_state = 42) Let’s run the model on our training data. For training, we are going to set up so that…\n 400 examples are used for training at a time - this is called the “mini batch” strategy that helps speed things up Run 50 epochs, or iterations.  # Let's run the model and run 50 iterations model.fit(train_x[train_x.columns[:-1]].values , labels[train_y], epochs=50, batch_size = 400) Epoch 1/50 13036/13036 [==============================] - 0s 32us/step - loss: 2.5404 - accuracy: 0.4580 Epoch 2/50 13036/13036 [==============================] - 0s 18us/step - loss: 1.0165 - accuracy: 0.7234 Epoch 3/50 13036/13036 [==============================] - 0s 21us/step - loss: 0.7074 - accuracy: 0.8045 ... Epoch 48/50 13036/13036 [==============================] - 0s 17us/step - loss: 0.1502 - accuracy: 0.9581 Epoch 49/50 13036/13036 [==============================] - 0s 18us/step - loss: 0.1515 - accuracy: 0.9571 Epoch 50/50 13036/13036 [==============================] - 0s 17us/step - loss: 0.1487 - accuracy: 0.9595  This is seriously impressive. The neural network starts out with 45.8% accuracy, but eventually climbs to 95.95% accuracy in assigning the correct tissue. Shall we see how it does on the test set?\n# Get Keras' own evaluation model.evaluate(test_x[test_x.columns[:-1]], labels[test_y])  4346/4346 [==============================] - 0s 39us/step [0.18317752908546694, 0.944086492061615]  Okay, so Keras says that it achieved a 94.4% accuracy on the test set. We can double check just to be sure.\n# Predict the classes given test set data predictions = model.predict_classes(test_x[test_x.columns[:-1]]) # Map the predictions back to a tissue name predictions_to_string = [ tissue_labels_rev[p] for p in predictions ] truth = test_x[\"TissueSite\"] pred_frame = pd.DataFrame(zip(truth, predictions_to_string), columns = [\"True Tissue\", \"Predicted Tissue\"]) # Let's see how many are incorrect pred_frame[pred_frame['True Tissue'] != pred_frame['Predicted Tissue']].shape # Plot a confusion matrix ax_len = len(set(truth)) lookup = dict([ (v,i) for i,v in enumerate(set(truth)) ]) mat = np.zeros((ax_len, ax_len)) for a,b in zip(truth, predictions_to_string): index_x, index_y = lookup[a], lookup[b] mat[index_x, index_y] += 1 normalised_mat = mat/np.sum(mat,axis=1) plt.imshow(normalised_mat, cmap = \"Blues\", interpolation = \"nearest\") plt.axis(\"off\") (218, 2)  So in over 4346 samples to predict, the neural network only got 218 incorrect! We can also see this visually in that coloured squares are predominantly found along the diagonal, suggesting that the true and predicted labels agree with each other.\nBenchmarking As a comparison, we can run a random forest for comparison. To get a refresher, check out my previous post on random forests.\nfrom sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression rfc = RandomForestClassifier() rfc.fit(train_x[train_x.columns[:-1]].values, tissue_labels[train_y]) RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  rf_predictions = rfc.predict(test_x[test_x.columns[:-1]].values) truth = test_x[\"TissueSite\"] pred_frame_rf = pd.DataFrame(zip(truth, rf_predictions), columns = [\"True Tissue\", \"Predicted Tissue (RF)\"]) # Let's see how many are incorrect pred_frame_rf[pred_frame_rf['True Tissue'] != pred_frame_rf['Predicted Tissue (RF)']].shape (205, 2)  This means the random forest had a 95.3% accuracy on the test set!\nInterpretation Hmm, interesting. So for the super hopeful, this result may seem like a surprise. How, or even why, is the random forest superior in this case? There are some things to be said here…\n The neural network that we’ve implemented is fairly basic. We only have one hidden layer, and for some applications, having more hidden layers can be a good way to increase accuracy. This could be one of those cases.  Possible solution: implement more layers.   The choice of the activation function can be influential; we haven’t exhaustively tested them here.  Possible solution: assess the impact of ReLU vs. Elu vs. Tanh… etc.   Random forests are remarkably good at detecting which features are most important for class discrimination. This is possible because the random forest does lots and lots of sampling to figure out the most pertinent feature sets.  Possible solution: allow the neural network to train for a longer time (epochs) to determine the more relevant features in the dataset.   The data itself could have other non-linear patterns that we have not fully exploited - anywhere from transforming the data (e.g. applying a kernel), to leveraging similar genes, are all options here.  Conclusions What next? Maybe for another time I’ll cover more detailed neural network architectures, and how they can be used for other types of problems. While the genes were not ordered in a particular way, another possibility is to order them in a certain manner that makes the gene expression matrix amenable to other deep learning methods like the convolutional neural network. Or we can even explore using autoencoders to cluster gene expression data.\nFor now, this is a wrap, and see you next time!\nAppendix 2. Optimisation of weights If you’re familiar with the mathematical theory behind linear regression, you would know that it’s essentially finding a line that minimises the residuals, or errors , of the prediction against your observations. This is done by (long story short) some fancy matrix algebra along the lines of\n$$ \\beta = (X^TX)^{-1}X^Ty $$\nFor neural networks, we often define a method to quantify that error in the form of a loss function. Essentially, this loss function states how far the prediction is from the true value that we’re trying to predict. There are loss functions for various types of data we want to predict. For instance, if we’re predicting what class a data point belongs to (i.e. a classification problem), we can use cross-entropy, or when we are predicting a continuous value (e.g. length of a throw given some conditions), then we can use mean absolute error, etc.\nWhatever the loss function, neural networks use a technique called backpropagation which takes the derivative of the loss function with respect to the weights. We use the derivative because this tells us how much the value of the loss function changes when we change the weights. Neat, right? These derivatives can then be used by optimisation algorithms like gradient descent.\nIf you’re familiar with calculus notations, we’re effectively trying to evaluate\n$$ \\dfrac{\\partial L}{\\partial w} $$\nwhich is then used to update the weights,\n$$ w \\leftarrow w + \\alpha \\dfrac{\\partial L}{\\partial w} $$\nwhere $$\\alpha$$ represents a “learning rate” to help convergence. While backpropagation is powerful, it doesn’t really carry a biological meaning. The implication here is that somehow we feed things backward to tell an upstream neuron how wrong it was.\n3. Many, many different architectures Why is it that “neural networks” always seem to headline insane prediction successes? That’s because neural networks can be flexibly constructed for different data types. For example,\n Convolutional neural networks use a 2D-set up of neurons to perceive a wider “field” of data - this is particularly useful for images. Recurrent neural networks have a repeated, sequential structure that’s suited for speech / text. Even traditional “feed-forward” neural networks, like one we’ll code today, can use tricks like “dropout” to ensure that there isn’t a huge over-fit to the data.  This is a section that deserves a post on its own!\n",
  "wordCount" : "4142",
  "inLanguage": "en",
  "datePublished": "2020-02-26T00:00:00Z",
  "dateModified": "2020-02-26T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/2020-02-26-anns/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Read between the rows",
    "logo": {
      "@type": "ImageObject",
      "url": "favicon.ico"
    }
  }
}
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="" accesskey="h" title="Read between the rows (Alt + H)">Read between the rows</a>
            <span class="logo-switches">
                <a id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </a>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      An introduction to Artificial Neural Networks
    </h1>
    <div class="post-meta">February 26, 2020&nbsp;·&nbsp;20 min

</div>
  </header> 

  <div class="toc">
    <details >
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#preamble--jargon-buster" aria-label="Preamble &#43; Jargon Buster">Preamble + Jargon Buster</a><ul>
            <li>
        <a href="#neuron-internals" aria-label="Neuron internals">Neuron internals</a></li></ul>
    </li><li>
        <a href="#jargon-summary" aria-label="Jargon Summary">Jargon Summary</a></li><li>
        <a href="#the-task-at-hand" aria-label="The task at hand">The task at hand</a><ul>
            <li>
        <a href="#what-are-the-use-cases" aria-label="What are the use cases?">What are the use cases?</a></li></ul>
    </li><li>
        <a href="#data-prep" aria-label="Data Prep">Data Prep</a><ul>
            <ul>
            <li>
        <a href="#looking-out-for-batch-effects" aria-label="Looking out for Batch Effects">Looking out for Batch Effects</a></li></ul>
        </ul>
    </li><li>
        <a href="#coding-up-the-nn" aria-label="Coding up the NN">Coding up the NN</a><ul>
            <li>
        <a href="#applying-the-neural-network" aria-label="Applying the Neural Network">Applying the Neural Network</a></li><li>
        <a href="#benchmarking" aria-label="Benchmarking">Benchmarking</a></li><li>
        <a href="#interpretation" aria-label="Interpretation">Interpretation</a></li></ul>
    </li><li>
        <a href="#conclusions" aria-label="Conclusions">Conclusions</a></li><li>
        <a href="#appendix" aria-label="Appendix">Appendix</a><ul>
            <li>
        <a href="#2-optimisation-of-weights" aria-label="2. Optimisation of weights">2. Optimisation of weights</a></li><li>
        <a href="#3-many-many-different-architectures" aria-label="3. Many, many different architectures">3. Many, many different architectures</a></li></ul>
</li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<p>For most AI aficionados, deep learning may seem like the innovation of yesterday. However, deep learning methods are still revolutionising the way we think about many fields today. I&rsquo;ve seen that happen with <a href="https://www.nature.com/articles/s41586-019-1923-7">protein structure prediction</a>.</p>
<p>Perhaps the most iconic deep learning methods that you may have heard of will include:</p>
<ul>
<li>The convolutional neural network (CNN) and</li>
<li>The recurrent neural network (RNN).</li>
</ul>
<p>CNNs have changed the way we think about image classification, while RNNs have been hugely influential in sequential problems (e.g. text recognition).</p>
<p>But, before we talk about CNNs and RNNs, we should take a step back and ask <em>what</em> is a neural network. For some, this may still seem like a black box, especially if you&rsquo;ve never had any formal training in the area. In this post, I&rsquo;ll try to explain what neural networks are, and hopefully do a bit of jargon busting along the way.</p>
<p>If you have&hellip;</p>
<ul>
<li><strong>30 seconds</strong>: neural networks are a <em>family</em> of algorithms in artificial intelligence. Inspired by biological neurons, they can perform extremely well. However, poor design choices can lead to some unusual behaviours, and for some tasks, they may not even be <em>the</em> method of choice.</li>
<li><strong>15 minutes</strong>: go on. If you&rsquo;re happy with some basic terminology, feel free to just jump to the code section.</li>
</ul>
<h2 id="preamble--jargon-buster">Preamble + Jargon Buster<a hidden class="anchor" aria-hidden="true" href="#preamble--jargon-buster">#</a></h2>
<p>The artificial neural network was inspired in part by the biological neuron:</p>
<!-- raw HTML omitted -->
<p>The idea here is, each neuron takes an input (i.e. incoming neurotransmitters). From the combination of inputs, it produces an output signal (i.e. an action potential). When a series of neurons work sequentially and in tandem, you get an entire network.</p>
<p>From the view of artificial intelligence, a &ldquo;neuron&rdquo; is really a unit that computes some function given some input. We can see some parallels when we use simplified circles and arrows, and how they can line up as a &ldquo;network&rdquo;:</p>
<!-- raw HTML omitted -->
<p>What makes neural networks special, in my view, are three things:</p>
<ul>
<li><strong>Neuron internals</strong>: Each neuron can transform the input data. Once transformed, the data lies in a so-called hidden or &ldquo;latent&rdquo; space. In the original space, the data may not have had any obvious patterns, but in the latent representation, the data may show some neat patterns.</li>
<li><strong>Backpropagation</strong>: Using some neat calculus tricks, neural networks can be fine-tuned very well</li>
<li><strong>Flexible architectures</strong>: There are loads of ways to build networks to tailor for your type of data.</li>
</ul>
<p>I&rsquo;ve explained points 2 and 3 in more detail in the &ldquo;Appendix&rdquo; at the bottom. We&rsquo;ll explain point 1 as it&rsquo;s crucial.</p>
<h3 id="neuron-internals">Neuron internals<a hidden class="anchor" aria-hidden="true" href="#neuron-internals">#</a></h3>
<p>To understand what we mean by</p>
<blockquote>
<p>neural networks can transform the input data</p>
</blockquote>
<p>We have to first review the humble linear regression - the &ldquo;line of best fit&rdquo;.</p>
<p>$$ y = mx + b$$</p>
<p>The idea is that the predictor variable, $$y$$, is equivalent to a sum of the variable $$x$$ that&rsquo;s multiplied by some
value $$m$$, along with an adjusting constant, $$b$$.</p>
<p>You can expand the idea to have multiple types of $$x$$s and their associated multipliers. To make life easier, and to keep consistency with math textbook notation, let&rsquo;s rewrite</p>
<p>$$ y = m_1x_1 + m_2x_2 + &hellip; + b $$</p>
<p>as</p>
<p>$$ y = b_1x_1 + b_2x_2 + &hellip; + b_0 $$</p>
<p>Each of these $$b_1, b_2, &hellip; $$ are multipliers; or in neural network speak, the <em>weights</em>. They are also known as a
model&rsquo;s &ldquo;parameters&rdquo;. By changing the $$b$$ values, the value of $$y$$ will change.</p>
<p>The final term, $$b_0$$, is the intercept of the linear model, or <em>bias</em> in neural network speak.</p>
<p>What neural networks do is to use the output of the linear regression, then apply <em>another</em> mathematical function.</p>
<p>These functions are called <em>activation functions</em>. For example, the <em>sigmoid</em> function</p>
<p>$$ S(y) = \dfrac{1}{1+e^{-y}} $$</p>
<p>is a non-linear function that transforms the output of the linear regression data into an S-shape.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#75715e"># Check out my matplotlib stylesheet!</span>
plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#34;bbc&#34;</span>)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)

<span style="color:#75715e"># Let&#39;s just get 20 values from -10 to +10. (we do +11 here otherwise it stops at 9.)</span>
y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">11</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(v):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span><span style="color:#f92672">*</span>v))

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(y)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(sigmoid(y))

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of y&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of sigmoid(y)&#34;</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><!-- raw HTML omitted -->
<p>Thus, a neuron with a <strong>sigmoid</strong> <em>activation function</em> squishes the output of a linear regression into an S-curve within 0 and 1. If we repeat a similar type of logic across all neurons, the inputs will get transformed according to what we choose as the activation function.</p>
<p>Once the data has been transformed by the activation function, this <em>transformed</em> data can then be used as the input to a new group of neurons downstream! The downstream layer of neurons may or may not use the same activation function on the transformed data from the previous layer.</p>
<p>There&rsquo;s quite a few activation functions, and I&rsquo;ve written a couple here for reference:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Equation</th>
<th>Range</th>
<th>What does it look like</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td>$$y$$</td>
<td>$$-\infty, +\infty$$</td>
<td>Just a straight diagonal line</td>
</tr>
<tr>
<td>Sigmoid</td>
<td>$$ S(y) = \dfrac{1}{1+e^{-y}} $$</td>
<td>0,1</td>
<td>Shaped like an S curve</td>
</tr>
<tr>
<td>TanH</td>
<td>$$ tanh(y) $$</td>
<td>-1,1</td>
<td>Shaped like an S curve</td>
</tr>
<tr>
<td>Rectified Linear Unit (ReLu)</td>
<td>$$ Relu(y) = max(0,y) $$</td>
<td>0, $$\infty$$</td>
<td>Flat until y = 0, then diagonal</td>
</tr>
<tr>
<td>Softmax</td>
<td>$$ Softmax(y) = \dfrac{e^y}{\sum_{i=1}^{K} e^{y}} $$</td>
<td>0,1</td>
<td>Shaped like an S curve</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tanh</span>(v):
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>tanh(v)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu</span>(v):
    <span style="color:#66d9ef">return</span> list(map(<span style="color:#66d9ef">lambda</span> _: max(<span style="color:#ae81ff">0</span>, _), v))

fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>)
ax <span style="color:#f92672">=</span> axes<span style="color:#f92672">.</span>flatten()

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(y)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(sigmoid(y))
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>plot(tanh(y))
ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>plot(relu(y))

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of y&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of sigmoid(y)&#34;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of tanh(y)&#34;</span>)
ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Value of Relu(y)&#34;</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">8</span>))
</code></pre></div><!-- raw HTML omitted -->
<p>Now when we talk about neural networks, there is some jargon that&rsquo;s bound to float around. I&rsquo;ll leave this here for reference.</p>
<h2 id="jargon-summary">Jargon Summary<a hidden class="anchor" aria-hidden="true" href="#jargon-summary">#</a></h2>
<ul>
<li><em>Neuron</em> : we refer to a neuron as one unit that computes some calculation.</li>
<li><em>Loss</em> : a metric to describe how far we are off from the true value</li>
<li><em>Bias</em> : the intercept</li>
<li><em>Weights</em> : a series of multipliers for each variable</li>
<li><em>Activation function</em> : a function that is applied to transform the output of a neuron</li>
</ul>
<h2 id="the-task-at-hand">The task at hand<a hidden class="anchor" aria-hidden="true" href="#the-task-at-hand">#</a></h2>
<p>To start, let&rsquo;s get some RNAseq data from GTEx. This diagram from <a href="https://www.nature.com/articles/nature24277">Nature</a> just shows the huge diversity of the gene expression data sources.
Building on a <a href="../../../2019/12/18/clustering-2.html">previous post</a> where I clustered gene expression levels using DBSCAN, let&rsquo;s try to use a supervised approach and predict the tissues instead.</p>
<p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature24277/MediaObjects/41586_2017_Article_BFnature24277_Fig1_HTML.jpg" alt="image"></p>
<h3 id="what-are-the-use-cases">What are the use cases?<a hidden class="anchor" aria-hidden="true" href="#what-are-the-use-cases">#</a></h3>
<ul>
<li>If there is a loss of documentation, the gene expression pattern may tell us what tissue a sample is likely to be derived from.</li>
<li>Determine if related tissues show similar expression profiles; vice-versa, do some genes behave in the same way across some tissues?</li>
<li>Establish the boundaries of a &ldquo;normal&rdquo; or &ldquo;healthy&rdquo; tissue based on gene expression, allowing us to detect anomalies</li>
</ul>
<h2 id="data-prep">Data Prep<a hidden class="anchor" aria-hidden="true" href="#data-prep">#</a></h2>
<p>As I covered in my previous post, the GTEx gene expression data is pretty big. In fact, getting it all at once is not ideal. We&rsquo;re going to use some tricks:</p>
<ul>
<li>&ldquo;Stream&rdquo; the data – have a pointer to the data, but don&rsquo;t bring it in all at once.</li>
<li>Use the most variably expressed genes - if a gene is expressed everywhere with low rates, or is only found in, say, one sample, it&rsquo;s not that informative.</li>
<li>Use protein-coding genes.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Importing useful things</span>

<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> urllib.request <span style="color:#f92672">import</span> urlopen
<span style="color:#f92672">from</span> gzip <span style="color:#f92672">import</span> GzipFile
<span style="color:#f92672">import</span> io
<span style="color:#f92672">import</span> multiprocessing <span style="color:#f92672">as</span> mp
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#75715e"># Some utility functions - feel free to skip</span>
DECODE_CODEC <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;utf-8&#39;</span>

<span style="color:#75715e"># This is the length of an Ensembl gene identifier which we&#39;ll use later.</span>
ENSEMBL_LENGTH <span style="color:#f92672">=</span> <span style="color:#ae81ff">15</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stream_request</span>(url):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    Open a connection to some url, then stream data from it
</span><span style="color:#e6db74">    This has the advantage of:
</span><span style="color:#e6db74">    A. We don&#39;t have to wait for the entire file to download to do operations
</span><span style="color:#e6db74">    B. We can perform some operations on-the-fly
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    fh <span style="color:#f92672">=</span> urlopen(url)
    buffer <span style="color:#f92672">=</span> io<span style="color:#f92672">.</span>StringIO()
    
    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> fh:
        decoded <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>decode(DECODE_CODEC)
        buffer<span style="color:#f92672">.</span>write(decoded)
    fh<span style="color:#f92672">.</span>close()
    
    <span style="color:#75715e"># Reset the StringIO buffer to byte position 0</span>
    buffer<span style="color:#f92672">.</span>seek(<span style="color:#ae81ff">0</span>)
    <span style="color:#66d9ef">return</span> buffer
    
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">stream_request_to_pandas</span>(url: str, sep: str <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>) <span style="color:#f92672">-&gt;</span> pd<span style="color:#f92672">.</span>DataFrame:
    streamed_buffer <span style="color:#f92672">=</span> stream_request(url)
    <span style="color:#66d9ef">return</span> pd<span style="color:#f92672">.</span>read_csv(streamed_buffer, sep <span style="color:#f92672">=</span> sep)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Stream in and read protein-coding genes from HGNC</span>
geneUrl <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://www.genenames.org/cgi-bin/download/custom?col=gd_app_sym&amp;col=gd_pub_ensembl_id&amp;status=Approved&amp;hgnc_dbtag=on&amp;order_by=gd_app_sym_sort&amp;format=text&amp;where=(gd_pub_chrom_map%20not</span><span style="color:#e6db74">%20li</span><span style="color:#e6db74">ke</span><span style="color:#e6db74">%20%</span><span style="color:#e6db74">27%25patch</span><span style="color:#e6db74">%25%</span><span style="color:#e6db74">27%20and</span><span style="color:#e6db74">%20g</span><span style="color:#e6db74">d_pub_chrom_map%20not</span><span style="color:#e6db74">%20li</span><span style="color:#e6db74">ke</span><span style="color:#e6db74">%20%</span><span style="color:#e6db74">27%25alternate</span><span style="color:#e6db74">%20r</span><span style="color:#e6db74">eference</span><span style="color:#e6db74">%20lo</span><span style="color:#e6db74">cus</span><span style="color:#e6db74">%25%</span><span style="color:#e6db74">27)%0Aand</span><span style="color:#e6db74">%20g</span><span style="color:#e6db74">d_locus_type%20=</span><span style="color:#e6db74">%20%</span><span style="color:#e6db74">27gene%20with%20protein%20product%27&amp;submit=submit&#34;</span>

<span style="color:#75715e"># This is a short way to convert that URL into a table</span>
gene_df <span style="color:#f92672">=</span> stream_request_to_pandas(geneUrl)

<span style="color:#75715e"># Let&#39;s see what it looks like</span>
gene_df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Keep a list of protein identifiers here</span>
proteins <span style="color:#f92672">=</span> gene_df[<span style="color:#e6db74">&#39;Ensembl gene ID&#39;</span>]<span style="color:#f92672">.</span>values
</code></pre></div><p>Now let&rsquo;s get that big GTEx file</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Open a handle onto the GTEx expression data</span>
URL <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://storage.googleapis.com/gtex_analysis_v8/rna_seq_data/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_tpm.gct.gz&#34;</span>
urlHandle <span style="color:#f92672">=</span> urlopen(URL)

fh <span style="color:#f92672">=</span> GzipFile(fileobj<span style="color:#f92672">=</span>urlHandle)

<span style="color:#75715e"># ignore the first two lines as they contain shape of the file</span>
_ <span style="color:#f92672">=</span> fh<span style="color:#f92672">.</span>readline()
_ <span style="color:#f92672">=</span> fh<span style="color:#f92672">.</span>readline()
</code></pre></div><p>From that big GTEx file, let&rsquo;s get the column names; this is in the third line of the file.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">header <span style="color:#f92672">=</span> fh<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>decode(DECODE_CODEC)<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)
</code></pre></div><p>Now, each line looks something like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">first_row <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(fh<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>decode(DECODE_CODEC)<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>))
columns <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>append( np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>]), np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">2</span>, len(header), <span style="color:#ae81ff">20</span>) )

pd<span style="color:#f92672">.</span>DataFrame([first_row[columns]], columns<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>array(header)[columns])
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<pre><code>  &lt;th&gt;...&lt;/th&gt;
  
  &lt;th&gt;GTEX-13OW7-0011-R4b-SM-5O9CX&lt;/th&gt;
  &lt;th&gt;GTEX-VJYA-1826-SM-4KL1W&lt;/th&gt;
&lt;/tr&gt;
</code></pre>
<!-- raw HTML omitted -->
<p>Okay, cool, so we now know that the third column onward just has numbers. The first column contains gene IDs.</p>
<p>Knowing this file format, let&rsquo;s get the most variable protein-coding genes. We&rsquo;ll also impose that at least half the samples have non-zero expression data. To assess variability, we&rsquo;re going to use a metric called mean absolute deviation, then store it. Another viable approach is calculating something like the entropy value.</p>
<p>Before we do so, we&rsquo;re going to log transform the data. GTEx data has lots of 0s, and TPM units can be out of scale.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_gene_mad</span>(values):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    The mean absolute deviation is equal to avg( abs(x - avg) )
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    mu_x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(values)
    mad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(np<span style="color:#f92672">.</span>abs(values <span style="color:#f92672">-</span> mu_x))    
    <span style="color:#66d9ef">return</span> mad

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">wrapper</span>(index, line):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    This is a function we&#39;ll call with a parallel pool connection.
</span><span style="color:#e6db74">    We ignore any line that contains non protein-coding genes
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> index <span style="color:#f92672">%</span> <span style="color:#ae81ff">5000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Processing line {}&#34;</span><span style="color:#f92672">.</span>format(index))
        
    tokenised_line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>decode(DECODE_CODEC)<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#39;</span>)
    gene_id <span style="color:#f92672">=</span> tokenised_line[<span style="color:#ae81ff">0</span>][:ENSEMBL_LENGTH]
    
    <span style="color:#75715e"># If it&#39;s not a protein-coding gene, let&#39;s ignore</span>
    <span style="color:#66d9ef">if</span> gene_id <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> proteins:
        <span style="color:#66d9ef">return</span> None
    
    numeric_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(tokenised_line[<span style="color:#ae81ff">2</span>:], dtype<span style="color:#f92672">=</span>float)
    array_cutoff <span style="color:#f92672">=</span> len(numeric_values) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2.</span>
    
    <span style="color:#75715e"># If more than half the data is zero, then let&#39;s ignore</span>
    <span style="color:#66d9ef">if</span> sum(numeric_values <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">&gt;=</span> array_cutoff:
        <span style="color:#66d9ef">return</span> None
    
    numeric_values <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>log(numeric_values<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    mad <span style="color:#f92672">=</span> get_gene_mad(numeric_values)
    
    <span style="color:#66d9ef">return</span> gene_id, mad, tokenised_line
</code></pre></div><p>To see what log transformation can do, see this plot from my <a href="../../../2019/12/18/clustering-2.html">previous post</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">keep_data <span style="color:#f92672">=</span> []
gene_mad <span style="color:#f92672">=</span> {}

<span style="color:#75715e"># Close the previous connection</span>
fh<span style="color:#f92672">.</span>close()

<span style="color:#75715e"># Re-establish the connection, and skip the first three lines</span>
urlHandle <span style="color:#f92672">=</span> urlopen(URL)
fh <span style="color:#f92672">=</span> GzipFile(fileobj<span style="color:#f92672">=</span>urlHandle)

<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Re-established connection, skipping 3 lines...&#34;</span>)
[fh<span style="color:#f92672">.</span>readline() <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>)]

<span style="color:#75715e"># Let&#39;s parallelise this to make it faster</span>
pool <span style="color:#f92672">=</span> mp<span style="color:#f92672">.</span>Pool(<span style="color:#ae81ff">4</span>)

<span style="color:#75715e"># Let&#39;s get some results where we have just protein coding genes.</span>
jobs <span style="color:#f92672">=</span> [pool<span style="color:#f92672">.</span>apply_async(wrapper, args<span style="color:#f92672">=</span>(i,line,)) <span style="color:#66d9ef">for</span> i,line <span style="color:#f92672">in</span> enumerate(fh) ]
results <span style="color:#f92672">=</span> [j<span style="color:#f92672">.</span>get() <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> jobs]
filtered_results <span style="color:#f92672">=</span> [ r <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> results <span style="color:#66d9ef">if</span> r <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None ]
<span style="color:#66d9ef">del</span> results
</code></pre></div><pre><code>Re-established connection, skipping 3 lines...
Processing line 0
...
Processing line 55000
</code></pre>
<p>Brilliant, now that we have our gene MAD values we can just select the top, say, 1000 genes.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">TOP <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
sorted_genes <span style="color:#f92672">=</span> sorted(filtered_results, key <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span>True)[:TOP]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Let&#39;s create a dataframe</span>
df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(
    [ np<span style="color:#f92672">.</span>log(np<span style="color:#f92672">.</span>array(_[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">2</span>:],float)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> sorted_genes ],
    columns <span style="color:#f92672">=</span> header[<span style="color:#ae81ff">2</span>:],
    index <span style="color:#f92672">=</span> [ _[<span style="color:#ae81ff">2</span>][<span style="color:#ae81ff">0</span>] <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> sorted_genes ]
)
df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<h4 id="looking-out-for-batch-effects">Looking out for Batch Effects<a hidden class="anchor" aria-hidden="true" href="#looking-out-for-batch-effects">#</a></h4>
<p>While we have our top 1000 most variably expressed genes, there is a caveat! Gene expression data can be prone to batch effects (i.e. technical variation). To check for this behaviour, let&rsquo;s get some sample annotations.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sample_url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://storage.googleapis.com/gtex_analysis_v8/annotations/GTEx_Analysis_v8_Annotations_SampleAttributesDS.txt&#34;</span>
sample_metadata <span style="color:#f92672">=</span> stream_request_to_pandas(sample_url)
subset <span style="color:#f92672">=</span> sample_metadata[sample_metadata[<span style="color:#e6db74">&#39;SAMPID&#39;</span>]<span style="color:#f92672">.</span>isin(df<span style="color:#f92672">.</span>columns)]<span style="color:#f92672">.</span>copy()

tissue_column <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SMTSD&#39;</span>
batch_column <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SMGEBTCH&#39;</span>

subset<span style="color:#f92672">.</span>iloc[:<span style="color:#ae81ff">3</span>][[<span style="color:#e6db74">&#39;SAMPID&#39;</span>, tissue_column, batch_column]]
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>We can see that each sample has an associated batch ID. We can colour our plot by the batch ID and the tissue:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA

sample_id_to_batch <span style="color:#f92672">=</span> subset[[<span style="color:#e6db74">&#39;SAMPID&#39;</span>, batch_column]]
sample_id_to_tissue <span style="color:#f92672">=</span> subset[[<span style="color:#e6db74">&#39;SAMPID&#39;</span>, tissue_column]]

pca <span style="color:#f92672">=</span> PCA()

<span style="color:#75715e"># We take the transpose of our gene expression because otherwise PCA would find the samples that are most varied</span>
<span style="color:#75715e"># Rather than the genes that are most varied.</span>
coords <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(df<span style="color:#f92672">.</span>T<span style="color:#f92672">.</span>values)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)

special_tissue <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Muscle - Skeletal&#39;</span>
tissue_dots <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(sample_id_to_tissue[tissue_column]<span style="color:#f92672">==</span>special_tissue)<span style="color:#f92672">.</span>flatten()

t1_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;LCSET-4822&#34;</span>
t2_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;LCSET-4824&#34;</span>
t3_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;LCSET-4416&#34;</span>
t4_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;LCSET-10515&#34;</span>

t1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(sample_id_to_batch[batch_column]<span style="color:#f92672">==</span>t1_name)<span style="color:#f92672">.</span>flatten() 
t2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(sample_id_to_batch[batch_column]<span style="color:#f92672">==</span>t2_name)<span style="color:#f92672">.</span>flatten()
t3 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(sample_id_to_batch[batch_column]<span style="color:#f92672">==</span>t3_name)<span style="color:#f92672">.</span>flatten()

t4 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argwhere(sample_id_to_batch[batch_column]<span style="color:#f92672">==</span>t4_name)<span style="color:#f92672">.</span>flatten()

t1_inter <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>intersect1d(t1, tissue_dots)
t2_inter <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>intersect1d(t2, tissue_dots)
t3_inter <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>intersect1d(t3, tissue_dots)
t4_inter <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>intersect1d(t4, tissue_dots)

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>scatter(coords[:,<span style="color:#ae81ff">0</span>], coords[:,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#c3c3c3&#39;</span>, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>scatter(coords[tissue_dots,<span style="color:#ae81ff">0</span>], coords[tissue_dots,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#348abd&#39;</span>, 
              label <span style="color:#f92672">=</span> special_tissue)

ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[:,<span style="color:#ae81ff">0</span>], coords[:,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#c3c3c3&#39;</span>, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[t1_inter,<span style="color:#ae81ff">0</span>], coords[t1_inter,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;green&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Batch {}&#34;</span><span style="color:#f92672">.</span>format(t1_name))
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[t2_inter,<span style="color:#ae81ff">0</span>], coords[t2_inter,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#a60628&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Batch {}&#34;</span><span style="color:#f92672">.</span>format(t2_name))
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[t3_inter,<span style="color:#ae81ff">0</span>], coords[t3_inter,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#ffd700&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Batch {}&#34;</span><span style="color:#f92672">.</span>format(t3_name))
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[t4_inter,<span style="color:#ae81ff">0</span>], coords[t4_inter,<span style="color:#ae81ff">1</span>], color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#f58426&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Batch {}&#34;</span><span style="color:#f92672">.</span>format(t4_name))

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper right&#39;</span>)
fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">5</span>))
</code></pre></div><!-- raw HTML omitted -->
<p>There does seem to be some level of batch effect, which we can adjust using tools like <a href="https://github.com/brentp/combat.py">ComBat</a>, but we won&rsquo;t do that here, and use the data as-is for simplicity.</p>
<h2 id="coding-up-the-nn">Coding up the NN<a hidden class="anchor" aria-hidden="true" href="#coding-up-the-nn">#</a></h2>
<p>To code up our neural network, we are spoiled for choice - there&rsquo;s loads of solutions (TensorFlow, PyTorch), but the simplest framework in my mind is Keras. One of the advantages of Keras is that it is very easy to read, and covers a a good range of use cases (e.g. convolutional layers). To train a neural network to predict the tissue that a gene belongs to, we need to know which tissue the sample is from (i.e. the truth).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">subset<span style="color:#f92672">.</span>iloc[:<span style="color:#ae81ff">3</span>][[<span style="color:#e6db74">&#39;SAMPID&#39;</span>, tissue_column]]
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Since our data is arranged in the format of <em>genes $$\times$$ samples</em> , we are going to transpose the matrix. This is because we want the neural network to take, the expression profile <em>per sample</em> as opposed to <em>per gene</em>.
This way, it&rsquo;ll predict what tissue we should assign for a particular sample.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Get a dictionary of sample id to tissue site</span>
sample_to_tissue <span style="color:#f92672">=</span> dict(subset[[<span style="color:#e6db74">&#39;SAMPID&#39;</span>, tissue_column]]<span style="color:#f92672">.</span>values)

<span style="color:#75715e"># Get the tranposed matrix and subset a few columns for preview</span>
mat <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>T
mat[<span style="color:#e6db74">&#39;TissueSite&#39;</span>] <span style="color:#f92672">=</span> [ sample_to_tissue[ix] <span style="color:#66d9ef">for</span> ix <span style="color:#f92672">in</span> mat<span style="color:#f92672">.</span>index ]
mat[ list(mat<span style="color:#f92672">.</span>columns[:<span style="color:#ae81ff">3</span>]) <span style="color:#f92672">+</span> [<span style="color:#e6db74">&#39;TissueSite&#39;</span>] ]
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<h3 id="applying-the-neural-network">Applying the Neural Network<a hidden class="anchor" aria-hidden="true" href="#applying-the-neural-network">#</a></h3>
<p>Now that we have a filtered dataset to work with (finally), we can now train a neural network. Each neural network can be described in terms of its &ldquo;layers&rdquo;. Each layer essentially represents a set of neurons. There are three types of layers that we&rsquo;ll have in our network today:</p>
<ul>
<li>The input layer - a set of neurons that just emit the gene expression data</li>
<li>The hidden layer - a set of neurons that take the gene expression values and transforms it</li>
<li>The output layer - a set of neurons that predict what tissue it belongs to.</li>
</ul>
<p>To illustrate this idea, it looks something like this:</p>
<!-- raw HTML omitted -->
<p>That sounds fairly easy, right? Let&rsquo;s set up our model using <a href="https://keras.io/">Keras</a>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># This is the simplest type of neural network setup</span>
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential

<span style="color:#75715e"># Dense layers</span>
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Dense

<span style="color:#75715e"># Function to convert tissue labels to a series of 1s and 0s</span>
<span style="color:#f92672">from</span> keras.utils <span style="color:#f92672">import</span> to_categorical

<span style="color:#75715e"># Convert the tissue labels to a Keras-friendly set of 1s and 0s</span>

<span style="color:#75715e"># Get the actual tissues</span>
tissue_labels <span style="color:#f92672">=</span> mat[mat<span style="color:#f92672">.</span>columns[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]

<span style="color:#75715e"># Get a mapping between tissue name and some random integer</span>
tissue_labels_int <span style="color:#f92672">=</span> dict([ (v,i) <span style="color:#66d9ef">for</span> i,v <span style="color:#f92672">in</span> enumerate(set(tissue_labels)) ])

<span style="color:#75715e"># Reverse this as well - it&#39;ll be useful for later.</span>
tissue_labels_rev <span style="color:#f92672">=</span> dict([ (v,k) <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> tissue_labels_int<span style="color:#f92672">.</span>items() ])

<span style="color:#75715e"># Convert the entire column to the mapped integer</span>
tissue_labels_encoded <span style="color:#f92672">=</span> [tissue_labels_int[t] <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> tissue_labels]

<span style="color:#75715e"># Convert to a Keras-friendly label set of 1s and 0s</span>
labels <span style="color:#f92672">=</span> to_categorical(tissue_labels_encoded, num_classes<span style="color:#f92672">=</span>len(set(tissue_labels)))

<span style="color:#75715e"># This will be useful for later.</span>
indices <span style="color:#f92672">=</span> list(range(len(labels)))
</code></pre></div><p>Our &ldquo;hidden&rdquo; layer, which does the bulk of the computation, will be taking in the gene expression data across all samples, then apply the <em>activation function</em>. Now, deciding the precise architecture for this problem is somewhat subjective. There&rsquo;s no real guide (as far as I&rsquo;m aware!) that recommends how many neurons should go in a particular layer, or how many layers there should be, full stop. Lots of this is about experimentation!</p>
<p>In this case, I&rsquo;m going to have <em>one</em> hidden layer that takes the expression values of a 1000 genes, then compresses it into 200 &ldquo;latent&rdquo; genes using the ReLu activation function. During the development of this notebook, I also toyed with having two hidden layers (one with TanH activation, and the second with ReLu).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">layer1_squish <span style="color:#f92672">=</span> <span style="color:#ae81ff">400</span>
layer2_squish <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>

<span style="color:#75715e"># Initialise our model</span>
model <span style="color:#f92672">=</span> Sequential()

<span style="color:#75715e"># Add one layer to fit in the data</span>
model<span style="color:#f92672">.</span>add(Dense(layer2_squish, input_shape<span style="color:#f92672">=</span>(len(mat<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]),), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>))

<span style="color:#75715e">### Alternative setup</span>
<span style="color:#75715e"># model.add(Dense(layer1_squish, input_shape=(len(mat.columns[:-1]),), activation=&#39;tanh&#39;))</span>
<span style="color:#75715e"># model.add(Dense(layer2_squish, input_shape=(layer1_squish,), activation=&#39;relu&#39;))</span>

</code></pre></div><p>To cap it off, I will have an output layer that uses the Softmax function. This is an activation function that is useful for categorical data, and can assign a value between 0 to 1 - essentially acting as a value that acts like a probability.</p>
<p>Finally I will compile the model by:</p>
<ul>
<li>Using the stochastic gradient descent (SGD) optimiser - details for another time.</li>
<li>A categorical cross-entropy loss function. This is essentially a function that tells the neural network how well it&rsquo;s doing with respect to the true labels. The job of the neural network is to minimise this loss using SGD.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Add one layer to do the prediction</span>
model<span style="color:#f92672">.</span>add(Dense(len(set(tissue_labels)), input_shape<span style="color:#f92672">=</span>(layer2_squish,), activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;softmax&#39;</span>))

<span style="color:#75715e"># Compile the model</span>
model<span style="color:#f92672">.</span>compile(
    optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sgd&#39;</span>,
    loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;categorical_crossentropy&#39;</span>,
    metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>]
)
</code></pre></div><p>Now that we got the model set up, what we&rsquo;ll do now is to split the data into a training and test set:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split

<span style="color:#75715e"># Split to a 75/25 split using sklearn</span>
train_x, test_x, train_y, test_y <span style="color:#f92672">=</span> train_test_split(mat, indices, random_state <span style="color:#f92672">=</span> <span style="color:#ae81ff">42</span>)
</code></pre></div><p>Let&rsquo;s run the model on our training data. For training, we are going to set up so that&hellip;</p>
<ul>
<li>400 examples are used for training at a time - this is called the &ldquo;mini batch&rdquo; strategy that helps speed things up</li>
<li>Run 50 epochs, or iterations.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Let&#39;s run the model and run 50 iterations</span>
model<span style="color:#f92672">.</span>fit(train_x[train_x<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]<span style="color:#f92672">.</span>values , labels[train_y], epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">400</span>)
</code></pre></div><pre><code>Epoch 1/50
13036/13036 [==============================] - 0s 32us/step - loss: 2.5404 - accuracy: 0.4580
Epoch 2/50
13036/13036 [==============================] - 0s 18us/step - loss: 1.0165 - accuracy: 0.7234
Epoch 3/50
13036/13036 [==============================] - 0s 21us/step - loss: 0.7074 - accuracy: 0.8045
...
Epoch 48/50
13036/13036 [==============================] - 0s 17us/step - loss: 0.1502 - accuracy: 0.9581
Epoch 49/50
13036/13036 [==============================] - 0s 18us/step - loss: 0.1515 - accuracy: 0.9571
Epoch 50/50
13036/13036 [==============================] - 0s 17us/step - loss: 0.1487 - accuracy: 0.9595

&lt;keras.callbacks.callbacks.History at 0x62601afd0&gt;
</code></pre>
<p>This is seriously impressive. The neural network starts out with 45.8% accuracy, but eventually climbs to
95.95% accuracy in assigning the correct tissue. Shall we see how it does on the test set?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Get Keras&#39; own evaluation</span>
model<span style="color:#f92672">.</span>evaluate(test_x[test_x<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]], labels[test_y])
</code></pre></div><pre><code> 4346/4346 [==============================] - 0s 39us/step

[0.18317752908546694, 0.944086492061615]
</code></pre>
<p>Okay, so Keras says that it achieved a 94.4% accuracy on the test set. We can double check just to be sure.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Predict the classes given test set data</span>
predictions <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict_classes(test_x[test_x<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]])

<span style="color:#75715e"># Map the predictions back to a tissue name</span>
predictions_to_string <span style="color:#f92672">=</span> [ tissue_labels_rev[p] <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> predictions  ]

truth <span style="color:#f92672">=</span> test_x[<span style="color:#e6db74">&#34;TissueSite&#34;</span>]
pred_frame <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(zip(truth, predictions_to_string), columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;True Tissue&#34;</span>, <span style="color:#e6db74">&#34;Predicted Tissue&#34;</span>])

<span style="color:#75715e"># Let&#39;s see how many are incorrect</span>
pred_frame[pred_frame[<span style="color:#e6db74">&#39;True Tissue&#39;</span>] <span style="color:#f92672">!=</span> pred_frame[<span style="color:#e6db74">&#39;Predicted Tissue&#39;</span>]]<span style="color:#f92672">.</span>shape

<span style="color:#75715e"># Plot a confusion matrix</span>
ax_len <span style="color:#f92672">=</span> len(set(truth))
lookup <span style="color:#f92672">=</span> dict([ (v,i) <span style="color:#66d9ef">for</span> i,v <span style="color:#f92672">in</span> enumerate(set(truth)) ])
mat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((ax_len, ax_len))

<span style="color:#66d9ef">for</span> a,b <span style="color:#f92672">in</span> zip(truth, predictions_to_string):
    index_x, index_y <span style="color:#f92672">=</span> lookup[a], lookup[b]
    mat[index_x, index_y] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

normalised_mat <span style="color:#f92672">=</span> mat<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>sum(mat,axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)

plt<span style="color:#f92672">.</span>imshow(normalised_mat, cmap <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Blues&#34;</span>, interpolation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;nearest&#34;</span>)
plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
</code></pre></div><!-- raw HTML omitted -->
<pre><code>(218, 2)
</code></pre>
<p>So in over 4346 samples to predict, the neural network only got 218 incorrect! We can also see this visually
in that coloured squares are predominantly found along the diagonal, suggesting that the true and predicted labels agree
with each other.</p>
<h3 id="benchmarking">Benchmarking<a hidden class="anchor" aria-hidden="true" href="#benchmarking">#</a></h3>
<p>As a comparison, we can run a random forest for comparison. To get a refresher, check out my <a href="../../../2019/11/01/supervised.html">previous post
on random forests</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestClassifier
<span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression

rfc <span style="color:#f92672">=</span> RandomForestClassifier()
rfc<span style="color:#f92672">.</span>fit(train_x[train_x<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]<span style="color:#f92672">.</span>values, tissue_labels[train_y])
</code></pre></div><pre><code>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">rf_predictions <span style="color:#f92672">=</span> rfc<span style="color:#f92672">.</span>predict(test_x[test_x<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]]<span style="color:#f92672">.</span>values)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">truth <span style="color:#f92672">=</span> test_x[<span style="color:#e6db74">&#34;TissueSite&#34;</span>]
pred_frame_rf <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(zip(truth, rf_predictions), columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;True Tissue&#34;</span>, <span style="color:#e6db74">&#34;Predicted Tissue (RF)&#34;</span>])

<span style="color:#75715e"># Let&#39;s see how many are incorrect</span>
pred_frame_rf[pred_frame_rf[<span style="color:#e6db74">&#39;True Tissue&#39;</span>] <span style="color:#f92672">!=</span> pred_frame_rf[<span style="color:#e6db74">&#39;Predicted Tissue (RF)&#39;</span>]]<span style="color:#f92672">.</span>shape
</code></pre></div><pre><code>(205, 2)
</code></pre>
<p>This means the random forest had a 95.3% accuracy on the test set!</p>
<h3 id="interpretation">Interpretation<a hidden class="anchor" aria-hidden="true" href="#interpretation">#</a></h3>
<p>Hmm, interesting. So for the super hopeful, this result may seem like a surprise. How, or even why, is the random forest superior in this case? There are some things to be said here&hellip;</p>
<ul>
<li>The neural network that we&rsquo;ve implemented is fairly basic. We only have one hidden layer, and for some applications, having more hidden layers can be a good way to increase accuracy. This could be one of those cases.
<ul>
<li>Possible solution: implement more layers.</li>
</ul>
</li>
<li>The choice of the activation function can be influential; we haven&rsquo;t exhaustively tested them here.
<ul>
<li>Possible solution: assess the impact of ReLU vs. Elu vs. Tanh&hellip; etc.</li>
</ul>
</li>
<li>Random forests are remarkably good at detecting which features are most important for class discrimination. This is possible because the random forest does lots and lots of sampling to figure out the most pertinent feature sets.
<ul>
<li>Possible solution: allow the neural network to train for a longer time (epochs) to determine the more relevant features in the dataset.</li>
</ul>
</li>
<li>The data itself could have other non-linear patterns that we have not fully exploited - anywhere from transforming the data (e.g. applying a kernel), to leveraging similar genes, are all options here.</li>
</ul>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>What next? Maybe for another time I&rsquo;ll cover more detailed neural network architectures, and how they can be used for other types of problems. While the genes were not ordered in a particular way, another possibility is to order them in a certain manner that makes the gene expression matrix amenable to other deep learning methods like the convolutional neural network. Or we can even explore using <a href="https://www.nature.com/articles/s42256-019-0037-0">autoencoders to cluster gene expression data</a>.</p>
<p>For now, this is a wrap, and see you next time!</p>
<h2 id="appendix">Appendix<a hidden class="anchor" aria-hidden="true" href="#appendix">#</a></h2>
<h3 id="2-optimisation-of-weights">2. Optimisation of weights<a hidden class="anchor" aria-hidden="true" href="#2-optimisation-of-weights">#</a></h3>
<p>If you&rsquo;re familiar with the mathematical theory behind linear regression, you would know that it&rsquo;s essentially finding a line that minimises the residuals, or <em>errors</em> , of the prediction against your observations. This is done by (long story short) some fancy matrix algebra along the lines of</p>
<p>$$ \beta = (X^TX)^{-1}X^Ty $$</p>
<p>For neural networks, we often define a method to quantify that error in the form of a <em>loss function</em>. Essentially, this loss function states how far the prediction is from the true value that we&rsquo;re trying to predict. There are loss functions for various types of data we want to predict. For instance, if we&rsquo;re predicting what class a data point belongs to (i.e. a classification problem), we can use <em>cross-entropy</em>, or when we are predicting a continuous value (e.g. length of a throw given some conditions), then we can use <em>mean absolute error</em>, etc.</p>
<p>Whatever the loss function, neural networks use a technique called <em>backpropagation</em> which takes the derivative of the loss function <strong>with respect to the weights</strong>. We use the derivative because this tells us how much the value of the loss function changes when we change the weights. Neat, right? These derivatives can then be used by optimisation algorithms like gradient descent.</p>
<p>If you&rsquo;re familiar with calculus notations, we&rsquo;re effectively trying to evaluate</p>
<p>$$ \dfrac{\partial L}{\partial w} $$</p>
<p>which is then used to update the weights,</p>
<p>$$ w \leftarrow w + \alpha \dfrac{\partial L}{\partial w} $$</p>
<p>where $$\alpha$$ represents a &ldquo;learning rate&rdquo; to help convergence. While backpropagation is powerful, it doesn&rsquo;t really carry a biological meaning. The implication here is that somehow we feed things backward to tell an upstream neuron how wrong it was.</p>
<h3 id="3-many-many-different-architectures">3. Many, many different architectures<a hidden class="anchor" aria-hidden="true" href="#3-many-many-different-architectures">#</a></h3>
<p>Why is it that &ldquo;neural networks&rdquo; always seem to headline insane prediction successes? That&rsquo;s because neural networks can be flexibly constructed for different data types. For example,</p>
<ul>
<li>Convolutional neural networks use a 2D-set up of neurons to perceive a wider &ldquo;field&rdquo; of data - this is particularly useful for images.</li>
<li>Recurrent neural networks have a repeated, sequential structure that&rsquo;s suited for speech / text.</li>
<li>Even traditional &ldquo;feed-forward&rdquo; neural networks, like one we&rsquo;ll code today, can use tricks like &ldquo;dropout&rdquo; to ensure that there isn&rsquo;t a huge over-fit to the data.</li>
</ul>
<p>This is a section that deserves a post on its own!</p>

</div>
  <footer class="post-footer">



<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on twitter"
        href="https://twitter.com/intent/tweet/?text=An%20introduction%20to%20Artificial%20Neural%20Networks&amp;url=%2fpost%2f2020-02-26-anns%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2f2020-02-26-anns%2f&amp;title=An%20introduction%20to%20Artificial%20Neural%20Networks&amp;summary=An%20introduction%20to%20Artificial%20Neural%20Networks&amp;source=%2fpost%2f2020-02-26-anns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on reddit"
        href="https://reddit.com/submit?url=%2fpost%2f2020-02-26-anns%2f&title=An%20introduction%20to%20Artificial%20Neural%20Networks">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fpost%2f2020-02-26-anns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on whatsapp"
        href="https://api.whatsapp.com/send?text=An%20introduction%20to%20Artificial%20Neural%20Networks%20-%20%2fpost%2f2020-02-26-anns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An introduction to Artificial Neural Networks on telegram"
        href="https://telegram.me/share/url?text=An%20introduction%20to%20Artificial%20Neural%20Networks&amp;url=%2fpost%2f2020-02-26-anns%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="">Read between the rows</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
