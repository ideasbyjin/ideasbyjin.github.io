<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>A primer to Clustering - Hierarchical clustering | Read between the rows</title>

<meta name="keywords" content="" />
<meta name="description" content="Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to &ldquo;learn&rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images&#39; pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.">
<meta name="author" content="">
<link rel="canonical" href="/post/2019-09-23-clustering-1/" />
<link href="/assets/css/stylesheet.min.ba79064eecb9dc2a7050b36f485dfa34d91c3405027f95064142d0c857cd81ba.css" integrity="sha256-unkGTuy53CpwULNvSF36NNkcNAUCf5UGQULQyFfNgbo=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" href="apple-touch-icon.png">
<link rel="mask-icon" href="safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.80.0" />



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="A primer to Clustering - Hierarchical clustering" />
<meta property="og:description" content="Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to &ldquo;learn&rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images&#39; pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2019-09-23-clustering-1/" />
<meta property="article:published_time" content="2019-09-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-09-23T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A primer to Clustering - Hierarchical clustering"/>
<meta name="twitter:description" content="Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to &ldquo;learn&rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images&#39; pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "A primer to Clustering - Hierarchical clustering",
  "name": "A primer to Clustering - Hierarchical clustering",
  "description": "Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we …",
  "keywords": [
    
  ],
  "articleBody": "Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to “learn” patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images' pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.\nTypically, machine learning methods are used for classification - that is, given some data features, such as image pixels, can we say if a photo contains a cat or a dog? Machine learning can also perform regression; for example, given Steph Curry’s scoring record for the past n games, how many points will he score this coming season? Anyway, for the purposes of this post let’s stick to classification.\nEven within classification, machine learning methods can be classified as:\n Supervised - that is, we tell our algorithm the “true” labels of our images, or Unsupervised - that is, we do not tell our algorithm what the labels are (because sometimes, we don’t know!)  The next series of posts will focus on clustering methods, which are unsupervised methods. This post will be on hierarchical clustering.\nProvisional use case I have data with lots of features, what groups do they belong to?\nExecutive summary  30 seconds: Hierarchical clustering is an unsupervised machine learning method. Users must…  Define a way to quantify the distance between data points (e.g. Euclidean distance), and How those distances are leveraged for grouping data, aka the linkage criterion (e.g. use the average distance or maximum distance between hypothetical clusters)   10 minutes or more: Read down below.  Walkthrough # as always, import all the good stuff import pandas as pd import matplotlib.pyplot as plt import numpy as np from scipy.spatial.distance import pdist # pairwise distances from scipy.cluster.hierarchy import linkage, dendrogram Let’s get the data, do any cleaning necessary beforehand. We’ll do the McDonald’s food menu, courtesy of Kaggle.\ndf = pd.read_csv(\"../../data/mcdonalds-menu/menu.csv\") df.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  What is very nice about this dataset (aside from making people hungry) is that the food categories are pre-defined for us. This gives us a nice validation of our clustering methods later on. There are some things we can see here:\n Some columns are redundant (e.g. Saturated Fat + Saturated Fat % daily value) Calories are effectively redundant (e.g. calories = sum of nutrients; total fat is related to sat. and trans. fat) Some columns are in strings and not numeric (e.g. Serving size) Some columns only have % daily value (e.g. Vitamin A) or don’t have any (e.g. Sugars) Serving sizes of drinks are mixed in with serving size of solid food  For simplicity, let’s…\n  Only use the following columns for clustering:\n Serving size with grams only Saturated Fat, Trans Fat, Cholesterol, Sodium, Carbohydrates, Dietary Fiber, Sugars, Protein Vitamin A (% Daily Value), Vitamin C (% Daily Value), Calcium (% Daily Value), Iron (% Daily Value)    For serving size, use a regex to search out for weight in grams.\n  We’ll keep Category and Food Item to check what our food items are.\n  # Define some regex stuff import re gramSearch = re.compile('(\\d+ g)') integerSearch = re.compile('\\d+') def regex(string, regexCompiler): if not string: return None match = regexCompiler.findall(string) # We could print an error message # if we detect more than 1 or 0 patterns. # This is a bit limited if we have a decimal; e.g. 161.1g - ['161', '1'] # but let's stick to this for now. if not match or len(match)  1: return None else: return match[0] # Not clean, but does the job for a one-time implementation. servingSize = df['Serving Size'].apply(lambda x: regex(x, gramSearch)).copy() servingSize = servingSize.apply(lambda x: regex(x, integerSearch)) df['Serving Size'] = servingSize food = df[~pd.isnull(df['Serving Size'])].copy() food = food[['Category', 'Item', 'Serving Size', 'Saturated Fat', 'Trans Fat', 'Cholesterol', 'Sodium', 'Carbohydrates', 'Dietary Fiber', 'Sugars', 'Protein', 'Vitamin A (% Daily Value)', 'Vitamin C (% Daily Value)', 'Calcium (% Daily Value)', 'Iron (% Daily Value)']] food['Serving Size'] = food['Serving Size'].astype(float) food.index = list(range(food.shape[0])) # reset the index for convenience food_names = dict(zip(food.index, food['Item'])) cat_names = dict(zip(food.index, food['Category'])) How far is an Egg McMuffin from a Sausage McMuffin? In any clustering algorithm, we need to define a distance metric, i.e., how far apart is an Egg McMuffin from a Sausage McMuffin? One of the most common ways to measure the distance between data points is Euclidean distance, also known as the $$l_2$$ norm. For two points a and b from q-dimensional data, the Euclidean distance is\n$$d = \\sqrt{\\sum_{i=1}^{q} (a_i - b_i) }$$\nWe can calculate the Euclidean distance between the food items based on the serving size and nutrition values. I chose Euclidean distance because it is the most common distance metric that’s used, but others exist (e.g. cosine distance, Manhattan distance… etc. Wikipedia is good for this!)\n# Calculate the pairwise distances between items using Euclidean distance food_values = food[food.columns[2:]].values distance_matrix = pdist(food_values, metric = 'euclidean') Just a heads-up that the distance matrix calculated by SciPy is a “condensed” matrix. The distance between two points in Euclidean space is the same whether you measure it $$a \\rightarrow b$$ or $$b \\rightarrow a$$, so we only calculate this in one direction. Furthermore, the distance of a point to itself is 0.\nOnce we have this matrix, then we can apply clustering! Now, we’ll use the scipy implementation because it natively allows us to visualise dendrograms, and it also doesn’t require us to define how many clusters we expect.\nHow do we merge points? - Linkage criteria. From the distance matrix, there are several types of “linkage” criteria we can use. The linkage criteria essentially asks,\n If there are two hypothetical clusters $$c_1$$ and $$c_2$$ of sizes $$n$$ and $$m$$, do we use the minimum, maximum, or average distances between the points in $$c_1$$ and $$c_2$$?\n The cluster with the lowest minimum/maximum/average distance is then merged to the current cluster.\n The Wikipedia page on Complete clustering explains the algorithm well.\n Note that the choice of the distance metric and linkage criteria can affect the results substantially. For this exercise, I will only use Euclidean distance, but will cycle through the different linkage criteria.\nAt first instance, we can apply the average linkage criterion, also known as UPGMA (unweighted pair group method with arithmetic mean).\nUPGMA toy example with distance matrix  Feel free to skip this section if you know how UPGMA works “under the hood”\n For a pairwise distance matrix like this:\n    a b c d e     a 0 17 21 31 23   b 17 0 30 34 21   c 21 30 0 28 39   d 31 34 28 0 43   e 23 21 39 43 0     Merge points a and b as they have the lowest distance among all possible pairs. Compute the distances between the new cluster, (a,b), with respect to c, d, e.   Thus, we have the following distances: $$d_{(a,b)\\rightarrow c}, d_{(a,b)\\rightarrow d}, d_{(a,b)\\rightarrow e}$$\n  $$d_{(a,b)\\rightarrow c} = \\dfrac{\\left(d_{a\\rightarrow c} + d_{b\\rightarrow c}\\right)}{2}$$,\n  $$d_{(a,b)\\rightarrow d} = \\dfrac{\\left(d_{a\\rightarrow d} + d_{b\\rightarrow d}\\right)}{2}$$,\n  etc.\n   This creates a new distance matrix,      a,b c d e     a,b 0 25.5 32.5 22   c 25.5 0 28 39   d 32.5 28 0 43   e 22 39 43 0    Merge e to (a, b) as it has the lowest average distance (22). Compute the distances between the new cluster (a,b,e) to c and d:  Thus, $$d_{(a,b,e)\\rightarrow c} = \\dfrac{\\left(d_{a,b\\rightarrow c} + d_{a,b\\rightarrow c} + d_{e\\rightarrow c}\\right)}{3}$$, and $$d_{(a,b,e)\\rightarrow d} = \\dfrac{\\left(d_{a,b\\rightarrow d} + d_{a,b\\rightarrow d} + d_{e\\rightarrow d}\\right)}{3}$$   Which then leads to…      a,b,e c d     a,b,e 0 30 36   c 30 0 28   d 36 28 0    and repeat!\nUPGMA in code # this is just to make the dendrograms a bit prettier from scipy.cluster import hierarchy hierarchy.set_link_color_palette(['#f58426', '#e8291c', '#ffc0cb']) # UPGMA, or average linkage UPGMA = linkage(distance_matrix, 'average') Done! You’ve now run a UPGMA on the distance matrix. We can see the results of the clustering in a tree-like visualisation called a dendrogram:\ndend_upgma = dendrogram(UPGMA, p = 7, truncate_mode='level') fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \"(\" in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) For the dendrogram, to avoid having too many labels that are too small to read, I’ve only plotted the top 7 levels. Anything in brackets is essentially saying that there are, say, 20 items in that branch.\nThe dendrogram shows how objects are grouped together, and whether there are any singletons. The height of the dendrogram corresponds to the distance between objects.\nWe can already see patterns; for instance, premium salads with chicken group together, while those without chicken go elsewhere. Likewise, the breakfast items are also grouping together. We can run clustering again with a different linkage criteria, such as complete linkage:\nsingle = linkage(distance_matrix, method='single') dend = dendrogram(single, p = 10, truncate_mode='level') fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \"(\" in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) Now with Ward clustering:\nward = linkage(distance_matrix, method='ward') dend = dendrogram(ward, p = 5, truncate_mode='level') fig = plt.gcf() ax = plt.gca() # Plot the dendrogram based on the name of the food item new_ticklabels = [ t.get_text() if \"(\" in t.get_text() else food_names[int(t.get_text())] for t in ax.get_xticklabels() ] ax.set_xticklabels(new_ticklabels, rotation = 90) fig.set_size_inches((8,4)) So while we have some common patterns, it’s clear that different linkage criteria affect the clustering results, leading to the different dendrograms. None of these are necessarily better than the other; they are just alternative ways to cluster your data. In fact, the linkage criterion that you end up choosing should largely depend on your data distribution; see here for an example.\nBonus visualisation As an additional visualisation, we can also apply principal component analysis (PCA) on the data, then colour the points according to their cluster membership.\nfrom scipy.spatial.distance import squareform from sklearn.decomposition import PCA from sklearn.cluster import AgglomerativeClustering pca = PCA(n_components = 2) coords = pca.fit_transform(food_values - food_values.mean()) # remember to scale the data # Let's use the actual categories from McDonald's as a \"true\" set of labels categories = food['Category'] cat_to_int = dict([ (label,i) for i, label in enumerate(set(categories)) ]) # Here, we can choose an arbitrary number of clusters to visualise. # Since we have the categories from McDonald's, we can use this for # visualisation purposes. num_clust = len(set(categories)) # Let's use Ward clustering, because, why not. ac = AgglomerativeClustering(n_clusters = num_clust, linkage= 'ward') ac.fit(food[food.columns[2:]].values) # map labels to colour palette - choose something that's discrete to allow easier colour disambiguation. cm = plt.get_cmap(\"Set1\") # Here, we first get a numpy array of evenly-spaced values between 0 and 1 # Then we map each float to an RGB value using the ColorMap above. # The RGBA values are then mapped to integers, as sklearn's labels are integers. # i.e. this will look like  # {0: (r,g,b,a), 1: (r1, g1, b1, a1)... etc.} mapped = dict([ (i, cm(v)) for i,v in enumerate(np.arange(0, 1.01, 1./num_clust)) ]) # plot the PCA and colour with our predicted cluster, and the categories from the McDonald's menu as a comparison fig, ax = plt.subplots(1,2) ax[0].scatter(coords[:,0], coords[:,1], color = [ mapped[label] for label in ac.labels_ ]) ax[1].scatter(coords[:,0], coords[:,1], color = [ mapped[cat_to_int[name]] for name in food['Category'] ]) fig.set_size_inches((10,5)) ax[0].set_title(\"Predicted categories\") ax[1].set_title(\"True categories\") Interpreting this plot may be difficult, but essentially, it would be ideal to have the same set of points in both the left-hand and right-hand plots to share a colour. In other words, the points that are red in the left-hand plot don’t necessarily have to be red in the right-hand plot per se. However, those same points should hopefully share one colour, whether it’s blue, pink, or whatever.\nWe see that this isn’t the case - so what then? This means that we’d have to do a more careful look into what categories the food items belong to, and question more carefully on what the “true” categories are here. Essentially, the true categories are only designating whether a food item is something you have for breakfast, or it contains fish, etc. However, we’ve clustered the data on the basis of their nutrition. In hindsight, what we used for clustering does not necessarily align with the true known information about a food item’s category. In other words, nutrition profiles aren’t exactly related to an item being a “breakfast” item.\nEarlier, we saw in the dendrograms that individually similar food items did cluster together (e.g. the different flavours of salads), so we know that there is some information of use here. However, grouping food items into larger categories may not be as intuitive.\nNonetheless, I hope this was a useful session on what clustering can offer you.\n",
  "wordCount" : "2185",
  "inLanguage": "en",
  "datePublished": "2019-09-23T00:00:00Z",
  "dateModified": "2019-09-23T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/2019-09-23-clustering-1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Read between the rows",
    "logo": {
      "@type": "ImageObject",
      "url": "favicon.ico"
    }
  }
}
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="" accesskey="h" title="Read between the rows (Alt + H)">Read between the rows</a>
            <span class="logo-switches">
                <a id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </a>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      A primer to Clustering - Hierarchical clustering
    </h1>
    <div class="post-meta">September 23, 2019&nbsp;·&nbsp;11 min

</div>
  </header> 

  <div class="toc">
    <details >
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#context" aria-label="Context">Context</a></li><li>
        <a href="#provisional-use-case" aria-label="Provisional use case">Provisional use case</a></li><li>
        <a href="#executive-summary" aria-label="Executive summary">Executive summary</a></li><li>
        <a href="#walkthrough" aria-label="Walkthrough">Walkthrough</a></li><li>
        <a href="#how-far-is-an-egg-mcmuffin-from-a-sausage-mcmuffin" aria-label="How far is an Egg McMuffin from a Sausage McMuffin?">How far is an Egg McMuffin from a Sausage McMuffin?</a></li><li>
        <a href="#how-do-we-merge-points---linkage-criteria" aria-label="How do we merge points? - Linkage criteria.">How do we merge points? - Linkage criteria.</a></li><li>
        <a href="#upgma-toy-example-with-distance-matrix" aria-label="UPGMA toy example with distance matrix">UPGMA toy example with distance matrix</a></li><li>
        <a href="#upgma-in-code" aria-label="UPGMA in code">UPGMA in code</a></li><li>
        <a href="#bonus-visualisation" aria-label="Bonus visualisation">Bonus visualisation</a></li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<h3 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h3>
<p>From the <a href="../17/pca.html">last blog post</a>, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to &ldquo;learn&rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images' pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.</p>
<p>Typically, machine learning methods are used for <em>classification</em> - that is, given some data features, such as image pixels, can we say if a photo contains a cat or a dog? Machine learning can also perform <em>regression</em>; for example, given Steph Curry&rsquo;s scoring record for the past <em>n</em> games, how many points will he score this coming season? Anyway, for the purposes of this post let&rsquo;s stick to classification.</p>
<p>Even within classification, machine learning methods can be classified as:</p>
<ul>
<li><em>Supervised</em> - that is, we tell our algorithm the &ldquo;true&rdquo; labels of our images, or</li>
<li><em>Unsupervised</em> - that is, we do not tell our algorithm what the labels are (because sometimes, we don&rsquo;t know!)</li>
</ul>
<p>The next series of posts will focus on <strong>clustering methods</strong>, which are <em>unsupervised</em> methods. This post will be on <strong>hierarchical clustering</strong>.</p>
<h3 id="provisional-use-case">Provisional use case<a hidden class="anchor" aria-hidden="true" href="#provisional-use-case">#</a></h3>
<p>I have data with lots of features, what groups do they belong to?</p>
<h3 id="executive-summary">Executive summary<a hidden class="anchor" aria-hidden="true" href="#executive-summary">#</a></h3>
<ul>
<li><strong>30 seconds</strong>: Hierarchical clustering is an unsupervised machine learning method. Users must&hellip;
<ul>
<li>Define a way to quantify the distance between data points (e.g. Euclidean distance), and</li>
<li>How those distances are leveraged for grouping data, aka the linkage criterion (e.g. use the average distance or maximum distance between hypothetical clusters)</li>
</ul>
</li>
<li><strong>10 minutes or more</strong>: Read down below.</li>
</ul>
<h3 id="walkthrough">Walkthrough<a hidden class="anchor" aria-hidden="true" href="#walkthrough">#</a></h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># as always, import all the good stuff</span>
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> scipy.spatial.distance <span style="color:#f92672">import</span> pdist     <span style="color:#75715e"># pairwise distances</span>
<span style="color:#f92672">from</span> scipy.cluster.hierarchy <span style="color:#f92672">import</span> linkage, dendrogram
</code></pre></div><p>Let&rsquo;s get the data, do any cleaning necessary beforehand. We&rsquo;ll do the McDonald&rsquo;s food menu, courtesy of Kaggle.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;../../data/mcdonalds-menu/menu.csv&#34;</span>)
df<span style="color:#f92672">.</span>head()
</code></pre></div><!-- raw HTML omitted -->
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>What is very nice about this dataset (aside from making people hungry) is that the food categories are pre-defined for us. This gives us a nice validation of our clustering methods later on. There are some things we can see here:</p>
<ul>
<li>Some columns are redundant (e.g. Saturated Fat + Saturated Fat % daily value)</li>
<li>Calories are effectively redundant (e.g. calories = sum of nutrients; total fat is related to sat. and trans. fat)</li>
<li>Some columns are in strings and not numeric (e.g. Serving size)</li>
<li>Some columns <em>only</em> have % daily value (e.g. Vitamin A) or don&rsquo;t have any (e.g. Sugars)</li>
<li>Serving sizes of drinks are mixed in with serving size of solid food</li>
</ul>
<p>For simplicity, let&rsquo;s&hellip;</p>
<ul>
<li>
<p>Only use the following columns for clustering:</p>
<ul>
<li>Serving size with grams only</li>
<li>Saturated Fat, Trans Fat, Cholesterol, Sodium, Carbohydrates, Dietary Fiber, Sugars, Protein</li>
<li>Vitamin A (% Daily Value), Vitamin C (% Daily Value), Calcium (% Daily Value), Iron (% Daily Value)</li>
</ul>
</li>
<li>
<p>For serving size, use a regex to search out for weight in grams.</p>
</li>
<li>
<p>We&rsquo;ll keep Category and Food Item to check what our food items are.</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define some regex stuff</span>
<span style="color:#f92672">import</span> re
gramSearch <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;(\d+ g)&#39;</span>)
integerSearch <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">&#39;\d+&#39;</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">regex</span>(string, regexCompiler):
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> string:
        <span style="color:#66d9ef">return</span> None
    
    match <span style="color:#f92672">=</span> regexCompiler<span style="color:#f92672">.</span>findall(string)
    
    <span style="color:#75715e"># We could print an error message</span>
    <span style="color:#75715e"># if we detect more than 1 or 0 patterns.</span>
    <span style="color:#75715e"># This is a bit limited if we have a decimal; e.g. 161.1g -&gt; [&#39;161&#39;, &#39;1&#39;]</span>
    <span style="color:#75715e"># but let&#39;s stick to this for now.</span>
    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> match <span style="color:#f92672">or</span> len(match) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
        <span style="color:#66d9ef">return</span> None
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> match[<span style="color:#ae81ff">0</span>]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Not clean, but does the job for a one-time implementation.</span>
servingSize <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;Serving Size&#39;</span>]<span style="color:#f92672">.</span>apply(<span style="color:#66d9ef">lambda</span> x: regex(x, gramSearch))<span style="color:#f92672">.</span>copy()
servingSize <span style="color:#f92672">=</span> servingSize<span style="color:#f92672">.</span>apply(<span style="color:#66d9ef">lambda</span> x: regex(x, integerSearch))
df[<span style="color:#e6db74">&#39;Serving Size&#39;</span>] <span style="color:#f92672">=</span> servingSize
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">food <span style="color:#f92672">=</span> df[<span style="color:#f92672">~</span>pd<span style="color:#f92672">.</span>isnull(df[<span style="color:#e6db74">&#39;Serving Size&#39;</span>])]<span style="color:#f92672">.</span>copy()
food <span style="color:#f92672">=</span> food[[<span style="color:#e6db74">&#39;Category&#39;</span>, <span style="color:#e6db74">&#39;Item&#39;</span>, 
       <span style="color:#e6db74">&#39;Serving Size&#39;</span>, <span style="color:#e6db74">&#39;Saturated Fat&#39;</span>, <span style="color:#e6db74">&#39;Trans Fat&#39;</span>, <span style="color:#e6db74">&#39;Cholesterol&#39;</span>, <span style="color:#e6db74">&#39;Sodium&#39;</span>,
       <span style="color:#e6db74">&#39;Carbohydrates&#39;</span>, <span style="color:#e6db74">&#39;Dietary Fiber&#39;</span>, <span style="color:#e6db74">&#39;Sugars&#39;</span>, <span style="color:#e6db74">&#39;Protein&#39;</span>,
       <span style="color:#e6db74">&#39;Vitamin A (% Daily Value)&#39;</span>, <span style="color:#e6db74">&#39;Vitamin C (% Daily Value)&#39;</span>,
       <span style="color:#e6db74">&#39;Calcium (% Daily Value)&#39;</span>, <span style="color:#e6db74">&#39;Iron (% Daily Value)&#39;</span>]]

food[<span style="color:#e6db74">&#39;Serving Size&#39;</span>] <span style="color:#f92672">=</span> food[<span style="color:#e6db74">&#39;Serving Size&#39;</span>]<span style="color:#f92672">.</span>astype(float)
food<span style="color:#f92672">.</span>index <span style="color:#f92672">=</span> list(range(food<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])) <span style="color:#75715e"># reset the index for convenience</span>

food_names <span style="color:#f92672">=</span> dict(zip(food<span style="color:#f92672">.</span>index, food[<span style="color:#e6db74">&#39;Item&#39;</span>]))
cat_names <span style="color:#f92672">=</span> dict(zip(food<span style="color:#f92672">.</span>index, food[<span style="color:#e6db74">&#39;Category&#39;</span>]))
</code></pre></div><h3 id="how-far-is-an-egg-mcmuffin-from-a-sausage-mcmuffin">How far is an Egg McMuffin from a Sausage McMuffin?<a hidden class="anchor" aria-hidden="true" href="#how-far-is-an-egg-mcmuffin-from-a-sausage-mcmuffin">#</a></h3>
<p>In any clustering algorithm, we need to define a distance metric, i.e., how far apart is an <code>Egg McMuffin</code> from a <code>Sausage McMuffin</code>? One of the most common ways to measure the distance between data points is Euclidean distance, also known as the $$l_2$$ norm. For two points <em>a</em> and <em>b</em> from <em>q</em>-dimensional data, the Euclidean distance is</p>
<p>$$d = \sqrt{\sum_{i=1}^{q} (a_i - b_i) }$$</p>
<p>We can calculate the Euclidean distance between the food items based on the serving size and nutrition values. I chose Euclidean distance because it is the most common distance metric that&rsquo;s used, but others exist (e.g. cosine distance, Manhattan distance&hellip; etc. Wikipedia is good for this!)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Calculate the pairwise distances between items using Euclidean distance</span>
food_values <span style="color:#f92672">=</span> food[food<span style="color:#f92672">.</span>columns[<span style="color:#ae81ff">2</span>:]]<span style="color:#f92672">.</span>values 
distance_matrix <span style="color:#f92672">=</span> pdist(food_values, metric <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;euclidean&#39;</span>)
</code></pre></div><p>Just a heads-up that the distance matrix calculated by SciPy is a &ldquo;condensed&rdquo; matrix. The distance between two points in Euclidean space is the same whether you measure it $$a \rightarrow b$$ or $$b \rightarrow a$$, so we only calculate this in one direction. Furthermore, the distance of a point to itself is 0.</p>
<p>Once we have this matrix, then we can apply clustering! Now, we&rsquo;ll use the <code>scipy</code> implementation because it natively allows us to visualise dendrograms, and it also doesn&rsquo;t require us to define how many clusters we expect.</p>
<h3 id="how-do-we-merge-points---linkage-criteria">How do we merge points? - Linkage criteria.<a hidden class="anchor" aria-hidden="true" href="#how-do-we-merge-points---linkage-criteria">#</a></h3>
<p>From the distance matrix, there are several types of &ldquo;linkage&rdquo; criteria we can use.
The linkage criteria essentially asks,</p>
<blockquote>
<p>If there are two hypothetical clusters $$c_1$$ and $$c_2$$ of sizes $$n$$ and $$m$$,
do we use the minimum, maximum, or average distances between the points in $$c_1$$ and $$c_2$$?</p>
</blockquote>
<p>The cluster with the <strong>lowest</strong> minimum/maximum/average distance is then merged to the current cluster.</p>
<blockquote>
<p>The Wikipedia page on <a href="https://en.wikipedia.org/wiki/Complete-linkage_clustering#First_step">Complete clustering</a> explains the algorithm well.</p>
</blockquote>
<p>Note that the choice of the distance metric and linkage criteria can affect the results substantially. For this exercise, I will only use Euclidean distance, but will cycle through the different linkage criteria.</p>
<p>At first instance, we can apply the average linkage criterion, also known as UPGMA (unweighted pair group method with
arithmetic mean).</p>
<h3 id="upgma-toy-example-with-distance-matrix">UPGMA toy example with distance matrix<a hidden class="anchor" aria-hidden="true" href="#upgma-toy-example-with-distance-matrix">#</a></h3>
<blockquote>
<p>Feel free to skip this section if you know how UPGMA works &ldquo;under the hood&rdquo;</p>
</blockquote>
<p>For a pairwise distance matrix like this:</p>
<table>
<thead>
<tr>
<th></th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>
<tbody>
<tr>
<td>a</td>
<td>0</td>
<td>17</td>
<td>21</td>
<td>31</td>
<td>23</td>
</tr>
<tr>
<td>b</td>
<td>17</td>
<td>0</td>
<td>30</td>
<td>34</td>
<td>21</td>
</tr>
<tr>
<td>c</td>
<td>21</td>
<td>30</td>
<td>0</td>
<td>28</td>
<td>39</td>
</tr>
<tr>
<td>d</td>
<td>31</td>
<td>34</td>
<td>28</td>
<td>0</td>
<td>43</td>
</tr>
<tr>
<td>e</td>
<td>23</td>
<td>21</td>
<td>39</td>
<td>43</td>
<td>0</td>
</tr>
</tbody>
</table>
<ol>
<li>Merge points <em>a</em> and <em>b</em> as they have the lowest distance among all possible pairs.</li>
<li>Compute the distances between the new cluster, (<em>a</em>,<em>b</em>), with respect to <em>c</em>, <em>d</em>, <em>e</em>.
<ul>
<li>
<p>Thus, we have the following distances: $$d_{(a,b)\rightarrow c}, d_{(a,b)\rightarrow d}, d_{(a,b)\rightarrow e}$$</p>
</li>
<li>
<p>$$d_{(a,b)\rightarrow c} = \dfrac{\left(d_{a\rightarrow c} + d_{b\rightarrow c}\right)}{2}$$,</p>
</li>
<li>
<p>$$d_{(a,b)\rightarrow d} = \dfrac{\left(d_{a\rightarrow d} + d_{b\rightarrow d}\right)}{2}$$,</p>
</li>
<li>
<p>etc.</p>
</li>
</ul>
</li>
<li>This creates a new distance matrix,</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>a,b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>
<tbody>
<tr>
<td>a,b</td>
<td>0</td>
<td>25.5</td>
<td>32.5</td>
<td>22</td>
</tr>
<tr>
<td>c</td>
<td>25.5</td>
<td>0</td>
<td>28</td>
<td>39</td>
</tr>
<tr>
<td>d</td>
<td>32.5</td>
<td>28</td>
<td>0</td>
<td>43</td>
</tr>
<tr>
<td>e</td>
<td>22</td>
<td>39</td>
<td>43</td>
<td>0</td>
</tr>
</tbody>
</table>
<ol start="4">
<li>Merge <em>e</em> to (<em>a</em>, <em>b</em>) as it has the lowest average distance (22).</li>
<li>Compute the distances between the new cluster (<em>a</em>,<em>b</em>,<em>e</em>) to <em>c</em> and <em>d</em>:
<ul>
<li>Thus, $$d_{(a,b,e)\rightarrow c} = \dfrac{\left(d_{a,b\rightarrow c} + d_{a,b\rightarrow c} + d_{e\rightarrow c}\right)}{3}$$,</li>
<li>and $$d_{(a,b,e)\rightarrow d} = \dfrac{\left(d_{a,b\rightarrow d} + d_{a,b\rightarrow d} + d_{e\rightarrow d}\right)}{3}$$</li>
</ul>
</li>
<li>Which then leads to&hellip;</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>a,b,e</th>
<th>c</th>
<th>d</th>
</tr>
</thead>
<tbody>
<tr>
<td>a,b,e</td>
<td>0</td>
<td>30</td>
<td>36</td>
</tr>
<tr>
<td>c</td>
<td>30</td>
<td>0</td>
<td>28</td>
</tr>
<tr>
<td>d</td>
<td>36</td>
<td>28</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>and repeat!</p>
<h3 id="upgma-in-code">UPGMA in code<a hidden class="anchor" aria-hidden="true" href="#upgma-in-code">#</a></h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># this is just to make the dendrograms a bit prettier</span>
<span style="color:#f92672">from</span> scipy.cluster <span style="color:#f92672">import</span> hierarchy
hierarchy<span style="color:#f92672">.</span>set_link_color_palette([<span style="color:#e6db74">&#39;#f58426&#39;</span>, <span style="color:#e6db74">&#39;#e8291c&#39;</span>, <span style="color:#e6db74">&#39;#ffc0cb&#39;</span>])

<span style="color:#75715e"># UPGMA, or average linkage</span>
UPGMA <span style="color:#f92672">=</span> linkage(distance_matrix, <span style="color:#e6db74">&#39;average&#39;</span>)
</code></pre></div><p>Done! You&rsquo;ve now run a UPGMA on the distance matrix. We can see the results of the clustering in a tree-like visualisation called a dendrogram:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dend_upgma <span style="color:#f92672">=</span> dendrogram(UPGMA, p <span style="color:#f92672">=</span> <span style="color:#ae81ff">7</span>, truncate_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;level&#39;</span>)
fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gcf()
ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()

<span style="color:#75715e"># Plot the dendrogram based on the name of the food item</span>
new_ticklabels <span style="color:#f92672">=</span> [ t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;(&#34;</span> <span style="color:#f92672">in</span> t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">else</span> food_names[int(t<span style="color:#f92672">.</span>get_text())] <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ax<span style="color:#f92672">.</span>get_xticklabels() ]
ax<span style="color:#f92672">.</span>set_xticklabels(new_ticklabels, rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><p><img src="/assets/notebooks/hclust/output_18_0.png" alt="png"></p>
<p>For the dendrogram, to avoid having too many labels that are too small to read, I&rsquo;ve only plotted the top 7 levels. Anything in brackets is essentially saying that there are, say, 20 items in that branch.</p>
<p>The dendrogram shows how objects are grouped together, and whether there are any singletons. The height of the dendrogram corresponds to the distance between objects.</p>
<p>We can already see patterns; for instance, premium salads with chicken group together, while those without chicken go elsewhere. Likewise, the breakfast items are also grouping together. We can run clustering again with a different linkage criteria, such as complete linkage:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">single <span style="color:#f92672">=</span> linkage(distance_matrix, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;single&#39;</span>)
dend <span style="color:#f92672">=</span> dendrogram(single, p <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>, truncate_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;level&#39;</span>)
fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gcf()
ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()

<span style="color:#75715e"># Plot the dendrogram based on the name of the food item</span>
new_ticklabels <span style="color:#f92672">=</span> [ t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;(&#34;</span> <span style="color:#f92672">in</span> t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">else</span> food_names[int(t<span style="color:#f92672">.</span>get_text())] <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ax<span style="color:#f92672">.</span>get_xticklabels() ]
ax<span style="color:#f92672">.</span>set_xticklabels(new_ticklabels, rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><p><img src="/assets/notebooks/hclust/output_20_0.png" alt="png"></p>
<p>Now with Ward clustering:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">ward <span style="color:#f92672">=</span> linkage(distance_matrix, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ward&#39;</span>)
dend <span style="color:#f92672">=</span> dendrogram(ward, p <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>, truncate_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;level&#39;</span>)
fig <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gcf()
ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()

<span style="color:#75715e"># Plot the dendrogram based on the name of the food item</span>
new_ticklabels <span style="color:#f92672">=</span> [ t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;(&#34;</span> <span style="color:#f92672">in</span> t<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">else</span> food_names[int(t<span style="color:#f92672">.</span>get_text())] <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> ax<span style="color:#f92672">.</span>get_xticklabels() ]
ax<span style="color:#f92672">.</span>set_xticklabels(new_ticklabels, rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">4</span>))
</code></pre></div><p><img src="/assets/notebooks/hclust/output_22_0.png" alt="png"></p>
<p>So while we have some common patterns, it&rsquo;s clear that different linkage criteria affect the clustering results, leading to the different dendrograms. None of these are necessarily better than the other; they are just alternative ways to cluster your data. In fact, the linkage criterion that you end up choosing should largely depend on your data distribution; see <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html">here</a> for an example.</p>
<h3 id="bonus-visualisation">Bonus visualisation<a hidden class="anchor" aria-hidden="true" href="#bonus-visualisation">#</a></h3>
<p>As an additional visualisation, we can also apply principal component analysis (PCA) on the data, then colour the points according to their cluster membership.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scipy.spatial.distance <span style="color:#f92672">import</span> squareform
<span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
<span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> AgglomerativeClustering

pca <span style="color:#f92672">=</span> PCA(n_components <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
coords <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(food_values <span style="color:#f92672">-</span> food_values<span style="color:#f92672">.</span>mean()) <span style="color:#75715e"># remember to scale the data</span>

<span style="color:#75715e"># Let&#39;s use the actual categories from McDonald&#39;s as a &#34;true&#34; set of labels</span>
categories <span style="color:#f92672">=</span> food[<span style="color:#e6db74">&#39;Category&#39;</span>]
cat_to_int <span style="color:#f92672">=</span> dict([ (label,i) <span style="color:#66d9ef">for</span> i, label <span style="color:#f92672">in</span> enumerate(set(categories)) ])

<span style="color:#75715e"># Here, we can choose an arbitrary number of clusters to visualise.</span>
<span style="color:#75715e"># Since we have the categories from McDonald&#39;s, we can use this for</span>
<span style="color:#75715e"># visualisation purposes.</span>
num_clust <span style="color:#f92672">=</span> len(set(categories))

<span style="color:#75715e"># Let&#39;s use Ward clustering, because, why not.</span>
ac <span style="color:#f92672">=</span> AgglomerativeClustering(n_clusters <span style="color:#f92672">=</span> num_clust, linkage<span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;ward&#39;</span>)
ac<span style="color:#f92672">.</span>fit(food[food<span style="color:#f92672">.</span>columns[<span style="color:#ae81ff">2</span>:]]<span style="color:#f92672">.</span>values)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># map labels to colour palette - choose something that&#39;s discrete to allow easier colour disambiguation.</span>
cm <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>get_cmap(<span style="color:#e6db74">&#34;Set1&#34;</span>)

<span style="color:#75715e"># Here, we first get a numpy array of evenly-spaced values between 0 and 1</span>
<span style="color:#75715e"># Then we map each float to an RGB value using the ColorMap above.</span>
<span style="color:#75715e"># The RGBA values are then mapped to integers, as sklearn&#39;s labels are integers.</span>
<span style="color:#75715e"># i.e. this will look like </span>
<span style="color:#75715e"># {0: (r,g,b,a), 1: (r1, g1, b1, a1)... etc.}</span>

mapped <span style="color:#f92672">=</span> dict([ (i, cm(v)) <span style="color:#66d9ef">for</span> i,v <span style="color:#f92672">in</span> enumerate(np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span>num_clust)) ])


<span style="color:#75715e"># plot the PCA and colour with our predicted cluster, and the categories from the McDonald&#39;s menu as a comparison</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>scatter(coords[:,<span style="color:#ae81ff">0</span>], coords[:,<span style="color:#ae81ff">1</span>],
           color <span style="color:#f92672">=</span> [ mapped[label] <span style="color:#66d9ef">for</span> label <span style="color:#f92672">in</span> ac<span style="color:#f92672">.</span>labels_ ])

ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>scatter(coords[:,<span style="color:#ae81ff">0</span>], coords[:,<span style="color:#ae81ff">1</span>],
           color <span style="color:#f92672">=</span> [ mapped[cat_to_int[name]] <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> food[<span style="color:#e6db74">&#39;Category&#39;</span>] ])

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">5</span>))
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Predicted categories&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;True categories&#34;</span>)
</code></pre></div><p><img src="/assets/notebooks/hclust/output_26_1.png" alt="png"></p>
<p>Interpreting this plot may be difficult, but essentially, it would be ideal to have the same set of points in both the left-hand and right-hand plots to share a colour. In other words, the points that are red in the left-hand plot don&rsquo;t necessarily have to be red in the right-hand plot <em>per se</em>.
However, those same points should hopefully share one colour, whether it&rsquo;s blue, pink, or whatever.</p>
<p>We see that this isn&rsquo;t the case - so what then? This means that we&rsquo;d have to do a more careful look into what categories the food items belong to, and question more carefully on what the &ldquo;true&rdquo; categories are here.
Essentially, the true categories are only designating whether a food item is something you have for breakfast, or it contains fish, etc.
However, we&rsquo;ve clustered the data on the basis of their nutrition. In hindsight, what we used for clustering does not necessarily align
with the true known information about a food item&rsquo;s category. In other words, nutrition profiles aren&rsquo;t exactly related to
an item being a &ldquo;breakfast&rdquo; item.</p>
<p>Earlier, we saw in the dendrograms that individually similar food items <em>did</em> cluster together (e.g. the different flavours of salads), so we know that there is some information of use here. However, grouping food items into larger categories may not be as intuitive.</p>
<p>Nonetheless, I hope this was a useful session on what clustering can offer you.</p>

</div>
  <footer class="post-footer">



<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on twitter"
        href="https://twitter.com/intent/tweet/?text=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering&amp;url=%2fpost%2f2019-09-23-clustering-1%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2f2019-09-23-clustering-1%2f&amp;title=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering&amp;summary=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering&amp;source=%2fpost%2f2019-09-23-clustering-1%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on reddit"
        href="https://reddit.com/submit?url=%2fpost%2f2019-09-23-clustering-1%2f&title=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on facebook"
        href="https://facebook.com/sharer/sharer.php?u=%2fpost%2f2019-09-23-clustering-1%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on whatsapp"
        href="https://api.whatsapp.com/send?text=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering%20-%20%2fpost%2f2019-09-23-clustering-1%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share A primer to Clustering - Hierarchical clustering on telegram"
        href="https://telegram.me/share/url?text=A%20primer%20to%20Clustering%20-%20Hierarchical%20clustering&amp;url=%2fpost%2f2019-09-23-clustering-1%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="">Read between the rows</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
