<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stats on Read between the rows</title>
    <link>/categories/stats/</link>
    <description>Recent content in stats on Read between the rows</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Jan 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/stats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Numba in action!</title>
      <link>/post/2021-01-30-numba/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-01-30-numba/</guid>
      <description>Accelerate your Numpy code with numba</description>
    </item>
    
    <item>
      <title>My favourite Pandas tricks</title>
      <link>/post/2021-01-26-pandas/</link>
      <pubDate>Tue, 26 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-01-26-pandas/</guid>
      <description>Pandas tips &amp;amp; tricks</description>
    </item>
    
    <item>
      <title>Example applications of probability distributions</title>
      <link>/post/2020-04-14-distributions-applications/</link>
      <pubDate>Tue, 14 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-04-14-distributions-applications/</guid>
      <description>This post shows how we can use a PMF and PDF to some toy problems.</description>
    </item>
    
    <item>
      <title>Distributions Cheat sheet</title>
      <link>/post/2020-04-11-distributions/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-04-11-distributions/</guid>
      <description>A cheat sheet for probability distributions.</description>
    </item>
    
    <item>
      <title>An introduction to Artificial Neural Networks</title>
      <link>/post/2020-02-26-anns/</link>
      <pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/2020-02-26-anns/</guid>
      <description>This is my intro to neural networks.</description>
    </item>
    
    <item>
      <title>Santa Bayes: a Christmas introduction to Bayesian inference</title>
      <link>/post/2019-12-25-santa-bayes/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-12-25-santa-bayes/</guid>
      <description>It&amp;rsquo;s Christmas, and Santa is here! He&amp;rsquo;s got his list and he&amp;rsquo;s about to see who&amp;rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.
Santa is in trouble. Without his list, it&amp;rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who&amp;rsquo;s been nice or naughty.</description>
    </item>
    
    <item>
      <title>Clustering Gene Expression Data using DBSCAN</title>
      <link>/post/2019-12-18-clustering-2/</link>
      <pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-12-18-clustering-2/</guid>
      <description>In a previous post, I covered arguably one of the most straight-forward clustering algorithms: hierarchical clustering. Remember that any clustering method requires a distance metric to quantify how &amp;ldquo;far apart&amp;rdquo; two points are placed in some N-dimensional space. While typically Euclidean, there&amp;rsquo;s loads of ways in doing this.
Generally, hierarchical clustering is a very good way of clustering your data, though it suffers from a couple of limitations:
 Users have to define the number of clusters The linkage criterion (UPGMA, Ward&amp;hellip;) can have a huge effect on the cluster shapes  Other clustering methods like K-means clustering also depend on the number of clusters to be determined beforehand, and it can be prone to hitting local minima.</description>
    </item>
    
    <item>
      <title>Supervised learning demo: what position do I play?</title>
      <link>/post/2019-11-01-supervised/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-11-01-supervised/</guid>
      <description>Last time I covered a section on clustering, a group of unsupervised learning methods â€“ so called because they are not given the class memberships of the data$$^\dagger$$. Don&amp;rsquo;t worry, I will do more posts on clustering soon. For now I wanted to give a quick overview of what supervised methods look like. For that, let&amp;rsquo;s look at the statistics of hockey players!
$$\dagger$$: this is a gross generalisation. More formally, for some dataset $$\mathbf{X}$$, if we are trying to predict an output variable $$\mathbf{Y}$$, we use supervised learning methods, otherwise unsupervised learning methods.</description>
    </item>
    
    <item>
      <title>A primer to Clustering - Hierarchical clustering</title>
      <link>/post/2019-09-23-clustering-1/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-23-clustering-1/</guid>
      <description>Context From the last blog post, we saw that data can come with many features. When data gets very complex (at least, more complex than the Starbucks data from the last post), we can rely on machine learning methods to &amp;ldquo;learn&amp;rdquo; patterns in the data. For example, suppose you have 1000 photos, of which 500 are cats, and the other 500 are dogs. Machine learning methods can, for instance, read the RGB channels of the images&amp;rsquo; pixels, then use that information to distinguish which combinations of pixels are associated with cat images, and which combinations are linked to dogs.</description>
    </item>
    
    <item>
      <title>Principal component analysis of Starbucks Nutrition data</title>
      <link>/post/2019-09-17-pca/</link>
      <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-09-17-pca/</guid>
      <description>Data is everywhere. Whether it&amp;rsquo;s political survey data, the DNA sequences of wacky organisms, nutritional profiles of our favourite foods, you name it. Data comes in various shapes and sizes, too - it can be several thousand samples with only a few features, or only a small number of examples with tons of features. For either case, and anything else in between, finding a lower-dimensional (i.e. fewer features) representation of our data is useful; however, how do we choose which features to use for capturing the essence of our data?</description>
    </item>
    
  </channel>
</rss>