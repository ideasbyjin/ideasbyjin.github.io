<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Santa Bayes: a Christmas introduction to Bayesian inference | Read between the rows</title>

<meta name="keywords" content="" />
<meta name="description" content="It&rsquo;s Christmas, and Santa is here! He&rsquo;s got his list and he&rsquo;s about to see who&rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.
Santa is in trouble. Without his list, it&rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who&rsquo;s been nice or naughty.">
<meta name="author" content="">
<link rel="canonical" href="https://ideasbyjin.github.io/post/2019-12-25-santa-bayes/" />
<link href="/assets/css/stylesheet.min.54720d48c4fa0c4ebe8555f04b9fc5b856112ec49cafef19cb385e89661150b7.css" integrity="sha256-VHINSMT6DE6&#43;hVXwS5/FuFYRLsScr&#43;8ZyzheiWYRULc=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://ideasbyjin.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ideasbyjin.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ideasbyjin.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ideasbyjin.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ideasbyjin.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.80.0" />



<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta property="og:title" content="Santa Bayes: a Christmas introduction to Bayesian inference" />
<meta property="og:description" content="It&rsquo;s Christmas, and Santa is here! He&rsquo;s got his list and he&rsquo;s about to see who&rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.
Santa is in trouble. Without his list, it&rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who&rsquo;s been nice or naughty." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ideasbyjin.github.io/post/2019-12-25-santa-bayes/" />
<meta property="article:published_time" content="2019-12-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-12-25T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Santa Bayes: a Christmas introduction to Bayesian inference"/>
<meta name="twitter:description" content="It&rsquo;s Christmas, and Santa is here! He&rsquo;s got his list and he&rsquo;s about to see who&rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.
Santa is in trouble. Without his list, it&rsquo;s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who&rsquo;s been nice or naughty."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Santa Bayes: a Christmas introduction to Bayesian inference",
  "name": "Santa Bayes: a Christmas introduction to Bayesian inference",
  "description": "It\u0026amp;rsquo;s Christmas, and Santa is here! He\u0026amp;rsquo;s got his list and he\u0026amp;rsquo;s about to see who\u0026amp;rsquo;s been naughty or nice this year. While on his way out of the North Pole, he …",
  "keywords": [
    
  ],
  "articleBody": "It’s Christmas, and Santa is here! He’s got his list and he’s about to see who’s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.\nSanta is in trouble. Without his list, it’s going to take ages to visit every house in every neighbourhood, then go down the chimney to see who’s been nice or naughty. However, Santa reasons that if a neighbourhood is generally full of bad kids, he can skip them, for now, and give gifts elsewhere.\nIn this alternate universe, Santa is, luckily, a Bayesian statistican. Santa decides he is going to use Bayesian inference to guess the number of naughty kids.\nIt’s going to be a long night, but one that’s hopefully salvaged by Bayesian methods!\nIf you have…\n 30 seconds: Bayesian inference is driven by this equation:  $$P(B\\vert A) = \\dfrac{P(A\\vert B)P(B)}{P(A)}$$\nUnlike frequentist methods that try to make point estimates of parameters, Bayesian methods are driven by estimating distributions of parameters.\n 15 minutes: It’s a long one, but hopefully it’ll be worth it. This post will discuss the basics of Bayesian inference, and how we can use the pymc3 library for statistical computing.  NB: I would not consider myself an expert in Bayesian statistics, and I’ll provide a list of references at the bottom.\n# import some stuff import pymc3 as pm import matplotlib.pyplot as plt import numpy as np from scipy.stats import gaussian_kde, norm, bernoulli plt.style.use(\"bbc\") # https://github.com/ideasbyjin/bbc-plot ;) Statistics Pre-Preamble - feel free to skip In many statistical applications, we want to create a model for our data $$x$$, given some model parameter, $$\\theta$$. We can then represent this model as a probability distribution, i.e. $$P(x\\vert \\theta)$$. (NB: This is the case for parametric models; non-parametric models are different but that’s for another time.)\nI think the easiest way to visualise this idea is using histograms:\n# Create a random set of observations np.random.seed(0) random_obs = np.random.normal(size=1000) # Plot a histogram and the density estimate fig, ax = plt.subplots(1,1) ax.hist(random_obs, bins = np.arange(-3, 3.01, 0.5), density = True, alpha = 0.8) # Plot the curves of three normal distributions interval = np.arange(-3, 3.01, 0.05) ax.plot(interval, [norm.pdf(x) for x in interval], label = \"Model 1\") ax.plot(interval, [norm.pdf(x, scale = 3) for x in interval ], label = \"Model 2\") ax.plot(interval, [norm.pdf(x, loc = 1, scale = 1.5) for x in interval ], label = \"Model 3\") ax.set_ylabel(\"Density\") ax.set_xlabel(\"$x$\") ax.legend() For my data (blue bars), which model (red, yellow, green lines), each with its own $$\\theta$$, is the most likely one that generated the data?\nNaturally, one can ask, what is the best $$\\theta$$ for my observed data? This can be obtained by the likelihood function, which is the joint density of the data,\n$$L(\\theta) = \\prod_{i=1}^{n} P(x_i\\vert \\theta)$$\nThis can start to get a bit confusing - why am I talking about $$\\theta$$ and likelihoods? What does it have to do with Bayesian inference?\nPhilosophically speaking, our observed data is (often) a sample and the true value of $$\\theta$$ is not known…\nBayesian Approach to Models Frequentist statisticans try to deliver a point estimate of the model parameter(s), $$\\theta$$. To quantify uncertainty around that estimate, standard error and/or confidence intervals are used (see this post for a good explanation).\n$$\\theta$$ is then estimated typically using maximum likelihood estimate (MLE) methods - either analytically (e.g. differentiating the log likelihood function), or numerically (e.g. using the expectation-maximisation algorithm).\nBayesian statisticans also agree that the true value of $$\\theta$$ is fixed, but they express the uncertainty of their estimate of $$\\theta$$ using distributions. The aim then is to derive the posterior distribution, $$P(\\theta\\vert x)$$, given some data and some prior belief about $$\\theta$$ itself. In other words,\n$$P(\\theta\\vert x) = \\dfrac{P(x\\vert \\theta)P(\\theta)}{P(x)}$$\nWhat do these probabilities represent?\n $$P(\\theta)$$ represents our prior belief about the parameter $$P(x\\vert \\theta)$$ represents our observations - evidence - or… likelihood! $$P(x)$$ is a normalising constant that represents the probability of $$x$$ irrespective of $$\\theta$$.  Typically, $$P(x)$$ is difficult to calculate, so we represent the posterior being proportional to the product of the likelihood and the prior.\n$$P(\\theta\\vert x) \\propto P(x\\vert \\theta)P(\\theta)$$\nSanta Bayes at work Let’s get to work! We can treat a child being naughty or nice as a binary variable, like a coin being heads or tails. Binary events like these are known as Bernoulli trials.\nThus, we can model the event of seeing a nice or naughty child as samples from a Bernoulli distribution.\n$$x_i \\sim \\text{Bernoulli}(\\theta)$$\nAt the crux of this is the rate of finding a naughty child, or $$\\theta$$. This represents the parameter of our model which we want to estimate using Bayesian inference.\n(From this point forth, a naughty child is represented by a 1, and good children by a 0).\n# Let's see what Bernoulli-distributed variables look like. # This is arbitrarily chosen for example reasons. arbitrary_theta = 0.5 # Let's conduct 100 Bernoulli trials of our own, i.e., Santa visits 100 homes np.random.seed(42) children = [ i for i in bernoulli.rvs(size = 100, p = arbitrary_theta) ] # Print the first 10 print(children[:10]) [0, 1, 1, 1, 0, 0, 0, 1, 1, 1]  Neat, so what happens when we change $$\\theta$$?\n## Plot the effects of bernoulli distribution parameters fig, axes = plt.subplots(3, 2, sharey=True) theta_init = 0.3 num_houses = 100 np.random.seed(42) for i, ax in enumerate(axes.flatten()): theta = theta_init + 0.1 * i _obs = bernoulli.rvs(p = theta, size = num_houses) ax.bar([0, 1], [ (_obs==0).sum(), (_obs==1).sum() ], width = 0.5, label = \"$\\\\theta$ = {:.1f}\".format(theta)) ax.set_xticks([0,1]) ax.set_xticklabels([\"Nice\", \"Naughty\"]) ax.legend() fig.set_size_inches((6,6)) _ = fig.text(0.04, 0.5, \"Number of naughty children\", va = 'center', rotation = 90) Depending on $$\\theta$$, the number of naughty children changes; higher $$\\theta$$ = more naughty kids!\nWhile the frequentist is busy trying to derive the MLE for the likelihood function $$L(\\theta)$$, the Bayesian creates yet another model describing the distribution of $$\\theta$$, as we don’t know the true value of $$\\theta$$.\nIn other words, Santa Bayes is uncertain about the true value of $$\\theta$$. However, he can make an initial stab at $$\\theta$$ - even if it isn’t hugely informative - and blend it with some observations to get an updated estimate of $$\\theta$$.\nThis initial stab is what’s known as the prior distribution of $$\\theta$$. Combined with some data, Santa will reach a new posterior distribution of $$\\theta$$.\nThe choice of the prior can affect the shape of the posterior, but in practice, more observations (moar data) will eventually swamp the effects of the prior.\nKnowing that our data - the distribution of naughty and nice children - follows a Bernoulli distribution, a suitable prior for $\\theta$ is the uniform distribution.\nThere are several nice aspects of using a uniform prior for this problem:\n It makes very little assumptions on what the true value of $$\\theta$$ can be It is very simple to implement!  Santa now decides to use a uniform prior for $$\\theta$$, what does that look like?\n# Initialise a pymc3 model model = pm.Model() with model: theta_dist = pm.Uniform(\"theta\") # Sample from the prior distribution of theta rvs = theta_dist.random(size=20000) bins = np.arange(0,1.01, 0.1) h = plt.hist(rvs, density=True, bins = bins) _ = plt.title(\"Prior distribution of $\\\\theta$\") With this prior distribution for $$\\theta$$ in hand, Santa goes around the neighbourhood and checks for the number of naughty kids in the neighbourhood.\nobservations_test = [1]*13 + [0]*7 # 13 naughty kids, 7 nice ones, i.e. theta ~ 0.65 These observations can then be used for the likelihood function in pymc3. The key here is that we provide the Bernoulli likelihood function in pymc3 the set of observations. We also specify that the parameter for the Bernoulli distribution is sampled from the uniform prior that we have discussed earlier.\n# Define a function to get the distribution of theta after MCMC def get_trace(observations): # Initialise a pymc3 model model = pm.Model() with model: # This creates a distribution on theta theta_dist = pm.Uniform(\"theta\") # Call the Bernoulli likelihood function observed_dist # The parameter for this distribution is from the prior, theta_dist; # observations are given to observed.  observed_dist = pm.Bernoulli(\"obs\", p = theta_dist, observed=observations) # Use the Metropolis-Hastings algorithm to estimate the posterior step = pm.Metropolis() trace = pm.sample(10000, step = step) return trace In the function above, we use something called the Metropolis-Hastings algorithm to determine the posterior distribution. It’s a bit outside the scope of this post, but it’s essentially a technique that provides a numeric estimate of the true posterior distribution. It’s handy when you can’t derive an analytical solution.\ntrace = get_trace(observations=observations_test) fig, ax = plt.subplots() # MCMC methods have a burn-in period, so we discard the first few thousand iterations. ax.hist(trace[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax.axvline(13./20, linestyle = '--', color = 'k', label = '$\\\\theta$ estimated from relative frequency') _ = ax.set_title(\"Posterior distribution of $\\\\theta$\") Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:08 What’s happened here? Essentially, by giving a set of observations, we see that the posterior distribution of $$\\theta$$ has been estimated, and it looks very different to our prior. In fact, it creates a bell curve-like histogram around $$\\theta = 0.65$$.\nGiven this updated, posterior distribution of $$\\theta$$, Santa can then do some calculations on this new distribution, such as:\n Summary statistics of the posterior distribution (mean, standard deviation) The maximum a posteriori estimate, or the MAP The 95% credible interval - not to be confused with confidence intervals!  Following these statistics, Santa can take further action on whether there are too many naughty kids in the neighbourhood, and whether it’s worth his time to stick around. Furthermore, Santa can use this updated posterior as a new prior for future inference activities.\nIn fact, we can see what happens when he goes to a different neighbourhood with a different number of nice kids, but with an identical uniform prior as before.\nobservations_nice = [1]*3 + [0]*17 # 9 naughty kids, 11 nice ones, i.e. theta ~ 0.15 trace = get_trace(observations=observations_nice) fig, ax = plt.subplots() # MCMC methods have a burn-in period, so we discard the first few thousand iterations. ax.hist(trace[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax.axvline(3./20, linestyle = '--', color = 'k', label = '$\\\\theta$ estimated from relative frequency') _ = ax.set_title(\"Posterior distribution of $\\\\theta$\") Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07 As we can see, for all intents and purposes, our implementation has remained almost identical, aside from the fact that our observation vectors are different. This leads to huge changes in the posterior distributions, and thus our understanding of $$\\theta$$!\nEffects of data size OK, so now Santa has a framework for estimating the rate in which he’ll come across naughty or nice kids. How much will his posterior be affected by the number of observations?\nFor this example, we can generate some observations using a pre-defined, arbitrary $$\\theta$$. We can then see if our posterior distribution converges to that arbitrary $$\\theta$$ value, too.\narbitrary_theta = 0.2 small, medium, large = bernoulli.rvs(p=arbitrary_theta,size=10), bernoulli.rvs(p=arbitrary_theta,size=20),\\ bernoulli.rvs(p=arbitrary_theta,size=200) low_obs_trace = get_trace(small) medium_trace = get_trace(medium) high_obs_trace = get_trace(large) Multiprocess sampling (4 chains in 4 jobs) Metropolis: [theta] Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [07:33 fig, ax = plt.subplots(1,3) ax[0].hist(low_obs_trace[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[0].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[0].set_title(\"Posterior $\\\\theta$ with 10 observations\") ax[1].hist(medium_trace[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[1].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[1].set_title(\"Posterior $\\\\theta$ with 20 observations\") ax[2].hist(high_obs_trace[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05)) ax[2].axvline(arbitrary_theta, linestyle = '--', color = 'k', label = 'Arbitrary $\\\\theta$') ax[2].set_title(\"Posterior $\\\\theta$ with 200 observations\") ax[2].legend() fig.set_size_inches((12,3)) As we can see here, with more observations, we eventually diverge away from the shape of the prior. In fact, even from 10 observations, we can see that the posterior distribution of $$\\theta$$ already looks dissimilar to the uniform prior we had specified before.\nWhile the posterior distributions don’t converge tightly enough around the arbitrarily defined $$\\theta$$ value unless there’s 500 observations, it’s clear that even with a small amount of data, we see positive results!\nEffects of the prior For the skeptic, we can choose a different prior distribution for $$\\theta$$ - the Beta distribution:\n$$\\theta \\sim \\text{Beta}(\\alpha, \\beta)$$\nThe beta distribution, like the uniform distribution, is bound to values between 0 and 1. It is defined by two additional parameters, $$\\alpha$$ and $$\\beta$$. Thus, in this context, they are known as hyper-parameters for our use case.\n Interestingly, the uniform distribution is a special case of the Beta distribution where $$\\alpha = 1$$ and $$\\beta = 1$$…\n Does it matter what $$\\alpha$$ and $$\\beta$$ are?\ndef get_trace_beta(observations = None, alpha = 1, beta = 1): # Initialise a pymc3 model model = pm.Model() with model: theta_dist = pm.Beta(\"theta\", alpha, beta) # Call the Bernoulli likelihood function obs # The parameter is from the prior, theta_dist # observations are given to observed.  observed_dist = pm.Bernoulli(\"obs\", p = theta_dist, observed=observations) # Use the Metropolis-Hastings algorithm  step = pm.Metropolis() trace = pm.sample(10000, step = step) return theta_dist, trace prior_a, hyper_a = get_trace_beta(small, alpha = 1, beta = 2) prior_b, hyper_b = get_trace_beta(small, alpha = 2, beta = 2) prior_c, hyper_c = get_trace_beta(small, alpha = 3, beta = 1) prior_1, hyper_1 = get_trace_beta(large, alpha = 1, beta = 2) prior_2, hyper_2 = get_trace_beta(large, alpha = 2, beta = 2) prior_3, hyper_3 = get_trace_beta(large, alpha = 3, beta = 1) fig, axes = plt.subplots(2,3, sharey=True) ax = axes.flatten() ax[0].hist(prior_a.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[0].hist(hyper_a[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[0].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[0].set_title(\"$n = 10; \\\\alpha=1, \\\\beta=2$\") ax[1].hist(prior_b.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[1].hist(hyper_b[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[1].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[1].set_title(\"$n = 10; \\\\alpha=2, \\\\beta=2$\") ax[2].hist(prior_c.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[2].hist(hyper_c[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[2].axvline(arbitrary_theta, linestyle = '--', color = 'k', label = 'Arbitrary $\\\\theta$') ax[2].set_title(\"$n = 10; \\\\alpha=3, \\\\beta=1$\") ax[3].hist(prior_1.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[3].hist(hyper_1[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[3].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[3].set_title(\"$n = 500; \\\\alpha=1, \\\\beta=2$\") ax[4].hist(prior_2.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[4].hist(hyper_2[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[4].axvline(arbitrary_theta, linestyle = '--', color = 'k') ax[4].set_title(\"$n = 500; \\\\alpha=2, \\\\beta=2$\") ax[5].hist(prior_3.random(size=20000), density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Prior\") ax[5].hist(hyper_3[\"theta\"][2000:], density=True, bins = np.arange(0, 1.01, 0.05), alpha = 0.4, label = \"Posterior\") ax[5].axvline(arbitrary_theta, linestyle = '--', color = 'k', label = 'Arbitrary $\\\\theta$') ax[5].set_title(\"$n = 500; \\\\alpha=3, \\\\beta=1$\") ax[5].legend() fig.text(0.5, 0.04, \"$\\\\theta$\", ha = 'center') fig.text(0.08, 0.5, \"Density\", va = 'center', rotation = 90) fig.set_size_inches((12,6)) In short, when given identical observation vectors, the prior does still have an effect on the shape of the posterior, but it is clear that with a large number of observations, the data dominates the shape.\nFinal thoughts and Limitations Enjoy reading this? :) I hope this was a nice intro to Bayesian inference! The take-aways really are:\n Bayesian inference is doable, but the learning curve is steep! Packages like pymc3 take away a lot of the leg-work, especially when your distributions aren’t mathematically nice to deal with. For real-world problems, they may not be as easily decomposable as we have in this example, but hopefully this gets you thinking about how Bayesians think about parameters in general. Conjugate priors and the details of MCMC haven’t been discussed but perhaps that’s for another time.  Here are some references I’ve come across and really benefited from when making this entry:\n Bayesian Methods for Hackers - hugely influential, I would say this formed a large chunk of my inspirations here. Bayesian Inference in One Hour - a solid set of lecutre notes from Patrick Lam. Refresher on Likelihood Some random SO / StackExchange threads, e.g. this one on credible intervals  ",
  "wordCount" : "2811",
  "inLanguage": "en",
  "datePublished": "2019-12-25T00:00:00Z",
  "dateModified": "2019-12-25T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ideasbyjin.github.io/post/2019-12-25-santa-bayes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Read between the rows",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ideasbyjin.github.io/favicon.ico"
    }
  }
}
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ideasbyjin.github.io/" accesskey="h" title="Read between the rows (Alt + H)">Read between the rows</a>
            <span class="logo-switches">
                <a id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </a>
                
                
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://ideasbyjin.github.io/search" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/archives" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/about" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://ideasbyjin.github.io/" title="home">
                    <span>home</span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">

    <h1 class="post-title">
      Santa Bayes: a Christmas introduction to Bayesian inference
    </h1>
    <div class="post-meta">December 25, 2019&nbsp;·&nbsp;14 min

</div>
  </header> 

  <div class="toc">
    <details >
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#statistics-pre-preamble---feel-free-to-skip" aria-label="Statistics Pre-Preamble - feel free to skip">Statistics Pre-Preamble - feel free to skip</a></li><li>
        <a href="#bayesian-approach-to-models" aria-label="Bayesian Approach to Models">Bayesian Approach to Models</a></li><li>
        <a href="#santa-bayes-at-work" aria-label="Santa Bayes at work">Santa Bayes at work</a></li><li>
        <a href="#effects-of-data-size" aria-label="Effects of data size">Effects of data size</a></li><li>
        <a href="#effects-of-the-prior" aria-label="Effects of the prior">Effects of the prior</a></li><li>
        <a href="#final-thoughts-and-limitations" aria-label="Final thoughts and Limitations">Final thoughts and Limitations</a></li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<p>It&rsquo;s Christmas, and Santa is here! He&rsquo;s got his list and he&rsquo;s about to see who&rsquo;s been naughty or nice this year. While on his way out of the North Pole, he comes across some turbulence, and he loses his list. Rudolph tried his hardest, but to no avail.</p>
<p>Santa is in trouble. Without his list, it&rsquo;s going to take ages to visit <em>every</em> house in <em>every</em> neighbourhood, then go down the chimney to see who&rsquo;s been nice or naughty. However, Santa reasons that if a neighbourhood is generally full of bad kids, he can skip them, for now, and give gifts elsewhere.</p>
<p>In this alternate universe, Santa is, luckily, a Bayesian statistican. Santa decides he is going to use Bayesian inference to guess the number of naughty kids.</p>
<p>It&rsquo;s going to be a long night, but one that&rsquo;s hopefully salvaged by Bayesian methods!</p>
<p>If you have&hellip;</p>
<ul>
<li><strong>30 seconds</strong>:
Bayesian inference is driven by this equation:</li>
</ul>
<p>$$P(B\vert A) = \dfrac{P(A\vert B)P(B)}{P(A)}$$</p>
<p>Unlike frequentist methods that try to make <em>point estimates</em> of parameters, Bayesian methods are driven by estimating <em>distributions</em> of parameters.</p>
<ul>
<li><strong>15 minutes</strong>: It&rsquo;s a long one, but hopefully it&rsquo;ll be worth it. This post will discuss the basics of Bayesian inference, and how we can use the <code>pymc3</code> library for statistical computing.</li>
</ul>
<p>NB: I would <em>not</em> consider myself an expert in Bayesian statistics, and I&rsquo;ll provide a list of references at the bottom.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import some stuff</span>
<span style="color:#f92672">import</span> pymc3 <span style="color:#f92672">as</span> pm
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> gaussian_kde, norm, bernoulli

plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#34;bbc&#34;</span>) <span style="color:#75715e"># https://github.com/ideasbyjin/bbc-plot ;)</span>
</code></pre></div><h2 id="statistics-pre-preamble---feel-free-to-skip">Statistics Pre-Preamble - feel free to skip<a hidden class="anchor" aria-hidden="true" href="#statistics-pre-preamble---feel-free-to-skip">#</a></h2>
<p>In many statistical applications, we want to create a model for our data $$x$$, given some model parameter,
$$\theta$$. We can then represent this model as a probability distribution, i.e. $$P(x\vert \theta)$$. (NB:
This is the case for <em>parametric</em> models; non-parametric models are different but that&rsquo;s for another time.)</p>
<p>I think the easiest way to visualise this idea is using histograms:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Create a random set of observations</span>
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)
random_obs <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)

<span style="color:#75715e"># Plot a histogram and the density estimate</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
ax<span style="color:#f92672">.</span>hist(random_obs, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3.01</span>, <span style="color:#ae81ff">0.5</span>), density <span style="color:#f92672">=</span> True, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.8</span>)

<span style="color:#75715e"># Plot the curves of three normal distributions</span>
interval <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3.01</span>, <span style="color:#ae81ff">0.05</span>)
ax<span style="color:#f92672">.</span>plot(interval, [norm<span style="color:#f92672">.</span>pdf(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> interval], label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Model 1&#34;</span>)
ax<span style="color:#f92672">.</span>plot(interval, [norm<span style="color:#f92672">.</span>pdf(x, scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> interval ], label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Model 2&#34;</span>)
ax<span style="color:#f92672">.</span>plot(interval, [norm<span style="color:#f92672">.</span>pdf(x, loc <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.5</span>) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> interval ], label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Model 3&#34;</span>)

ax<span style="color:#f92672">.</span>set_ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>)
ax<span style="color:#f92672">.</span>set_xlabel(<span style="color:#e6db74">&#34;$x$&#34;</span>)
ax<span style="color:#f92672">.</span>legend()
</code></pre></div><p><img src="/assets/notebooks/santa_bayes/output_3_1.png" alt="png"></p>
<p>For my data (blue bars), which model (red, yellow, green lines), each with its own $$\theta$$, is the most likely one that generated the data?</p>
<p>Naturally, one can ask, what is the <strong>best</strong> $$\theta$$ for my observed data? This can be obtained by the <em>likelihood function</em>, which is the joint density of the data,</p>
<p>$$L(\theta) = \prod_{i=1}^{n} P(x_i\vert \theta)$$</p>
<p>This can start to get a bit confusing - why am I talking about $$\theta$$ and likelihoods? What does it have to do with Bayesian inference?</p>
<p>Philosophically speaking, our observed data is (often) a sample and the <em>true</em> value of $$\theta$$ is not known&hellip;</p>
<h2 id="bayesian-approach-to-models">Bayesian Approach to Models<a hidden class="anchor" aria-hidden="true" href="#bayesian-approach-to-models">#</a></h2>
<p>Frequentist statisticans try to deliver a point estimate of the model parameter(s), $$\theta$$. To quantify <em>uncertainty</em> around that estimate, standard error and/or confidence intervals are used (see this <a href="https://genomicsclass.github.io/book/pages/confidence_intervals.html">post</a> for a good explanation).</p>
<p>$$\theta$$ is then estimated typically using maximum likelihood estimate (MLE) methods - either analytically (e.g. differentiating the log likelihood function), or numerically (e.g. using the expectation-maximisation algorithm).</p>
<p>Bayesian statisticans also agree that the true value of $$\theta$$ is fixed, but they express the uncertainty of their
estimate of $$\theta$$ using distributions. The aim then is to derive the <em>posterior distribution</em>, $$P(\theta\vert x)$$,
<strong>given some data</strong> and some <strong>prior belief</strong> about $$\theta$$ itself. In other words,</p>
<p>$$P(\theta\vert x) = \dfrac{P(x\vert \theta)P(\theta)}{P(x)}$$</p>
<p>What do these probabilities represent?</p>
<ul>
<li>$$P(\theta)$$ represents our <em>prior</em> belief about the parameter</li>
<li>$$P(x\vert \theta)$$ represents our observations - evidence - or&hellip; <strong>likelihood</strong>!</li>
<li>$$P(x)$$ is a normalising constant that represents the probability of $$x$$ irrespective of $$\theta$$.</li>
</ul>
<p>Typically, $$P(x)$$ is difficult to calculate, so we represent the posterior being proportional to the product of the
likelihood and the prior.</p>
<p>$$P(\theta\vert x) \propto P(x\vert \theta)P(\theta)$$</p>
<h2 id="santa-bayes-at-work">Santa Bayes at work<a hidden class="anchor" aria-hidden="true" href="#santa-bayes-at-work">#</a></h2>
<p>Let&rsquo;s get to work! We can treat a child being naughty or nice as a <em>binary variable</em>, like a coin being heads or tails.
Binary events like these are known as <em>Bernoulli trials</em>.</p>
<p>Thus, we can model the event of seeing a nice or naughty child as samples from a Bernoulli distribution.</p>
<p>$$x_i \sim \text{Bernoulli}(\theta)$$</p>
<p>At the crux of this is the <em>rate</em> of finding a naughty child, or $$\theta$$. This represents the <strong>parameter</strong> of our
model which we want to estimate using Bayesian inference.</p>
<p>(From this point forth, a naughty child is represented by a 1, and good children by a 0).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Let&#39;s see what Bernoulli-distributed variables look like.</span>
<span style="color:#75715e"># This is arbitrarily chosen for example reasons.</span>
arbitrary_theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>

<span style="color:#75715e"># Let&#39;s conduct 100 Bernoulli trials of our own, i.e., Santa visits 100 homes</span>
np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
children <span style="color:#f92672">=</span> [ i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> bernoulli<span style="color:#f92672">.</span>rvs(size <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>, p <span style="color:#f92672">=</span> arbitrary_theta) ]

<span style="color:#75715e"># Print the first 10</span>
<span style="color:#66d9ef">print</span>(children[:<span style="color:#ae81ff">10</span>])
</code></pre></div><pre><code>[0, 1, 1, 1, 0, 0, 0, 1, 1, 1]
</code></pre>
<p>Neat, so what happens when we change $$\theta$$?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">## Plot the effects of bernoulli distribution parameters</span>
fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, sharey<span style="color:#f92672">=</span>True)
theta_init <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>
num_houses <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>

np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
<span style="color:#66d9ef">for</span> i, ax <span style="color:#f92672">in</span> enumerate(axes<span style="color:#f92672">.</span>flatten()):
    
    theta <span style="color:#f92672">=</span> theta_init <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.1</span> <span style="color:#f92672">*</span> i
    _obs <span style="color:#f92672">=</span> bernoulli<span style="color:#f92672">.</span>rvs(p <span style="color:#f92672">=</span> theta, size <span style="color:#f92672">=</span> num_houses)
    ax<span style="color:#f92672">.</span>bar([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [ (_obs<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>sum(), (_obs<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>sum() ], width <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;$</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ = {:.1f}&#34;</span><span style="color:#f92672">.</span>format(theta))
    
    ax<span style="color:#f92672">.</span>set_xticks([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>])
    ax<span style="color:#f92672">.</span>set_xticklabels([<span style="color:#e6db74">&#34;Nice&#34;</span>, <span style="color:#e6db74">&#34;Naughty&#34;</span>])
    ax<span style="color:#f92672">.</span>legend()

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">6</span>))
_ <span style="color:#f92672">=</span> fig<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.04</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;Number of naughty children&#34;</span>, va <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;center&#39;</span>, rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)
</code></pre></div><p><img src="/assets/notebooks/santa_bayes/output_9_0.png" alt="png"></p>
<p>Depending on $$\theta$$, the number of naughty children changes; <strong>higher $$\theta$$ = more naughty kids!</strong></p>
<p>While the frequentist is busy trying to derive the MLE for the likelihood function $$L(\theta)$$, the Bayesian
creates yet <em>another</em> model describing the <em>distribution</em> of $$\theta$$, as <strong>we don&rsquo;t know the true value of $$\theta$$.</strong></p>
<p>In other words, Santa Bayes is <em>uncertain</em> about the true value of $$\theta$$. However, he can make an initial stab at
$$\theta$$ - even if it isn&rsquo;t hugely informative - and blend it with some observations to get an <em>updated</em> estimate of
$$\theta$$.</p>
<p>This initial stab is what&rsquo;s known as the <em>prior distribution</em> of $$\theta$$. Combined with some data, Santa will reach a
new <em>posterior distribution</em> of $$\theta$$.</p>
<p>The choice of the prior can affect the shape of the posterior, but in practice, more observations (<code>moar data</code>) will
eventually swamp the effects of the prior.</p>
<p>Knowing that our data - the distribution of naughty and nice children - follows a Bernoulli distribution, a suitable prior for $\theta$ is the <strong>uniform distribution</strong>.</p>
<p>There are several nice aspects of using a uniform prior for this problem:</p>
<ul>
<li>It makes very little assumptions on what the true value of $$\theta$$ can be</li>
<li>It is very simple to implement!</li>
</ul>
<p>Santa now decides to use a uniform prior for $$\theta$$, what does that look like?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Initialise a pymc3 model</span>
model <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Model()

<span style="color:#66d9ef">with</span> model:
    theta_dist <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Uniform(<span style="color:#e6db74">&#34;theta&#34;</span>)

<span style="color:#75715e"># Sample from the prior distribution of theta</span>
rvs <span style="color:#f92672">=</span> theta_dist<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>)
bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.1</span>)

h <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>hist(rvs, density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> bins)
_ <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Prior distribution of $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#34;</span>)
</code></pre></div><p><img src="/assets/notebooks/santa_bayes/output_11_0.png" alt="png"></p>
<p>With this prior distribution for $$\theta$$ in hand, Santa goes around the neighbourhood and checks for the
number of naughty kids in the neighbourhood.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observations_test <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>]<span style="color:#f92672">*</span><span style="color:#ae81ff">13</span> <span style="color:#f92672">+</span> [<span style="color:#ae81ff">0</span>]<span style="color:#f92672">*</span><span style="color:#ae81ff">7</span> <span style="color:#75715e"># 13 naughty kids, 7 nice ones, i.e. theta ~ 0.65</span>
</code></pre></div><p>These observations can then be used for the likelihood function in <code>pymc3</code>. The key here is that we provide the Bernoulli likelihood function in <code>pymc3</code> the set of observations. We also specify that the <em>parameter</em> for the Bernoulli distribution is <strong>sampled from the uniform prior</strong> that we have discussed earlier.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Define a function to get the distribution of theta after MCMC</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_trace</span>(observations):
    
    <span style="color:#75715e"># Initialise a pymc3 model</span>
    model <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Model()

    <span style="color:#66d9ef">with</span> model:
        <span style="color:#75715e"># This creates a distribution on theta</span>
        theta_dist <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Uniform(<span style="color:#e6db74">&#34;theta&#34;</span>)

        <span style="color:#75715e"># Call the Bernoulli likelihood function observed_dist</span>
        <span style="color:#75715e"># The parameter for this distribution is from the prior, theta_dist;</span>
        <span style="color:#75715e"># observations are given to observed. </span>
        observed_dist <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Bernoulli(<span style="color:#e6db74">&#34;obs&#34;</span>, p <span style="color:#f92672">=</span> theta_dist, observed<span style="color:#f92672">=</span>observations)

        <span style="color:#75715e"># Use the Metropolis-Hastings algorithm to estimate the posterior</span>
        step <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Metropolis()
        trace <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>sample(<span style="color:#ae81ff">10000</span>, step <span style="color:#f92672">=</span> step)
    
    <span style="color:#66d9ef">return</span> trace
</code></pre></div><p>In the function above, we use something called the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings algorithm</a>
to determine the posterior distribution. It&rsquo;s a bit outside the scope of this post, but it&rsquo;s essentially a technique that
provides a numeric estimate of the true posterior distribution. It&rsquo;s handy when you can&rsquo;t derive an analytical solution.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">trace <span style="color:#f92672">=</span> get_trace(observations<span style="color:#f92672">=</span>observations_test)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()

<span style="color:#75715e"># MCMC methods have a burn-in period, so we discard the first few thousand iterations.</span>
ax<span style="color:#f92672">.</span>hist(trace[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>))
ax<span style="color:#f92672">.</span>axvline(<span style="color:#ae81ff">13.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">20</span>, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;$</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ estimated from relative frequency&#39;</span>)
_ <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior distribution of $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#34;</span>)
</code></pre></div><pre><code>Multiprocess sampling (4 chains in 4 jobs)
Metropolis: [theta]
Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:08&lt;00:00, 5162.22draws/s]
The number of effective samples is smaller than 25% for some parameters.
</code></pre>
<p><img src="/assets/notebooks/santa_bayes/output_17_2.png" alt="png"></p>
<p>What&rsquo;s happened here? Essentially, by giving a set of observations, we see that the posterior distribution of
$$\theta$$ has been estimated, and it looks very different to our prior. In fact, it creates a bell curve-like
histogram around $$\theta = 0.65$$.</p>
<p>Given this updated, posterior distribution of $$\theta$$, Santa can then do some calculations on this new distribution,
such as:</p>
<ul>
<li>Summary statistics of the posterior distribution (mean, standard deviation)</li>
<li>The maximum <em>a posteriori</em> estimate, or the MAP</li>
<li>The 95% credible interval - not to be confused with confidence intervals!</li>
</ul>
<p>Following these statistics, Santa can take further action on whether there are too many naughty kids in the neighbourhood,
and whether it&rsquo;s worth his time to stick around. Furthermore, Santa can use this <a href="http://www.stats.ox.ac.uk/~steffen/teaching/bs2HT9/kalman.pdf">updated posterior as a new prior</a> for future
inference activities.</p>
<p>In fact, we can see what happens when he goes to a different neighbourhood with a different number of nice kids, but with
an identical uniform prior as before.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">observations_nice <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>]<span style="color:#f92672">*</span><span style="color:#ae81ff">3</span> <span style="color:#f92672">+</span> [<span style="color:#ae81ff">0</span>]<span style="color:#f92672">*</span><span style="color:#ae81ff">17</span> <span style="color:#75715e"># 9 naughty kids, 11 nice ones, i.e. theta ~ 0.15</span>

trace <span style="color:#f92672">=</span> get_trace(observations<span style="color:#f92672">=</span>observations_nice)

fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()

<span style="color:#75715e"># MCMC methods have a burn-in period, so we discard the first few thousand iterations.</span>
ax<span style="color:#f92672">.</span>hist(trace[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>))
ax<span style="color:#f92672">.</span>axvline(<span style="color:#ae81ff">3.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">20</span>, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;$</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ estimated from relative frequency&#39;</span>)
_ <span style="color:#f92672">=</span> ax<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior distribution of $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#34;</span>)
</code></pre></div><pre><code>Multiprocess sampling (4 chains in 4 jobs)
Metropolis: [theta]
Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07&lt;00:00, 5823.77draws/s]
The number of effective samples is smaller than 25% for some parameters.
</code></pre>
<p><img src="/assets/notebooks/santa_bayes/output_19_2.png" alt="png"></p>
<p>As we can see, for all intents and purposes, our implementation has remained almost identical, aside from the fact that
our observation vectors are different. This leads to huge changes in the posterior distributions, and thus our understanding
of $$\theta$$!</p>
<h2 id="effects-of-data-size">Effects of data size<a hidden class="anchor" aria-hidden="true" href="#effects-of-data-size">#</a></h2>
<p>OK, so now Santa has a framework for estimating the rate in which he&rsquo;ll come across naughty or nice kids.
How much will his posterior be affected by the <em>number of observations</em>?</p>
<p>For this example, we can generate some observations using a pre-defined, arbitrary $$\theta$$. We can then see if
our posterior distribution converges to that arbitrary $$\theta$$ value, too.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">arbitrary_theta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
small, medium, large <span style="color:#f92672">=</span> bernoulli<span style="color:#f92672">.</span>rvs(p<span style="color:#f92672">=</span>arbitrary_theta,size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>), bernoulli<span style="color:#f92672">.</span>rvs(p<span style="color:#f92672">=</span>arbitrary_theta,size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>),\
                       bernoulli<span style="color:#f92672">.</span>rvs(p<span style="color:#f92672">=</span>arbitrary_theta,size<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)

low_obs_trace <span style="color:#f92672">=</span> get_trace(small)
medium_trace <span style="color:#f92672">=</span> get_trace(medium)
high_obs_trace <span style="color:#f92672">=</span> get_trace(large)
</code></pre></div><pre><code>Multiprocess sampling (4 chains in 4 jobs)
Metropolis: [theta]
Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [07:33&lt;00:00, 92.67draws/s]  
The number of effective samples is smaller than 25% for some parameters.
Multiprocess sampling (4 chains in 4 jobs)
Metropolis: [theta]
Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07&lt;00:00, 5564.29draws/s]
The number of effective samples is smaller than 25% for some parameters.
Multiprocess sampling (4 chains in 4 jobs)
Metropolis: [theta]
Sampling 4 chains, 0 divergences: 100%|██████████| 42000/42000 [00:07&lt;00:00, 5974.85draws/s]
The number of effective samples is smaller than 25% for some parameters.
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>)

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>hist(low_obs_trace[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>))
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ with 10 observations&#34;</span>)

ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>hist(medium_trace[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>))
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ with 20 observations&#34;</span>)

ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>hist(high_obs_trace[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>))
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Arbitrary $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#39;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Posterior $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$ with 200 observations&#34;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>legend()

fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">3</span>))
</code></pre></div><p><img src="/assets/notebooks/santa_bayes/output_23_0.png" alt="png"></p>
<p>As we can see here, with more observations, we eventually diverge away from the shape of the prior. In fact, even from
10 observations, we can see that the posterior distribution of $$\theta$$ already looks dissimilar to the uniform prior
we had specified before.</p>
<p>While the posterior distributions don&rsquo;t converge tightly enough around the arbitrarily defined $$\theta$$ value unless
there&rsquo;s 500 observations, it&rsquo;s clear that even with a small amount of data, we see positive results!</p>
<h2 id="effects-of-the-prior">Effects of the prior<a hidden class="anchor" aria-hidden="true" href="#effects-of-the-prior">#</a></h2>
<p>For the skeptic, we can choose a different prior distribution for $$\theta$$ - the Beta distribution:</p>
<p>$$\theta \sim \text{Beta}(\alpha, \beta)$$</p>
<p>The beta distribution, like the uniform distribution, is bound to values between 0 and 1. It is defined by two <em>additional</em>
parameters, $$\alpha$$ and $$\beta$$. Thus, in this context, they are known as <em>hyper-parameters</em> for our use case.</p>
<blockquote>
<p>Interestingly, the uniform distribution is a special case of the Beta distribution where $$\alpha = 1$$ and $$\beta = 1$$&hellip;</p>
</blockquote>
<p>Does it matter what $$\alpha$$ and $$\beta$$ are?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_trace_beta</span>(observations <span style="color:#f92672">=</span> None, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>):
    
    <span style="color:#75715e"># Initialise a pymc3 model</span>
    model <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Model()

    <span style="color:#66d9ef">with</span> model:
        theta_dist <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Beta(<span style="color:#e6db74">&#34;theta&#34;</span>, alpha, beta)

        <span style="color:#75715e"># Call the Bernoulli likelihood function obs</span>
        <span style="color:#75715e"># The parameter is from the prior, theta_dist</span>
        <span style="color:#75715e"># observations are given to observed. </span>
        observed_dist <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Bernoulli(<span style="color:#e6db74">&#34;obs&#34;</span>, p <span style="color:#f92672">=</span> theta_dist, observed<span style="color:#f92672">=</span>observations)

        <span style="color:#75715e"># Use the Metropolis-Hastings algorithm  </span>
        step <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>Metropolis()
        trace <span style="color:#f92672">=</span> pm<span style="color:#f92672">.</span>sample(<span style="color:#ae81ff">10000</span>, step <span style="color:#f92672">=</span> step)
    
    <span style="color:#66d9ef">return</span> theta_dist, trace
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">prior_a, hyper_a <span style="color:#f92672">=</span> get_trace_beta(small, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
prior_b, hyper_b <span style="color:#f92672">=</span> get_trace_beta(small, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
prior_c, hyper_c <span style="color:#f92672">=</span> get_trace_beta(small, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)

prior_1, hyper_1 <span style="color:#f92672">=</span> get_trace_beta(large, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
prior_2, hyper_2 <span style="color:#f92672">=</span> get_trace_beta(large, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
prior_3, hyper_3 <span style="color:#f92672">=</span> get_trace_beta(large, alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>, beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>, sharey<span style="color:#f92672">=</span>True)

ax <span style="color:#f92672">=</span> axes<span style="color:#f92672">.</span>flatten()

ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>hist(prior_a<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>hist(hyper_a[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 10; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=1, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=2$&#34;</span>)

ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>hist(prior_b<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>hist(hyper_b[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 10; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=2, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=2$&#34;</span>)

ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>hist(prior_c<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>hist(hyper_c[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Arbitrary $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#39;</span>)
ax[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 10; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=3, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=1$&#34;</span>)

ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>hist(prior_1<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>hist(hyper_1[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 500; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=1, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=2$&#34;</span>)

ax[<span style="color:#ae81ff">4</span>]<span style="color:#f92672">.</span>hist(prior_2<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">4</span>]<span style="color:#f92672">.</span>hist(hyper_2[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">4</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>)
ax[<span style="color:#ae81ff">4</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 500; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=2, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=2$&#34;</span>)

ax[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>hist(prior_3<span style="color:#f92672">.</span>random(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>), density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Prior&#34;</span>)
ax[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>hist(hyper_3[<span style="color:#e6db74">&#34;theta&#34;</span>][<span style="color:#ae81ff">2000</span>:], density<span style="color:#f92672">=</span>True, bins <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.01</span>, <span style="color:#ae81ff">0.05</span>), alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.4</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Posterior&#34;</span>)
ax[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>axvline(arbitrary_theta, linestyle <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;--&#39;</span>, color <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;k&#39;</span>, label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Arbitrary $</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#39;</span>)
ax[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;$n = 500; </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">alpha=3, </span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">beta=1$&#34;</span>)
ax[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>legend()

fig<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.04</span>, <span style="color:#e6db74">&#34;$</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$&#34;</span>, ha <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;center&#39;</span>)
fig<span style="color:#f92672">.</span>text(<span style="color:#ae81ff">0.08</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;Density&#34;</span>, va <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;center&#39;</span>, rotation <span style="color:#f92672">=</span> <span style="color:#ae81ff">90</span>)
fig<span style="color:#f92672">.</span>set_size_inches((<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">6</span>))
</code></pre></div><p><img src="/assets/notebooks/santa_bayes/output_28_0.png" alt="png"></p>
<p>In short, when given identical observation vectors, the prior does still have an effect on the shape of the posterior, but it is clear that with a large number of observations, the data dominates the shape.</p>
<h2 id="final-thoughts-and-limitations">Final thoughts and Limitations<a hidden class="anchor" aria-hidden="true" href="#final-thoughts-and-limitations">#</a></h2>
<p>Enjoy reading this? :) I hope this was a nice intro to Bayesian inference! The take-aways really are:</p>
<ul>
<li>Bayesian inference is doable, but the learning curve is steep!</li>
<li>Packages like pymc3 take away a lot of the leg-work, especially when your distributions aren&rsquo;t mathematically nice to deal with.</li>
<li>For real-world problems, they may not be as easily decomposable as we have in this example, but hopefully this gets you thinking about how Bayesians think about parameters in general.</li>
<li>Conjugate priors and the details of MCMC haven&rsquo;t been discussed but perhaps that&rsquo;s for another time.</li>
</ul>
<p>Here are some references I&rsquo;ve come across and really benefited from when making this entry:</p>
<ul>
<li><a href="https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Bayesian Methods for Hackers</a> - hugely influential, I would say this formed a large chunk of my inspirations here.</li>
<li><a href="http://patricklam.org/teaching/bayesianhour_print.pdf">Bayesian Inference in One Hour</a> - a solid set of lecutre notes from Patrick Lam.</li>
<li><a href="http://www.stat.cmu.edu/~larry/=stat705/Lecture6.pdf">Refresher on Likelihood</a></li>
<li>Some random SO / StackExchange threads, e.g. this one on <a href="https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval">credible intervals</a></li>
</ul>

</div>
  <footer class="post-footer">
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://ideasbyjin.github.io/">Read between the rows</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>



<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
